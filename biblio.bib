@article{Zhu2013,
abstract = {Max-margin learning is a powerful approach to building classifiers and structured output predictors. Recent work on max-margin supervised topic models has successfully integrated it with Bayesian topic models to discover discriminative latent semantic structures and make accurate predictions for unseen testing data. However, the resulting learning problems are usually hard to solve because of the non-smoothness of the margin loss. Existing approaches to building max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents an alternative approach by defining a new max-margin loss. Namely, we present Gibbs max-margin supervised topic models, a latent variable Gibbs classifier to discover hidden topic representations for various tasks, including classification, regression and multi-task learning. Gibbs max-margin supervised topic models minimize an expected margin loss, which is an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, we develop simple Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems. Furthermore, each step of the "augment-and-collapse" Gibbs sampling algorithms has an analytical conditional distribution, from which samples can be easily drawn. Experimental results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors on binary, multi-class and multi-label classification tasks.},
archivePrefix = {arXiv},
arxivId = {1310.2816},
author = {Zhu, Jun and Chen, Ning and Perkins, Hugh and Zhang, Bo},
eprint = {1310.2816},
keywords = {gibbs classifiers,max-margin learning,regularized,supervised topic models},
pages = {1073--1110},
title = {{Gibbs Max-margin Topic Models with Data Augmentation}},
url = {http://arxiv.org/abs/1310.2816},
volume = {15},
year = {2013}
}

@article{Zhou2019a,
abstract = {Aim: We performed a meta-analysis to evaluate the efficacy and safety of dutasteride and finasteride in treating men with androgenetic alopecia (AGA) during a 24-week treatment cycle. Methods: Randomized controlled trials of dutasteride and finasteride for treating AGA were searched using MEDLINE, EMBASE, and the Cochrane Controlled Trials Register. The data were calculated using Rev Man v5.3.0. The reference lists of retrieved studies were also investigated. Results: Three articles including 576 participants which compared dutasteride with finasteride were selected for our analysis. The mean change in total hair count (mean difference [MD], 28.57; 95{\%} CI, 18.75–38.39; P{\textless}0.00001), investigator's assessment of global photographs for the vertex (MD, 0.68; 95{\%} CI, 0.13–1.23; P=0.02) and frontal (MD, 0.63; 95{\%} CI, 0.13–1.13; P=0.01) views, panel global photographic assessment for the vertex (MD, 0.17; 95{\%} CI, 0.09–0.24; P{\textless}0.00001) and frontal (MD, 0.25; 95{\%} CI, 0.18–0.31; P{\textless}0.00001) views, and subjects' assessment (MD, 0.56; 95{\%} CI, 0.18–0.94; P=0.003) suggested that dutasteride provided a better efficacy in treating men with AGA compared with finasteride. With regard to the assessment of safety, altered libido (P=0.54), erectile dysfunction (P=0.07), and ejaculation disorders (P=0.58), dutasteride did not show a significant difference compared with finasteride. Conclusion: Dutasteride seems to provide a better efficacy compared with finasteride in treating AGA. The two drugs appear to show similar rates of adverse reactions, especially in sexual dysfunction.},
author = {Zhou, Zhongbao and Song, Shiqiang and Gao, Zhenli and Wu, Jitao and Ma, Jiajia and Cui, Yuanshan},
doi = {10.2147/CIA.S192435},
issn = {1178-1998},
journal = {Clinical Interventions in Aging},
keywords = {Androgenetic alopecia,Dutasteride,Finasteride,Meta-analysis,Randomized controlled trials},
month = {feb},
pages = {399--406},
title = {{The efficacy and safety of dutasteride compared with finasteride in treating men with androgenetic alopecia: a systematic review and meta-analysis}},
url = {https://www.dovepress.com/the-efficacy-and-safety-of-dutasteride-compared-with-finasteride-in-tr-peer-reviewed-article-CIA},
volume = {Volume 14},
year = {2019}
}

@article{Zhang2019,
abstract = {With the increasing amount of waste plastics being used domestically and industrially, the disposition of those being not reusable is a challenging task. Herein, the catalytic pyrolysis of waste plastics over seven types of commercial and home-made activated carbons was studied in a facile tube reactor. A central composite experimental design was further adapted to optimize the reaction conditions and up to 100 area{\%} of the obtained liquid components belonged to jet fuel-ranged hydrocarbons, in which alkanes and aromatics accounted for 71.8{\%} and 28.2{\%}, respectively. Experiment results revealed that these activated carbons although generated via various physical and chemical activation processes could all exhibited excellent catalytic performance in converting low-density polyethylene into jet fuel and H2-enriched gases. Properties of activated carbons were also characterized by scanning electron microscope, Fourier transform infrared spectrometer, nitrogen gas adsorption, and chemical adsorption. It can be concluded that the acidity was a critical factor in determining the catalyst activity, where jet fuel-ranged alkanes and aromatics were favored by using activated carbons of weak and relevant strong acidity, respectively. Rising catalytic reaction temperature could enhance the aromatization of alkanes to increase the percentage of aromatics and release more hydrogen molecules. In addition, the production of jet fuel was also achieved from daily waste plastics, which was also confirmed by nuclear magnetic resonance analysis. The present work offers a novel route of converting waste plastics directly into transportation jet fuel.},
author = {Zhang, Yayun and Duan, Dengle and Lei, Hanwu and Villota, Elmar and Ruan, Roger},
doi = {10.1016/J.APENERGY.2019.113337},
issn = {0306-2619},
journal = {Applied Energy},
month = {oct},
pages = {113337},
publisher = {Elsevier},
title = {{Jet fuel production from waste plastics via catalytic pyrolysis with activated carbons}},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919310116?via{\%}3Dihub},
volume = {251},
year = {2019}
}

@article{Zhang2016,
abstract = {Forecasting the flow of crowds is of great importance to traffic management and public safety, yet a very challenging task affected by many complex factors, such as inter-region traffic, events and weather. In this paper, we propose a deep-learning-based approach, called ST-ResNet, to collectively forecast the in-flow and out-flow of crowds in each and every region through a city. We design an end-to-end structure of ST-ResNet based on unique properties of spatio-temporal data. More specifically, we employ the framework of the residual neural networks to model the temporal closeness, period, and trend properties of the crowd traffic, respectively. For each property, we design a branch of residual convolutional units, each of which models the spatial properties of the crowd traffic. ST-ResNet learns to dynamically aggregate the output of the three residual neural networks based on data, assigning different weights to different branches and regions. The aggregation is further combined with external factors, such as weather and day of the week, to predict the final traffic of crowds in each and every region. We evaluate ST-ResNet based on two types of crowd flows in Beijing and NYC, finding that its performance exceeds six well-know methods.},
archivePrefix = {arXiv},
arxivId = {1610.00081},
author = {Zhang, Junbo and Zheng, Yu and Qi, Dekang},
eprint = {1610.00081},
title = {{Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction}},
url = {http://arxiv.org/abs/1610.00081},
year = {2016}
}

@article{Yurichev,
author = {Yurichev, Dennis},
title = {{SAT/SMT by Example}}
}

@article{You2019,
abstract = {(Frankle {\&} Carbin, 2019) shows that there exist winning tickets (small but critical subnetworks) for dense, randomly initialized networks, that can be trained alone to achieve comparable accuracies to the latter in a similar number of iterations. However, the identification of these winning tickets still requires the costly train-prune-retrain process, limiting their practical benefits. In this paper, we discover for the first time that the winning tickets can be identified at the very early training stage, which we term as early-bird (EB) tickets, via low-cost training schemes (e.g., early stopping and low-precision training) at large learning rates. Our finding of EB tickets is consistent with recently reported observations that the key connectivity patterns of neural networks emerge early. Furthermore, we propose a mask distance metric that can be used to identify EB tickets with low computational overhead, without needing to know the true winning tickets that emerge after the full training. Finally, we leverage the existence of EB tickets and the proposed mask distance to develop efficient training methods, which are achieved by first identifying EB tickets via low-cost schemes, and then continuing to train merely the EB tickets towards the target accuracy. Experiments based on various deep networks and datasets validate: 1) the existence of EB tickets, and the effectiveness of mask distance in efficiently identifying them; and 2) that the proposed efficient training via EB tickets can achieve up to 4.7x energy savings while maintaining comparable or even better accuracy, demonstrating a promising and easily adopted method for tackling cost-prohibitive deep network training. Code available at https://github.com/RICE-EIC/Early-Bird-Tickets.},
archivePrefix = {arXiv},
arxivId = {1909.11957},
author = {You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G. and Wang, Zhangyang and Lin, Yingyan},
eprint = {1909.11957},
month = {sep},
pages = {1--13},
title = {{Drawing early-bird tickets: Towards more efficient training of deep networks}},
url = {http://arxiv.org/abs/1909.11957},
year = {2019}
}

@article{Yorgey,
author = {Yorgey, Brent and Pierce, Benjamin C and Casinghino, Chris and Gaboardi, Marco and Greenberg, Michael and Hriţcu, Cătălin and Sj{\"{o}}berg, Vilhelm},
title = {{Software Foundations}}
}

@article{Ye2020,
abstract = {Typesetting software like LATEX has accelerated scientific communication by beautifully typesetting plain- text notation, but no equivalent exists for making diagrams. We are therefore building a system called PENROSE, which allows people to create beautiful diagrams by just typing mathematical notation in plain text. Penrose aims to enable non-experts to create and explore high-quality diagrams, providing deeper insight into challenging technical concepts. Figure 1 illustrates how abstract statements about concepts in linear algebra may be automatically converted into a diagram. (I will use linear algebra as a running example, but PENROSE is designed to be extensible to any mathematical domain.) PENROSE is a highly interdisciplinary project that draws on insights from the areas of programming languages, computer graphics, and systems. The core challenge in designing the PENROSE languages is to provide customizable visual representations for concept-level expressions in an extensible collection of domains. The core challenge in designing the the numerical solver is to quickly create diagrams with user-defined shapes, objectives, and constraints so these diagrams are both beautiful and meaningful. And the core challenge of building the whole system is to bridge the two: to automatically turn the conceptual relationships defined at the language level into a concrete visual representation. Rather than focusing on a fixed set of mathematical objects, we aim to make this framework user-extensible.},
author = {Ye, Katherine},
pages = {1--4},
title = {{Creating Beautiful Diagrams Using Language}},
year = {2020}
}

@article{Yang2019,
abstract = {Humans prove theorems by relying on substantial high-level reasoning and problem-specific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, representing theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the problem of using machine learning to automate the interaction with proof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written proofs from 123 projects developed with the Coq proof assistant. We develop ASTactic, a deep learning-based model that generates tactics as programs in the form of abstract syntax trees (ASTs). Experiments show that ASTactic trained on CoqGym can generate effective tactics and can be used to prove new theorems not previously provable by automated methods. Code is available at https://github.com/princeton-vl/CoqGym.},
archivePrefix = {arXiv},
arxivId = {1905.09381},
author = {Yang, Kaiyu and Deng, Jia},
eprint = {1905.09381},
month = {may},
title = {{Learning to Prove Theorems via Interacting with Proof Assistants}},
url = {http://arxiv.org/abs/1905.09381},
year = {2019}
}

@book{Wrede2002a,
abstract = {Fortunately for you, theres Schaums Outlines. More than 40 million students have trusted Schaums to help them succeed in the classroom and on exams. Schaums is the key to faster learning and higher grades in every subject. Each Outline presents all the essential course information in an easy-to-follow, topic-by-topic format. You also get hundreds of examples, solved problems, and practice exercises to test your skills.},
author = {Wrede, Robert C and Spiegel, Murray R},
doi = {10.1111/j.1468-2982.2006.01158.x},
edition = {2},
isbn = {0071398341},
issn = {0333-1024},
pages = {356},
publisher = {McGraw Hill Professional},
title = {{Schaum's Outline of Advanced Calculus}},
year = {2002}
}

@book{Wrede2002,
abstract = {Mathematics, Calculus, Vector Calculus},
author = {Wrede, R C and Spiegel, Murray},
isbn = {0071398341},
title = {{Advanced Calculus}},
year = {2002}
}

@article{Woods,
author = {Woods, David},
pages = {1--5},
title = {{Useful LATEX Commands}}
}

@article{Wood2018,
abstract = {Differential privacy is a formal mathematical framework for quantifying and managing privacy risks. It provides provable privacy protection against a wide range of potential attacks, including those currently unforeseen. Differential privacy is primarily studied in the context of the collection, analysis, and release of aggregate statistics. These range from simple statistical estimations, such as averages, to machine learning. Tools for differentially private analysis are now in early stages of implementation and use across a variety of academic, industry, and government settings. Interest in the concept is growing among potential users of the tools, as well as within legal and policy communities, as it holds promise as a potential approach to satisfying legal requirements for privacy protection when handling personal information. In particular, differential privacy may be seen as a technical solution for analyzing and sharing data while protecting the privacy of individuals in accordance with existing legal or policy requirements for de-identification or disclosure limitation. This primer seeks to introduce the concept of differential privacy and its privacy implications to non-technical audiences. It provides a simplified and informal, but mathematically accurate, description of differential privacy. Using intuitive illustrations and limited mathematical formalism, it discusses the definition of differential privacy, how differential privacy addresses privacy risks, how differentially private analyses are constructed, and how such analyses can be used in practice. A series of illustrations is used to show how practitioners and policymakers can conceptualize the guarantees provided by differential privacy. These illustrations are also used to explain related concepts, such as composition (the accumulation of risk across multiple analyses), privacy loss parameters, and privacy budgets. This primer aims to provide a foundation that can guide future decisions when analyzing and sharing statistical data about individuals, informing individuals about the privacy protection they will be afforded, and designing policies and regulations for robust privacy protection.},
author = {Wood, Alexandra and Altman, Micah and Bembenek, Aaron and Bun, Mark and Gaboardi, Marco and Honaker, James and Nissim, Kobbi and O'Brien, David and Steinke, Thomas and Vadhan, Salil},
doi = {10.2139/ssrn.3338027},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
number = {1237235},
title = {{Differential Privacy: A Primer for a Non-Technical Audience}},
url = {https://www.ssrn.com/abstract=3338027},
year = {2018}
}

@book{Wolf2017,
author = {Wolf, Dave and Henley, A J},
booktitle = {Java EE Web Application Primer},
doi = {10.1007/978-1-4842-3195-1},
isbn = {9781484231944},
title = {{Java EE Web Application Primer}},
year = {2017}
}

@techreport{Winitzki,
author = {Winitzki, Sergei},
keywords = {Category theory,Formal logic,Functional programming,Programming languages,Type theory},
title = {{The Science of Functional Programming: A Tutorial, with Examples in Scala}}
}

@article{Williams2013,
author = {Williams, Tim},
number = {December},
title = {{Haskell at Barclays : Exotic tools for exotic trades}},
year = {2013}
}

@article{Williams2010,
author = {Williams, Michael Peretzian},
journal = {Johns Hopkins Apl Technical Digest},
number = {4},
pages = {354--363},
title = {{Solving Polynomial Equations Using Linear Algebra}},
url = {https://www.jhuapl.edu/Content/techdigest/pdf/V28-N04/28-04-Williams.pdf},
volume = {28},
year = {2010}
}

@misc{Wikipedia,
author = {Wikipedia},
booktitle = {Wikipedia},
title = {{Programming paradigms}}
}

@book{Wikibooks2016,
author = {Wikibooks},
pages = {620},
publisher = {Wikibooks},
title = {{Haskell}},
year = {2016}
}

@article{Wiegley2017,
abstract = {Correctness and performance are ooen at odds in the of sys-tems engineering, either because correct programs are too costly to write or impractical to execute, or because well-performing code involves so many tricks of the trade that formal analysis is unable to isolate the main properties of the algorithm. As a prime example of this tension, Coq is an established proof environment that allows writing correct, dependently-typed code, but it has been criticized for exorbitant development times, forcing the developer to choose between optimal code or tractable proofs. On the other side of the divide, Haskell has proven itself to be a capable, well-typed programming environment, yet easy-to-read, straightforward code must all too ooen be replaced by highly opti-mized variants that obscure the author's original intention. paper builds on the existing Fiat reenement framework to bridge this divide, demonstrating how to derive a correct-by-construction implementation that meets (or exceeds) the perfor-mance characteristics of highly optimized Haskell, starting from a high-level Coq speciication. To achieve this goal, we extend Fiat with a stateful notion of reenement of abstract data types and add support for extracting stateful code via a free monad equipped with an algebra of heap-manipulating operations. As a case study, we reimplement a subset of the popular bytestring library, with liile to no loss of performance, while retaining a high guarantee of program correctness.},
author = {Wiegley, John and Delaware, Benjamin},
doi = {10.1145/3156695.3122962},
isbn = {9781450351829},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {2017,Performant Certified Software,Stepwise Refinement,acm reference format,acm sigplan international haskell,correct haskell,delaware,fast and,in proceedings of 10th,john wiegley and benjamin,performant certified software,stepwise refinement,using coq to write},
number = {10},
pages = {52--62},
title = {{Using Coq to write fast and correct Haskell}},
volume = {52},
year = {2017}
}

@article{Wiegley,
author = {Wiegley, John},
title = {{Ledger : Command-Line Accounting}}
}

@article{Werner1986,
abstract = {Iterative methods for the solution of nonlinear systems of equations such as Newton's method are usually based on local linearizations; iterative procedures which locally use quadratic approximations of the functions whose zero is to be determined are studied. Our approach yields fourth order iterative methods which are more efficient than their classical counterparts such as Chebyshev's and Halley's method. Iterative methods of the kind discussed here are from a practical point of view certainly not as important as Newton-like methods; they may be appropriate, however, for special types of nonlinear problems. {\textcopyright}1986.},
author = {Werner, Wilhelm},
doi = {10.1016/0898-1221(86)90192-6},
issn = {08981221},
journal = {Computers {\&} Mathematics with Applications},
month = {mar},
number = {3},
pages = {331--343},
title = {{Iterative Solution of Systems of Nonlinear Equations Based Upon Quadratic Approximations}},
url = {https://linkinghub.elsevier.com/retrieve/pii/0898122186901926},
volume = {12},
year = {1986}
}

@article{Werner1985,
author = {Werner, W},
pages = {331--343},
title = {{Iterative Solution of Systems of Nonlinear Equations Based Upon Quadratic Approximations}},
year = {1985}
}

@incollection{Wenzel1999,
author = {Wenzel, Markus},
doi = {10.1007/3-540-48256-3_12},
pages = {167--183},
title = {{Isar — A Generic Interpretative Approach to Readable Formal Proof Documents}},
url = {http://link.springer.com/10.1007/3-540-48256-3{\_}12},
year = {1999}
}

@article{Wenzel2002,
author = {Wenzel, Markus},
journal = {undefined},
title = {{Isabelle, Isar - a versatile environment for human readable formal proof documents}},
url = {https://www.semanticscholar.org/paper/Isabelle{\%}2C-Isar-a-versatile-environment-for-human-Wenzel/4dbcafadae534f8d41b06fe87b7a9a9120459a0b{\#}paper-header},
year = {2002}
}

@article{Weitz2017,
author = {Weitz, Konstantin and Lyubomirsky, Steven and Heule, Stefan and Torlak, Emina and Ernst, Michael D and Tatlock, Zachary},
doi = {10.1145/3110269},
issn = {24751421},
journal = {Proceedings of the ACM on Programming Languages},
keywords = {Bagpipe,Coq,SMT solver-aided tools,SaltShaker},
number = {ICFP},
pages = {1--28},
publisher = {ACM},
title = {{SpaceSearch: a library for building and verifying solver-aided tools}},
url = {http://dl.acm.org/citation.cfm?doid=3136534.3110269},
volume = {1},
year = {2017}
}

@book{Weissman2009,
author = {Weissman, Martin H},
title = {{Number Theory}},
year = {2009}
}

@book{Weidman2014,
author = {Weidman, Georgia},
isbn = {1593275641},
publisher = {William Pollock},
title = {{Penetration Testing: A Hands-on Introduction To Hacking}},
year = {2014}
}

@article{Wang2018,
abstract = {Deep learning has seen tremendous success over the past decade in computer vision, machine translation, and gameplay. This success rests in crucial ways on gradient-descent optimization and the ability to learn parameters of a neural network by backpropagating observed errors. However, neural network architectures are growing increasingly sophisticated and diverse, which motivates an emerging quest for even more general forms of differentiable programming, where arbitrary parameterized computations can be trained by gradient descent. In this paper, we take a fresh look at automatic differentiation (AD) techniques, and especially aim to demystify the reverse-mode form of AD that generalizes backpropagation in neural networks. We uncover a tight connection between reverse-mode AD and delimited continuations, which permits implementing reverse-mode AD purely via operator overloading and without any auxiliary data structures. We further show how this formulation of AD can be fruitfully combined with multi-stage programming (staging), leading to a highly efficient implementation that combines the performance benefits of deep learning frameworks based on explicit reified computation graphs (e.g., TensorFlow) with the expressiveness of pure library approaches (e.g., PyTorch).},
archivePrefix = {arXiv},
arxivId = {1803.10228},
author = {Wang, Fei and Zheng, Daniel and Decker, James and Wu, Xilun and Essertel, Gr{\'{e}}gory M and Rompf, Tiark},
eprint = {1803.10228},
journal = {Proceedings of the ACM on Programming Languages},
month = {mar},
number = {ICFP},
pages = {1--31},
publisher = {Association for Computing Machinery (ACM)},
title = {{Demystifying Differentiable Programming: Shift/Reset the Penultimate Backpropagator}},
url = {http://arxiv.org/abs/1803.10228},
volume = {3},
year = {2018}
}

@article{Walton1994,
abstract = {The paths of cutting tools used in a computer-aided manufacturing environment are usually described by means of circular arcs and straight line segments. In a computer aided design environment, however, objects are often designed using B-splines or B{\'{e}}zier curves. This paper develops a simple technique to find an arbitrarily close approximation to a quadratic B{\'{e}}zier curve by a G1curve consisting of circular arcs. {\textcopyright}1994.},
author = {Walton, D J and Meek, D S},
doi = {10.1016/0377-0427(94)90398-0},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Approximation,Arc splines,B{\'{e}}zier curves},
number = {1},
pages = {107--120},
title = {{Approximation of quadratic B{\'{e}}zier curves by arc splines}},
volume = {54},
year = {1994}
}

@book{Wallwork2016,
abstract = {Publishing your research in an international journal is key to your success in academia. This guide is based on a study of over 1000 manuscripts and reviewers' reports revealing why papers written by non-native researchers are often rejected due to problems with English usage and poor structure and content. With easy-to-follow rules and tips, and examples taken from published and unpublished papers, you will learn how to: prepare and structure a manuscript increase readability and reduce the number of mistakes you make in English by writing concisely, with no redundancy and no ambiguity write a title and an abstract that will attract attention and be read decide what to include in the various parts of the paper (Introduction, Methodology, Discussion etc) highlight your claims and contribution avoid plagiarism discuss the limitations of your research choose the correct tenses and style satisfy the requirements of editors and reviewers This new edition contains over 40{\%} new material, including two new chapters, stimulating factoids, and discussion points both for self-study and in-class use. EAP teachers will find this book to be a great source of tips for training students, and for preparing both instructive and entertaining lessons. Other books in the series cover: presentations at international conferences; academic correspondence; English grammar, usage and style; interacting on campus, plus exercise books and a teacher's guide to the whole series. Please visit http://www.springer.com/series/13913 for a full list of titles in the series. Adrian Wallwork is the author of more than 30 ELT and EAP textbooks. He has trained several thousand PhD students and academics from 35 countries to write research papers, prepare presentations, and communicate with editors, referees and fellow researchers.},
address = {Cham},
author = {Wallwork, Adrian},
booktitle = {English for Writing Research Papers},
doi = {10.1007/978-3-319-26094-5},
edition = {2},
isbn = {978-3-319-26092-1},
pages = {377},
publisher = {Springer International Publishing},
title = {{English for Writing Research Papers}},
url = {http://link.springer.com/10.1007/978-3-319-26094-5},
year = {2016}
}

@book{Walck1996,
author = {Walck, Christian},
pages = {202},
title = {{Hand-book on statistical distributions for experimentalists}},
url = {http://inspirehep.net/record/1389910/files/suf9601.pdf},
year = {1996}
}

@inproceedings{Wadler1989a,
abstract = {From the type of a polymorphic function we can derive a theorem that it satisfies. Every function of the same type satisfies the same theorem. This provides a free source of useful theorems, courtesy of Reynolds' abstraction theorem for the polymorphic lambda calculus.},
address = {New York, New York, USA},
author = {Wadler, Philip},
booktitle = {Proceedings of the fourth international conference on Functional programming languages and computer architecture - FPCA '89},
doi = {10.1145/99370.99404},
isbn = {0897913280},
pages = {347--359},
publisher = {ACM Press},
title = {{Theorems for free!}},
url = {http://portal.acm.org/citation.cfm?doid=99370.99404},
year = {1989}
}

@techreport{Voelker2010,
author = {Voelker, Karl and Advisor, Graduate and Bischof, Hans-Peter},
title = {{Practical Programming with Total Functions}},
year = {2010}
}

@article{Virine,
abstract = {This paper is a detailed description of event chain methodology: schedule network analysis and an uncertainty modeling technique for project management. Event chain methodology focuses on identifying and managing the events and event chains that affect projects. Event chain methodology improves the accuracy of project planning simplifying the modeling and analysis of uncertainties in the project schedules. As a result, it helps to mitigate the negative impact of cognitive and motivational biases related to project planning.},
author = {Virine, Lev and Trumper, Michael},
journal = {Intaver Institute},
number = {1974},
title = {{Event Chain Methodology In Details}},
url = {http://www.projectdecisions.org/paper/Paper{\_}EventChainMeethodology.pdf}
}

@article{VictoriaLin2017a,
abstract = {Oftentimes, a programmer may have diiculty implementing a desired operation. Even when the programmer can describe her goal in English, it can be diicult to translate into code. Existing resources, such as question-and-answer websites, tabulate speciic operations that someone has wanted to perform in the past, but they are not eeective in generalizing to new tasks, to compound tasks that require combining previous questions, or sometimes even to variations of listed tasks. Our goal is to make programming easier and more productive by leeing programmers use their own words and concepts to express the intended operation, rather than forcing them to accommodate the machine by memorizing its grammar. We have built a system that lets a programmer describe a desired operation in natural language , then automatically translates it to a programming language for review and approval by the programmer. Our system, Tellina, does the translation using recurrent neural networks (RNNs), a state-of-the-art natural language processing technique that we augmented with slot (argument))lling and other enhancements. We evaluated Tellina in the context of shell scripting. We trained Tellina's RNNs on textual descriptions of fle system operations and bash one-liners, scraped from the web. Although recovering completely correct commands is challenging, Tellina achieves top-3 accuracy of 80{\%} for producing the correct command structure. In a controlled study, programmers who had access to Tellina outper-formed those who did not, even when Tellina's predictions were not completely correct, to a statistically signiicant degree.},
author = {{Victoria Lin}, Xi and Wang, Chenglong and Pang, Deric and Vu, Kevin and Zeelemoyer, Luke and Ernst, Michael D},
journal = {Lrec},
pages = {440--450},
title = {{Program Synthesis from Natural Language Using Recurrent Neural Networks}},
volume = {1},
year = {2017}
}

@article{Verdon2017,
abstract = {The question has remained open if near-term gate model quantum computers will offer a quantum advantage for practical applications in the pre-fault tolerance noise regime. A class of algorithms which have shown some promise in this regard are the so-called classical-quantum hybrid variational algorithms. Here we develop a low-depth quantum algorithm to train quantum Boltzmann machine neural networks using such variational methods. We introduce a method which employs the quantum approximate optimization algorithm as a subroutine in order to approximately sample from Gibbs states of Ising Hamiltonians. We use this approximate Gibbs sampling to train neural networks for which we demonstrate training convergence for numerically simulated noisy circuits with depolarizing errors of rates of up to 4{\%}.},
archivePrefix = {arXiv},
arxivId = {1712.05304},
author = {Verdon, Guillaume and Broughton, Michael and Biamonte, Jacob},
eprint = {1712.05304},
pages = {1--8},
title = {{A quantum algorithm to train neural networks using low-depth circuits}},
url = {http://arxiv.org/abs/1712.05304},
year = {2017}
}

@article{Vega2006,
abstract = {Most of the available digital color cameras use a single image sensor with a color filter array (CFA) in acquiring an image. In order to produce a visible color image, a demosaicing process must be applied, which produces undesirable artifacts. An additional problem appears when the observed color image is also blurred. This paper addresses the problem of deconvolving color images observed with a single coupled charged device (CCD) from the super-resolution point of view. Utilizing the Bayesian paradigm, an estimate of the reconstructed image and the model parameters is generated. The proposed method is tested on real images.},
author = {Vega, Miguel and Molina, Rafael and Katsaggelos, Aggelos K},
doi = {10.1155/ASP/2006/25072},
issn = {11108657},
journal = {Eurasip Journal on Applied Signal Processing},
pages = {1--12},
title = {{A bayesian super-resolution approach to demosaicing of blurred images}},
volume = {2006},
year = {2006}
}

@book{Vazirani2006,
abstract = {Look around you. Computers and networks are everywhere, enabling an intricate web ofcom- plex human activities: education, commerce, entertainment, research, manufacturing, health management, human communication, even war. Ofthe two main technological underpinnings of this amazing proliferation, one is obvious: the breathtaking pace with which advances in microelectronics and chip design have been bringing us faster and faster hardware. This book tells the story of the other intellectual enterprise that is crucially fueling the computer revolution: efficient algorithms. It is a fascinating story.},
author = {Vazirani, U V},
pages = {318},
title = {{Algorithms}},
year = {2006}
}

@article{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
archivePrefix = {arXiv},
arxivId = {1706.03762},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Attention is all you need}},
year = {2017}
}

@article{Vartak2017,
abstract = {Data visualization is often used as the first step while performing a variety of analytical tasks. With the advent of large, high-dimensional datasets and strong interest in data science, there is a need for tools that can support rapid visual analysis. In this paper we describe our vision for a new class of visualization recommendation systems that can automatically identify and interactively recommend visualizations relevant to an analytical task.},
author = {Vartak, Manasi and Huang, Silu and Siddiqui, Tarique and Madden, Samuel and Parameswaran, Aditya},
doi = {10.1145/3092931.3092937},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {4},
pages = {34--39},
title = {{Towards Visualization Recommendation Systems}},
volume = {45},
year = {2017}
}

@book{Vugt2009,
abstract = {This book is for anyone who wants to master Linux from the command line. When writing it, I had in mind system administrators, software developers, and enthusiastic users who want to get things going from the Linux command line. For beginning users, this may be a daunting task, as Linux commands often have many options documented only in pages that are not that easy to understand. This book is distribution agnostic. That is, while writing it, I've checked all items against Ubuntu, Red Hat, and SUSE. Since most distributions are quite similar to one of these three, this book should help you with other distributions as well. There is only one item in the book that is not distribution agnostic: the Appendix, which explains how to install OpenSUSE. I've chosen to cover installation of just one distribution, because if you don't have any Linux installed yet, you probably don't care what you install. If you do care what distribution to work with, you probably have it installed already.},
author = {van Vugt, Sander},
isbn = {1430218908},
keywords = {administrator ar+o archive arguments authenticatio},
mendeley-tags = {administrator ar+o archive arguments authenticatio},
pages = {392},
publisher = {Apress},
title = {{Beginning the Linux Command Line}},
year = {2009}
}

@article{VanKrieken2019,
abstract = {We introduce Differentiable Reasoning (DR), a novel semi-supervised learning technique which uses relational background knowledge to benefit from unlabeled data. We apply it to the Semantic Image Interpretation (SII) task and show that background knowledge provides significant improvement. We find that there is a strong but interesting imbalance between the contributions of updates from Modus Ponens (MP) and its logical equivalent Modus Tollens (MT) to the learning process, suggesting that our approach is very sensitive to a phenomenon called the Raven Paradox. We propose a solution to overcome this situation.},
archivePrefix = {arXiv},
arxivId = {1908.04700},
author = {van Krieken, Emile and Acar, Erman and van Harmelen, Frank},
eprint = {1908.04700},
title = {{Semi-Supervised Learning using Differentiable Reasoning}},
url = {http://arxiv.org/abs/1908.04700},
year = {2019}
}

@article{VanGestel1991,
abstract = {This book contains a thorough introduction to type theory, with information on polymorphic sets, subsets, monomorphic sets, and a full set ofhelpful examples.},
author = {{Van Gestel}, E},
doi = {10.1016/0377-0427(91)90052-l},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
number = {2},
pages = {N4},
title = {{Programming in Martin-L{\"{o}}f's Type Theory: an Introduction}},
volume = {34},
year = {1991}
}

@article{VanDyk2001a,
abstract = {The term data augmentation refers to methods for constructing iterative optimization or sampling algorithms via the introductionof unobserved data or latent variables. For de- terministic algorithms, the method was popularizedin the general statistical community by the seminal article by Dempster, Laird, and Rubin on the EM algorithm for maximizing a likelihood function or, more generally, a posterior density. For stochastic algorithms, the method was popularized in the statistical literature by Tanner and Wong's Data Augmenta- tion algorithmfor posterior sampling and in the physics literatureby Swendsen andWang's algorithm for sampling from the Ising and Potts models and their generalizations; in the physics literature, the method of data augmentationis referred to as the method of auxiliary variables. Data augmentationschemes were used by Tanner and Wong to make simulation feasible and simple, while auxiliary variables were adopted by Swendsen and Wang to im- prove the speedof iterativesimulation.In general,however, constructingdata augmentation schemes that result in both simple and fast algorithms is a matter of art in that successful strategiesvary greatlywith the (observed-data)models being considered.After an overview ofdata augmentation/auxiliaryvariablesand some recent developmentsin methods for con- structingsuchef?cientdataaugmentationschemes,we introduceaneffectivesearch strategy that combines the ideas of marginal augmentationand conditionalaugmentation, together with a deterministic approximationmethod for selecting good augmentation schemes. We then apply this strategy to three common classes of models (speci?cally, multivariate t, probit regression,and mixed-effectsmodels) to obtain ef?cient Markov chainMonte Carlo algorithms for posterior sampling. We provide theoretical and empirical evidence that the resulting algorithms, while requiring similar programming effort, can show dramatic im- provement over the Gibbs samplers commonly used for these models in practice. A key featureofall thesenewalgorithmsis that they arepositiverecurrentsubchainsofnonpositive recurrent Markov chains constructed in larger spaces.},
author = {van Dyk, David A and Meng, Xiao-Li},
doi = {10.1198/10618600152418584},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Auxiliary variables,Conditional augmentation,EM algorithm,Gibbs sampler,Haar measure,Hierarchical models,Marginal augmentation,Markov chain Monte Carlo,Mixed-effects models,Nonpositive recurrent Markov chain,Posterior distributions,Probit regression,Rate of convergence.},
month = {mar},
number = {1},
pages = {1--50},
title = {{The Art of Data Augmentation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/10618600152418584},
volume = {10},
year = {2001}
}

@article{VandenBurg2019,
abstract = {Data scientists spend the majority of their time on preparing data for analysis. One of the first steps in this preparation phase is to load the data from the raw storage format. Comma-separated value (CSV) files are a popular format for tabular data due to their simplicity and ostensible ease of use. However, formatting standards for CSV files are not followed consistently, so each file requires manual inspection and potentially repair before the data can be loaded, an enormous waste of human effort for a task that should be one of the simplest parts of data science. The first and most essential step in retrieving data from CSV files is deciding on the dialect of the file, such as the cell delimiter and quote character. Existing dialect detection approaches are few and non-robust. In this paper, we propose a dialect detection method based on a novel measure of data consistency of parsed data files. Our method achieves 97{\%} overall accuracy on a large corpus of real-world CSV files and improves the accuracy on messy CSV files by almost 22{\%} compared to existing approaches, including those in the Python standard library. Our measure of data consistency is not specific to the data parsing problem, and has potential for more general applicability.},
archivePrefix = {arXiv},
arxivId = {1811.11242},
author = {van den Burg, G J J and Naz{\'{a}}bal, A and Sutton, C},
doi = {10.1007/s10618-019-00646-y},
eprint = {1811.11242},
issn = {1573756X},
journal = {Data Mining and Knowledge Discovery},
keywords = {Comma separated values,Data parsing,Data wrangling},
month = {nov},
number = {6},
pages = {1799--1820},
publisher = {Springer New York LLC},
title = {{Wrangling messy CSV files by detecting row and type patterns}},
volume = {33},
year = {2019}
}

@inproceedings{Vale2008,
abstract = {We are engaged in a project named Mathematics and patterns in elementary schools: perspectives and classroom experiences of students and teachers. Our aim is to analyze the impact of an intervention centered on the study of patterns in the learning of mathematics concepts and on the development of communication and development of higher order thinking skills. In this paper we present part of an ongoing research with pre-service teachers concerning the development of teachers' algebraic thinking, in particular how they move through pattern tasks involving generalization. We will present some of the tasks used in the didactical experience and some preliminary conclusions of its implementation in the mathematics didactics classes of a mathematics elementary teachers' course of a School of Education.},
author = {Vale, Isabel and Cabrita, Isabel},
booktitle = {ETEN},
pages = {63--69},
title = {{Learning through patterns: a powerful approach to algebraic thinking}},
url = {https://sahlgrenska.gu.se/digitalAssets/1292/1292475{\_}ETEN{\_}Proceedings{\_}18.pdf{\#}page=73},
year = {2008}
}

@article{Ungar1991,
author = {Ungar, David},
title = {{Organizing Programs Without Classes}},
year = {1991}
}

@article{Turner2018,
abstract = {We introduce the Metropolis-Hastings generative adversarial network (MH-GAN), which combines aspects of Markov chain Monte Carlo and GANs. The MH-GAN draws samples from the distribution implicitly defined by a GAN's discriminator-generator pair, as opposed to sampling in a standard GAN which draws samples from the distribution defined by the generator. It uses the discriminator from GAN training to build a wrapper around the generator for improved sampling. With a perfect discriminator, this wrapped generator samples from the true distribution on the data exactly even when the generator is imperfect. We demonstrate the benefits of the improved generator on multiple benchmark datasets, including CIFAR-10 and CelebA, using DCGAN and WGAN.},
archivePrefix = {arXiv},
arxivId = {1811.11357},
author = {Turner, Ryan and Hung, Jane and Saatci, Yunus and Yosinski, Jason},
eprint = {1811.11357},
number = {NeurIPS},
pages = {1--10},
title = {{Metropolis-Hastings Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1811.11357},
year = {2018}
}

@article{Turek1999,
abstract = {Volumes of data used in science and industry are growing rapidly. When researchers face the challenge of analyzing them, their format is often the first obstacle. Lack of standardized ways of exploring different data layouts requires an effort each time to solve the problem from scratch. Possibility to access data in a rich, uniform manner, e.g. using Structured Query Language (SQL) would offer expressiveness and user-friendliness. Comma-separated values (CSV) are one of the most common data storage formats. Despite its simplicity, with growing file size handling it becomes non-trivial. Importing CSVs into existing databases is time-consuming and troublesome, or even impossible if its horizontal dimension reaches thousands of columns. Most databases are optimized for handling large number of rows rather than columns, therefore, performance for datasets with non-typical layouts is often unacceptable. Other challenges include schema creation, updates and repeated data imports. To address the above-mentioned problems, I present a system for accessing very large CSV-based datasets by means of SQL. It's characterized by: "no copy" approach - data stay mostly in the CSV files; "zero configuration" - no need to specify database schema; written in C++, with boost [1], SQLite [2] and Qt [3], doesn't require installation and has very small size; query rewriting, dynamic creation of indices for appropriate columns and static data retrieval directly from CSV files ensure efficient plan execution; effortless support for millions of columns; due to per-value typing, using mixed text/numbers data is easy; very simple network protocol provides efficient interface for MATLAB and reduces implementation time for other languages. The software is available as freeware along with educational videos on its website [4]. It doesn't need any prerequisites to run, as all of the libraries are included in the distribution package. I test it against existing database solutions using a battery of benchmarks and discuss the results. {\textcopyright}2014 Stanislaw Adaszewski.},
author = {Turek, Stefan},
doi = {10.1007/978-3-642-58393-3_3},
pages = {97--280},
title = {{Other mathematical components}},
year = {1999}
}

@book{Tuncay2008,
abstract = {The time evolution of Earth with her cities, languages and countries is considered in terms of the multiplicative noise1 and the fragmentation processes, where the related families, size distributions, lifetimes, bilinguals, etc. are studied. Earlier we treated the cities and the languages differently (and as connected; languages split since cities split, etc.). Hence, two distributions are obtained in the same computation at the same time. The same approach is followed here and ParetoZipf law for the distribution of the cities, log-normal for the languages, decreasing exponential for the city families (countries) in the rank order over population, and power law -2 for the language families over the number of languages in rank order are obtained theoretically in this combination for the first time (up to our knowledge) in the literature; all of which are in good agreement with the present empirical data. {\textcopyright}2008 World Scientific Publishing Company.},
archivePrefix = {arXiv},
arxivId = {0710.2023},
author = {Tuncay, {\c{C}}ǎlar},
booktitle = {International Journal of Modern Physics C},
doi = {10.1142/S0129183108012261},
eprint = {0710.2023},
isbn = {9780133591620},
issn = {01291831},
keywords = {Cities,Families,Languages},
number = {3},
pages = {471--484},
title = {{Model of world: Her cities, languages and countries}},
volume = {19},
year = {2008}
}

@article{Tsou,
author = {Tsou, Don-Min},
title = {{Decomposition of a Relation Scheme Into Boyce-Codd Normal Form}}
}

@article{Trimeche2006,
abstract = {One critical aspect to achieve efficient implementations of image super-resolution is the need for accurate subpixel registration of the input images. The overall performance of super-resolution algorithms is particularly degraded in the presence of persistent outliers, for which registration has failed. To enhance the robustness of processing against this problem, we propose in this paper an integrated adaptive filtering method to reject the outlier image regions. In the process of combining the gradient images due to each low-resolution image, we use adaptive FIR filtering. The coefficients of the FIR filter are updated using the LMS algorithm, which automatically isolates the outlier image regions by decreasing the corresponding coefficients. The adaptation criterion of the LMS estimator is the error between the median of the samples from the LR images and the output of the FIR filter. Through simulated experiments on synthetic images and on real camera images, we show that the proposed technique performs well in the presence of motion outliers. This relatively simple and fastmechanism enables to add robustness in practical implementations of image super-resolution, while still being effective against Gaussian noise in the image formation model.},
author = {Trimeche, Mejdi and Bilcu, Radu Ciprian and Yrj{\"{a}}n{\"{a}}inen, Jukka},
doi = {10.1155/ASP/2006/38052},
issn = {11108657},
journal = {Eurasip Journal on Applied Signal Processing},
pages = {1--12},
title = {{Adaptive outlier rejection in image super-resolution}},
volume = {2006},
year = {2006}
}

@article{TranconyWidemann2016,
abstract = {We use the concept of laminar flow, as opposed to turbulent flow, as a metaphor for the decomposition of well-behaved purely functional data-flow programs into largely independent parts, necessitated by aspects with different execution constraints. In the context of the total functional data-flow language Sig, we identify three distinct but methodologically related implementation challenges, namely multirate scheduling, declarative initialization, and conditional execution, and demonstrate how they can be solved orthogonally, by decomposition using the standard program transformation technique, slicing.},
author = {{Tranc{\'{o}}n y Widemann}, Baltasar and Lepper, Markus},
doi = {10.1007/978-3-319-39110-6_5},
isbn = {9783319391090},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
title = {{Laminar data flow: On the role of slicing in functional data-flow programming}},
year = {2016}
}

@article{TranconyWidemann2014,
abstract = {The field of declarative stream programming (discrete time, clocked synchronous, modular, data-centric) is divided between the data-flow graph paradigm favored by domain experts, and the functional reactive paradigm favored by academics. In this paper, we describe the foundations of a framework for unifying functional and data-flow styles that differs from FRP proper in significant ways: It is based on set theory to match the expectations of domain experts, and the two paradigms are reduced symmetrically to a low-level middle ground, with strongly compositional semantics. The design of the framework is derived from mathematical first principles, in particular coalgebraic coinduction and a standard relational model of stateful computation. The abstract syntax and semantics introduced here constitute the full core of a novel stream programming language.},
author = {{Tranc{\'{o}}n y Widemann}, Baltasar and Lepper, Markus},
doi = {10.4204/eptcs.153.10},
journal = {Electronic Proceedings in Theoretical Computer Science},
keywords = {coinduction,data flow,stream programming,total functions},
number = {Msfp},
pages = {143--167},
title = {{Foundations of Total Functional Data-Flow Programming}},
volume = {153},
year = {2014}
}

@article{Tran2017,
abstract = {Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm — generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above — the results also show that our approach produces better classification results than similar GAN models.},
archivePrefix = {arXiv},
arxivId = {1710.10564},
author = {Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
eprint = {1710.10564},
number = {Nips},
pages = {1--10},
title = {{A Bayesian Data Augmentation Approach for Learning Deep Models}},
url = {http://arxiv.org/abs/1710.10564},
year = {2017}
}

@book{Tran2019,
author = {Tran, Angela and Wertz, Boris},
title = {{Startup Handbook}},
year = {2019}
}

@techreport{TranVui2010,
author = {{Tran Vui}, Assoc},
pages = {7--12},
title = {{A Combined Abduction-Induction Strategy in Teaching Mathematics to Gifted Students-with-Computers through Dynamic Representation}},
year = {2010}
}

@article{Tizno2019,
abstract = {Whilst the different forms of conventional (charge-based) memories are well suited to their individual roles in computers and other electronic devices, flaws in their properties mean that intensive research into alternative, or emerging, memories continues. In particular, the goal of simultaneously achieving the contradictory requirements of non-volatility and fast, low-voltage (low-energy) switching has proved challenging. Here, we report an oxide-free, floating-gate memory cell based on III-V semiconductor heterostructures with a junctionless channel and non-destructive read of the stored data. Non-volatile data retention of at least 104 s in combination with switching at ≤2.6 V is achieved by use of the extraordinary 2.1 eV conduction band offsets of InAs/AlSb and a triple-barrier resonant tunnelling structure. The combination of low-voltage operation and small capacitance implies intrinsic switching energy per unit area that is 100 and 1000 times smaller than dynamic random access memory and Flash respectively. The device may thus be considered as a new emerging memory with considerable potential.},
author = {Tizno, Ofogh and Marshall, Andrew R J and Fern{\'{a}}ndez-Delgado, Natalia and Herrera, Miriam and Molina, Sergio I and Hayne, Manus},
doi = {10.1038/s41598-019-45370-1},
issn = {2045-2322},
journal = {Scientific Reports},
keywords = {Electrical and electronic engineering,Electronics,Semiconductors,photonics and device physics},
number = {1},
pages = {8950},
publisher = {Nature Publishing Group},
title = {{Room-temperature Operation of Low-voltage, Non-volatile, Compound-semiconductor Memory Cells}},
url = {http://www.nature.com/articles/s41598-019-45370-1},
volume = {9},
year = {2019}
}

@article{Thompson2018,
abstract = {Causal asymmetry is one of the great surprises in predictive modelling: the memory required to predict the future differs from the memory required to retrodict the past. There is a privileged temporal direction for modelling a stochastic process where memory costs are minimal. Models operating in the other direction incur an unavoidable memory overhead. Here we show that this overhead can vanish when quantum models are allowed. Quantum models forced to run in the less natural temporal direction not only surpass their optimal classical counterparts, but also any classical model running in reverse time. This holds even when the memory overhead is unbounded, resulting in quantum models with unbounded memory advantage.},
author = {Thompson, Jayne and Garner, Andrew J P and Mahoney, John R and Crutchfield, James P and Vedral, Vlatko and Gu, Mile},
doi = {10.1103/PhysRevX.8.031013},
issn = {21603308},
journal = {Physical Review X},
keywords = {doi:10.1103/PhysRevX.8.031013 url:https://doi.org/,quantum physics},
number = {3},
pages = {31013},
publisher = {American Physical Society},
title = {{Causal Asymmetry in a Quantum World}},
url = {https://doi.org/10.1103/PhysRevX.8.031013},
volume = {8},
year = {2018}
}

@article{Thesis2012,
author = {Thesis, Bachelor},
pages = {1--244},
title = {{Holmes-New-Foundations}},
url = {papers3://publication/uuid/D889ADB1-420C-4E17-AB49-4FFC982997E5},
year = {2012}
}

@article{Tang2004,
abstract = {NV -},
author = {Tang, Jonathan},
keywords = {Haskell,Programming,Scheme interpreter},
pages = {130},
title = {{Write Yourself a Scheme in 48 Hours}},
url = {https://www.google.com/patents/US6691155{\%}5Cnhttps://upload.wikimedia.org/wikipedia/commons/a/aa/Write{\_}Yourself{\_}a{\_}Scheme{\_}in{\_}48{\_}Hours.pdf},
year = {2004}
}

@article{Tang2007,
author = {Tang, Jonathan},
keywords = {Haskell,Programming,Scheme interpreter},
pages = {138},
title = {{Write Yourself a Scheme in 48 Hours: An Introduction to Haskell through Example}},
url = {https://upload.wikimedia.org/wikipedia/commons/a/aa/Write{\_}Yourself{\_}a{\_}Scheme{\_}in{\_}48{\_}Hours.pdf{\%}0Ahttps://www.google.com/patents/US6691155{\%}5Cnhttps://upload.wikimedia.org/wikipedia/commons/a/aa/Write{\_}Yourself{\_}a{\_}Scheme{\_}in{\_}48{\_}Hours.pdf},
year = {2007}
}

@book{Tanenbaum2012,
abstract = {Structured Computer Organization, specifically written for undergraduate students, is a best-selling guide that provides an accessible introduction to computer hardware and architecture. This text will also serve as a useful resource for all computer professionals and engineers who need an overview or introduction to computer architecture. This book takes a modern structured, layered approach to understanding computer systems. It's highly accessible - and it's been thoroughly updated to reflect today's most critical new technologies and the latest developments in computer organization and architecture. Tanenbaum's renowned writing style and painstaking research make this one of the most accessible and accurate books available, maintaining the author's popular method of presenting a computer as a series of layers, each one built upon the ones below it, and understandable as a separate entity.},
author = {Tanenbaum, Andrew S and Austin, Todd},
edition = {6},
isbn = {9780133061796},
pages = {800},
publisher = {Pearson Education},
title = {{Structured Computer Organization}},
year = {2012}
}

@book{Tanenbaum2008,
abstract = {The widely anticipated revision of this worldwide best-seller incorporates the latest developments in operating systems (OS)technologies. The Third Edition includes up-to-date materials on relevant. OS such as Linux, Windows, and embedded real-time and multimedia systems. Tanenbaum also provides information on current research based on his experience as an operating systems researcher.},
author = {Tanenbaum, Andrew S},
edition = {3},
isbn = {9780135013014},
pages = {1076},
publisher = {Pearson College Division},
title = {{Modern Operating Systems}},
year = {2008}
}

@article{Tanaka2019,
abstract = {In this paper we propose the use of Generative Adversarial Networks (GAN) to generate artificial training data for machine learning tasks. The generation of artificial training data can be extremely useful in situations such as imbalanced data sets, performing a role similar to SMOTE or ADASYN. It is also useful when the data contains sensitive information, and it is desirable to avoid using the original data set as much as possible (example: medical data). We test our proposal on benchmark data sets using different network architectures, and show that a Decision Tree (DT) classifier trained using the training data generated by the GAN reached the same, (and surprisingly sometimes better), accuracy and recall than a DT trained on the original data set.},
archivePrefix = {arXiv},
arxivId = {1904.09135},
author = {Tanaka, Fabio Henrique Kiyoiti dos Santos and Aranha, Claus},
eprint = {1904.09135},
keywords = {data augmentation,data imbalance,generative adversarial networks,pri-},
pages = {1--16},
title = {{Data Augmentation Using GANs}},
url = {http://arxiv.org/abs/1904.09135},
volume = {2019},
year = {2019}
}

@article{THooft2012,
abstract = {General relativity is a beautiful scheme for describing the gravitational field and the equations it obeys. Nowadays this theory is often used as a prototype for other, more intricate constructions to describe forces between elementary particles or other branches of fundamental physics. This is why in an introduction to general relativity it is of importance to separate as clearly as possible the various ingredients that together give shape to this paradigm. After explaining the physical motivations we first introduce curved coordinates, then add to this the notion of an affine connection field and only as a later step add to that the metric field. One then sees clearly how space and time get more and more structure, until finally all we have to do is deduce Einstein's field equations.},
author = {t Hooft, Gerard't},
journal = {Canonical Quantum Gravity},
month = {jul},
title = {{Introduction to General Relativity}},
url = {http://inspirehep.net/record/1373852/},
year = {2012}
}

@article{Szubert2016,
abstract = {In recent years, a number of methods have been
proposed that attempt to improve the performance of
genetic programming by exploiting information about
program semantics. One of the most important
developments in this area is semantic backpropagation.
The key idea of this method is to decompose a program
into two parts: a sub program and a context, and
calculate the desired semantics of the subprogram that
would make the entire program correct, assuming that
the context remains unchanged. In this paper we
introduce Forward Propagation Mutation, a novel
operator that relies on the opposite assumption.
instead of preserving the context, it retains the
subprogram and attempts to place it in the semantically
right context. We empirically compare the performance
of semantic backpropagation and forward propagation
operators on a set of symbolic regression benchmarks.
The experimental results demonstrate that semantic
forward propagation produces smaller programs that
achieve significantly higher generalization
performance.},
author = {Szubert, Marcin and Kodali, Anuradha and Ganguly, Sangram and Das, Kamalika and Bongard, Josh C},
doi = {10.1007/978-3-319-45823-6_34},
isbn = {9783319458229},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Genetic programming,Problem decomposition,Program semantics,Semantic backpropagation,Symbolic regression},
pages = {364--374},
title = {{Semantic forward propagation for symbolic regression}},
volume = {9921 LNCS},
year = {2016}
}

@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
eprint = {1512.00567},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}

@article{Szegedya,
abstract = {We propose a deep convolutional neural network ar- chitecture codenamed Inception that achieves the new state of the art for classification and detection in the Im- ageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the compu- tational budget constant. To optimize quality, the architec- tural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular in- carnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
author = {Szegedy, C and Liu, W and Jia, Y and Sermanet, P and Reed, S and Anguelov, D and Erhan, D and Vanhoucke, V and Rabinovich, A},
journal = {Modern Plastics},
title = {{Going Deeper with Convolutions}}
}

@inproceedings{Swearingen2017,
author = {Swearingen, Thomas and Drevo, Will and Cyphers, Bennett and Cuesta-Infante, Alfredo and Ross, Arun and Veeramachaneni, Kalyan},
booktitle = {2017 IEEE International Conference on Big Data (Big Data)},
doi = {10.1109/BigData.2017.8257923},
isbn = {978-1-5386-2715-0},
pages = {151--162},
publisher = {IEEE},
title = {{ATM: A distributed, collaborative, scalable system for automated machine learning}},
url = {http://ieeexplore.ieee.org/document/8257923/},
year = {2017}
}

@book{Sussman2016,
abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text.There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published.A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises.In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
author = {Sussman, Gerald Jay and Sussman, Julie},
booktitle = {Computers {\&} Mathematics with Applications},
doi = {10.1016/S0898-1221(97)90051-1},
issn = {08981221},
title = {{Structure and interpretation of computer programs, (second edition)}},
url = {https://opendocs.github.io/sicp/sicp.pdf},
year = {2016}
}

@article{Surendra2017,
abstract = {Due to the technological advancement, enormous micro data containing detailed individual information is being collected by both public and private organizations. The demand for releasing this data to public for social and economic welfare is growing. Also the organizations holding the data are under pressure to publish the data for proving their transparency. Since this micro data contains sensitive information about individuals, the raw data needs to be sanitized to preserve privacy of the individuals before releasing it to the public. There are different types of data sanitization methods and many techniques are being proposed for Privacy Preserving Data Publishing (PPDP) of micro data. Synthetic Data Generation is an alternative to data masking techniques for preserving privacy. In this paper different fully and partially synthetic data generation techniques are reviewed and key research gaps are identified which needs to be focused in the future research.},
author = {Surendra, H and Mohan, H S},
journal = {International Journal of Scientific {\&} Technology Research},
number = {3},
pages = {95--101},
title = {{A Review Of Synthetic Data Generation Methods For Privacy Preserving Data Publishing}},
volume = {6},
year = {2017}
}

@article{Strachey2000,
abstract = {This paper forms the substance of a course of lectures given at the International Summer School in Computer Programming at Copenhagen in August, 1967. The lectures were originally given from notes and the paper was written after the course was finished. In spite of this, and only partly because of the shortage of time, the paper still retains many of the shortcomings of a lecture course. The chief of these are an uncertainty of aim-it is never quite clear what sort of audience there will be for such lectures-and an associated switching from formal to informal modes of presentation which may well be less acceptable in print than it is natural in the lecture room. For these (and other) faults, I apologise to the reader. There are numerous references throughout the course to CPL [1-3]. This is a programming language which has been under development since 1962 at Cambridge and London and Oxford. It has served as a vehicle for research into both programming languages and the design of compilers. Partial implementations exist at Cambridge and London. The language is still evolving so that there is no definitive manual available yet. We hope to reach another resting point in its evolution quite soon and to produce a compiler and reference manuals for this version. The compiler will probably be written in such a way that it is relatively easy to transfer it to another machine, and in the first instance we hope to establish it on three or four machines more or less at the same time. The lack of a precise formulation for CPL should not cause much difficulty in this course, as we are primarily concerned with the ideas and concepts involved rather than with their precise representation in a programming language.},
author = {Strachey, C},
keywords = {ad hoc polymorphism,binding,cpl,foundations of computing,functions as data,l-values,mechanisms,meter passing,para-,parametric polymorphism,programming languages,r-values,semantics,type completeness,variable binding},
title = {{Fundamental Concepts in Programming Languages}},
year = {2000}
}

@article{Steorts,
author = {Steorts, Rebecca C},
title = {{The Multi Stage Gibbs Sampling : Data Augmentation Dutch Example Example : Data augmentation}}
}

@article{Stahl2006,
abstract = {This study describes the distribution of hobo-hybridizing sequences in the genus Drosophila. Southern blot analysis of 134 species revealed that hobo sequences are limited to the melanogaster and montium subgroups of the melanogaster-species group. Of the hobo-bearing species, only D. melanogaster and two of its sibling species, D. simulans and D. mauritiana, were found to contain potentially complete hobo elements. The distribution of hobo sequences is one of the narrowest distributions thus far described for any Drosophila transposable element.},
author = {Stahl, Saul},
doi = {10.2307/27642916},
issn = {0025570X},
journal = {Mathematics Magazine},
number = {2},
pages = {96},
title = {{The Evolution of the Normal Distribution}},
volume = {79},
year = {2006}
}

@book{Spivak1995,
author = {Spivak, Michael},
isbn = {0-8053-9021-9},
pages = {158},
title = {{Calculus on Manifolds - A Modern Approach to Classical Theorems of Advanced Calculus}},
year = {1995}
}

@article{Spivak2009,
abstract = {In this paper, we define a category DB, called the category of simplicial databases, whose objects are databases and whose morphisms are data-preserving maps. Along the way we give a precise formulation of the category of relational databases, and prove that it is a full subcategory of DB. We also prove that limits and colimits always exist in DB and that they correspond to queries such as select, join, union, etc. One feature of our construction is that the schema of a simplicial database has a natural geometric structure: an underlying simplicial set. The geometry of a schema is a way of keeping track of relationships between distinct tables, and can be thought of as a system of foreign keys. The shape of a schema is generally intuitive (e.g. the schema for round-trip flights is a circle consisting of an edge from {\$}A{\$} to {\$}B{\$} and an edge from {\$}B{\$} to {\$}A{\$}), and as such, may be useful for analyzing data. We give several applications of our approach, as well as possible advantages it has over the relational model. We also indicate some directions for further research.},
archivePrefix = {arXiv},
arxivId = {0904.2012},
author = {Spivak, David I},
eprint = {0904.2012},
pages = {1--35},
title = {{Simplicial Databases}},
url = {http://arxiv.org/abs/0904.2012},
year = {2009}
}

@article{Spivak2018,
abstract = {There are many books designed to introduce category theory to either a mathematical audience or a computer science audience. In this book, our audience is the broader scientific community. We attempt to show that category theory can be applied throughout the sciences as a framework for modeling phenomena and communicating results. In order to target the scientific audience, this book is example-based rather than proof-based. For example, monoids are framed in terms of agents acting on objects, sheaves are introduced with primary examples coming from geography, and colored operads are discussed in terms of their ability to model self-similarity. A new version with solutions to exercises will be available through MIT Press.},
archivePrefix = {arXiv},
arxivId = {1302.6946},
author = {Spivak, David I},
eprint = {1302.6946},
month = {feb},
title = {{Category theory for scientists}},
url = {http://arxiv.org/abs/1302.6946v1},
year = {2018}
}

@article{Spielman2011,
abstract = {The Laplacian matrices of graphs are fundamental. In addition to facilitating the application of linear algebra to graph theory, they arise in many practical problems. In this talk we survey recent progress on the design of provably fast algorithms for solving linear equations in the Laplacian matrices of graphs. These algorithms motivate and rely upon fascinating primitives in graph theory, including low-stretch spanning trees, graph sparsifiers, ultra-sparsifiers, and local graph clustering. These are all connected by a definition of what it means for one graph to approximate another. While this definition is dictated by Numerical Linear Algebra, it proves useful and natural from a graph theoretic perspective.},
author = {Spielman, Daniel A},
doi = {10.1142/9789814324359_0164},
keywords = {keywords},
pages = {2698--2722},
title = {{Algorithms, Graph Theory, and Linear Equations in Laplacian Matrices}},
year = {2011}
}

@book{Spiegel2009,
author = {Spiegel, R Murray},
isbn = {9780071635400},
title = {{Advanced Mathematics for Engineers and Scientists}},
year = {2009}
}

@book{Spiegel2008,
abstract = {This third edition covers elementary concepts in algebra, geometry, etc. and more advanced concepts in differential equations and vector analysis. It also expands its section on Probability and Statistics and includes a new section on Financial Mathematics to keep up with the current developments in finance studies as well as in the studies of math and the sciences.},
author = {Spiegel, Murray and Lipschutz, Seymour and Liu, John},
edition = {3},
isbn = {0071548564},
pages = {312},
publisher = {McGraw Hill Professional},
title = {{Schaum's Mathematical Handbook Of Formulas And Tables}},
year = {2008}
}

@book{Spiegel1998,
annote = {Not abstract alegebra},
author = {Spiegel, Murray R and Moyer, Robert E},
edition = {2},
isbn = {0070602662},
pages = {416},
publisher = {McGraw-Hill},
title = {{Schaum's Outline of Theory and Problems of College Algebra}},
year = {1998}
}

@article{Song2018,
abstract = {A process is described for the transformation of bulk wood into a low-cost, strong, tough, lightweight structural material, by the partial removal of lignin and hemicellulose followed by hot-pressing to densify the natural wood.},
author = {Song, Jianwei and Chen, Chaoji and Zhu, Shuze and Zhu, Mingwei and Dai, Jiaqi and Ray, Upamanyu and Li, Yiju and Kuang, Yudi and Li, Yongfeng and Quispe, Nelson and Yao, Yonggang and Gong, Amy and Leiste, Ulrich H and Bruck, Hugh A and Zhu, J Y and Vellore, Azhar and Li, Heng and Minus, Marilyn L and Jia, Zheng and Martini, Ashlie and Li, Teng and Hu, Liangbing},
doi = {10.1038/nature25476},
issn = {14764687},
journal = {Nature},
number = {7691},
pages = {224--228},
publisher = {Nature Publishing Group},
title = {{Processing bulk natural wood into a high-performance structural material}},
url = {http://dx.doi.org/10.1038/nature25476},
volume = {554},
year = {2018}
}

@article{Software2019,
author = {Software, International and Qualifications, Testing},
keywords = {EWG},
pages = {1--22},
title = {{Sample Exam-Questions ISTQB {\textregistered}Certified Tester Syllabus Foundation Level Exam ID: A International Software Testing Qualifications Board}},
year = {2019}
}

@article{Society2011a,
abstract = {This volume contains a collection of clever mathematical applications of linear algebra, mainly in combinatorics, geometry, and algorithms. Each chapter covers a single main result with motivation and full proof in at most ten pages and can be read independently of all other chapters (with minor exceptions), assuming only a modest background in linear algebra. The topics include a number of well-known mathematical gems, such as Hamming codes, the matrix-tree theorem, the Lovasz bound on the Shannon capacity, and a counterexample to Borsuk's conjecture, as well as other, perhaps less popular but similarly beautiful results, e.g., fast associativity testing, a lemma of Steinitz on ordering vectors, a monotonicity result for integer partitions, or a bound for set pairs via exterior products. The simpler results in the first part of the book provide ample material to liven up an undergraduate course of linear algebra. The more advanced parts can be used for a graduate course of linear-algebraic methods or for seminar presentations. Table of Contents: Fibonacci numbers, quickly; Fibonacci numbers, the formula; The clubs of Oddtown; Same-size intersections; Error-correcting codes; Odd distances; Are these distances Euclidean?; Packing complete bipartite graphs; Equiangular lines; Where is the triangle?; Checking matrix multiplication; Tiling a rectangle by squares; Three Petersens are not enough; Petersen, Hoffman-Singleton, and maybe 57; Only two distances; Covering a cube minus one vertex; Medium-size intersection is hard to avoid; On the difficulty of reducing the diameter; The end of the small coins; Walking in the yard; Counting spanning trees; In how many ways can a man tile a board?; More bricks--more walls?; Perfect matchings and determinants; Turning a ladder over a finite field; Counting compositions; Is it associative?; The secret agent and umbrella; Shannon capacity of the union: a tale of two fields; Equilateral sets; Cutting cheaply using eigenvectors; Rotating the cube; Set pairs and exterior products; Index. (STML/53)},
author = {Society, American Mathematical},
doi = {10.5860/choice.48-2733},
issn = {0009-4978},
journal = {Choice Reviews Online},
number = {05},
pages = {48--2733},
title = {{Thirty-three miniatures: mathematical and algorithmic applications of linear algebra}},
volume = {48},
year = {2011}
}

@article{Sobhani2016,
abstract = {{\textcopyright}2016 IEEE.The applications of ultrasound in medicine have been increasing in the last decade either in diagnostics or in treatments. Ultrasound is routinely used in clinical examinations, such as pregnancy exams. On the other hand, a typical ultrasound system costs somewhere between 100k to 250k{\$} because of its (1) expensive ultrasound transducers, (2) large driving electronics, (3) processing and visualization units. High cost and large volume of the ultrasound systems prevent even wider usage of these systems. It is possible to extent the use of ultrasound in clinic environment like a stethoscope, if the size and cost had been reduced orders of magnitude. The aim of this work is to develop an ultraportable and very low cost diagnostic ultrasound imaging probe; by combining inertial sensors with the probes. The manual motion of the probe by the operator's hand movement enables scanning. The position of the probe is tracked using inertial sensors. Finally, the acoustic reflections are registered together by the help of position information of the probe to form an image.},
author = {Sobhani, M Rahim and Ozum, H E and Yaralioglu, G G and Ergun, A S and Bozkurt, A},
doi = {10.1109/ULTSYM.2016.7728837},
isbn = {9781467398978},
issn = {19485727},
journal = {IEEE International Ultrasonics Symposium, IUS},
keywords = {3D imaging,3D ultrasound probe,low cost ultrasound probe,ultraportable ultrasound probe},
number = {113},
title = {{Portable Low Cost Ultrasound Imaging System}},
volume = {2016-Novem},
year = {2016}
}

@article{Smullyan1985,
author = {Smullyan, Raymond M},
isbn = {0192801422 9780192801425},
title = {{To Mock a Mockingbird and Other Logic Puzzles}},
year = {1985}
}

@book{Smola,
author = {Smola, A and Vishwanathan, S V N},
isbn = {0 521 82583 0},
title = {{Introduction to Machine Learning}}
}

@article{Smith2010,
author = {Smith, Michael S},
title = {{Estimation of Copula Models with Discrete Margins Estimation}},
year = {2010}
}

@article{Smaragdakis2019,
abstract = {The dream of programming language design is to bring about orders-of-magnitude productivity improvements in software development tasks. Designers can endlessly debate on how this dream can be realized and on how close we are to its realization. Instead, I would like to focus on a question with an answer that can be, surprisingly, clearer: what will be the common principles behind next-paradigm, high-productivity programming languages, and how will they change everyday program development? Based on my decade-plus experience of heavy-duty development in declarative languages, I speculate that certain tenets of high-productivity languages are inevitable. These include, for instance, enormous variations in performance (including automatic transformations that change the asymptotic complexity of algorithms); a radical change in a programmer's workflow, elevating testing from a near-menial task to an act of deep understanding; a change in the need for formal proofs; and more.},
archivePrefix = {arXiv},
arxivId = {1905.00402},
author = {Smaragdakis, Yannis},
eprint = {1905.00402},
title = {{Next-Paradigm Programming Languages: What Will They Look Like and What Changes Will They Bring?}},
url = {http://arxiv.org/abs/1905.00402},
year = {2019}
}

@book{Siyavula,
editor = {Siyavula},
title = {{Mathematics Grade 11}}
}

@book{Siyavulaa,
editor = {Siyavula},
title = {{Mathematics Grade 10}}
}

@book{Siyavula2007,
booktitle = {Science},
editor = {Siyavula},
pages = {1--176},
title = {{Mathematics Grade 10 Teachers}},
year = {2007}
}

@book{Siyavulab,
editor = {Siyavula},
title = {{Mathematics Grade 11 Teachers}}
}

@book{Siyavulac,
editor = {Siyavula},
title = {{Mathematics Grade 12}}
}

@book{Siyavulad,
editor = {Siyavula},
title = {{Mathematics Grade 12 Teacher}}
}

@book{Shoup2015,
abstract = {Number theory and algebra play an increasingly significant role in computing and communications, as evidenced by the striking applications of these subjects to such fields as cryptography and coding theory. This introductory book emphasises algorithms and applications, such as cryptography and error correcting codes, and is accessible to a broad audience.},
address = {Cambridge},
author = {Shoup, Victor},
doi = {10.1017/CBO9780511814549},
edition = {2},
isbn = {9780511814549},
pages = {598},
publisher = {Cambridge University Press},
title = {{A Computational Introduction to Number Theory and Algebra}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511814549},
year = {2008}
}

@inproceedings{Shotts2017,
abstract = {This book is a broad overview of “living” on the Linux command line. Unlike some books that concentrate on just a single program, such as the shell program, bash, this book will try to convey how to get along with the command line interface in a larger sense. How does it all work? What can it do? What's the best way to use it?},
author = {Shotts, William},
pages = {544},
publisher = {IEEE},
title = {{The Linux Command Line: Fourth Internet Edition}},
year = {2017}
}

@article{Shen2018,
abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F{\_}0{\$} features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.},
archivePrefix = {arXiv},
arxivId = {arXiv:1712.05884v1},
author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and Saurous, Rif A and Agiomvrgiannakis, Yannis and Wu, Yonghui},
doi = {10.1109/ICASSP.2018.8461368},
eprint = {arXiv:1712.05884v1},
isbn = {9781538646588},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Tacotron 2,Text-to-speech,WaveNet},
pages = {4779--4783},
title = {{Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions}},
volume = {2018-April},
year = {2018}
}

@article{Shen2017,
author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
doi = {10.1101/240317},
title = {{Deep image reconstruction from human brain activity}},
year = {2017}
}

@book{Shapiro2015,
abstract = {Treatment of hair disorders has progressed considerably over time. More patients are now interested in hair care and some are bombarded by promising advertisements. In reality, hair disorders may be complex and require accurate diagnosis for suitable treatment. Hair Loss and Restoration provides an extensive look at the practical management, both medical and surgical, of all forms of hair loss. Proper examination of the patient with hair loss is discussed in depth as is androgenetic alopecia, the most common cause of hair loss. The autoimmune disease alopecia areata is examined comprehensively, including its pathogenesis, clinical features, differential diagnosis, and treatment. This edition also covers new developments on the diagnosis and treatment of the disease. Hair loss from drugs and radiation is reviewed along with other topics such as telogen effluvium and frontal fibrosing alopecia—an increasing concern in scarring hair loss. Hair restoration surgery is described in detail and an excellent review of what is available from a nonmedical approach to hair loss is provided. Extensively referenced and illustrated with more than 300 clinical color photographs, this compact and easy-to-read book is a valuable resource for both doctors and patients.},
author = {Shapiro, Jerry and Otberg, Nina},
booktitle = {Academic Medicine},
doi = {10.1201/b18330},
edition = {2},
isbn = {9780429160592},
pages = {232},
publisher = {CRC Press},
title = {{Hair Loss and Restoration}},
url = {https://www.taylorfrancis.com/books/9781482231991},
year = {2015}
}

@article{Shalizi2013,
abstract = {课程：http://www.stat.cmu.edu/{\~{}}cshalizi/uADA/ 缺点：从统计角度，R语言},
author = {Shalizi, Cosma Rohilla},
journal = {Book Manuscript},
pages = {801},
title = {{Advanced data analysis from an elementary point of view}},
url = {http://www.stat.cmu.edu/{\%}7B{~}{\%}7Dcshalizi/ADAfaEPoV/{\%}0Ahttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.371.4613{\&}rep=rep1{\&}type=pdf},
year = {2013}
}

@book{Shalev-Shwartz2014,
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
address = {Cambridge},
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
doi = {10.1017/CBO9781107298019},
isbn = {978-1-107-05713-5},
pages = {449},
publisher = {Cambridge University Press},
title = {{Understanding Machine Learning: From Theory to Algorithms}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107298019},
volume = {9781107057},
year = {2014}
}

@article{Shahani-Denning2003,
author = {Shahani-Denning, Comila},
journal = {Hofstra Horizon},
pages = {14--17},
title = {{Physical attractiveness bias in hiring: What is beautiful is good}},
year = {2003}
}

@misc{Shafer1984,
author = {Shafer, Glenn R and Kahnerman, Daniel and Slovic, Paul and Tversky, Amos},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2288362},
issn = {01621459},
number = {385},
pages = {223},
title = {{Judgment Under Uncertainty: Heuristics and Biases.}},
volume = {79},
year = {1984}
}

@article{Sekiyama2018,
abstract = {This work explores the application of deep learning, a machine learning technique that uses deep neural networks (DNN) in its core, to an automated theorem proving (ATP) problem. To this end, we construct a statistical model which quantifies the likelihood that a proof is indeed a correct one of a given proposition. Based on this model, we give a proof-synthesis procedure that searches for a proof in the order of the likelihood. This procedure uses an estimator of the likelihood of an inference rule being applied at each step of a proof. As an implementation of the estimator, we propose a proposition-to-proof architecture, which is a DNN tailored to the automated proof synthesis problem. To empirically demonstrate its usefulness, we apply our model to synthesize proofs of the minimal propositional logic. We train the proposition-to-proof model using a training dataset of proposition–proof pairs. The evaluation against a benchmark set shows the very high accuracy and an improvement to the recent work of neural proof synthesis.},
archivePrefix = {arXiv},
arxivId = {1805.11799},
author = {Sekiyama, Taro and Suenaga, Kohei},
doi = {10.1007/978-3-030-02768-1_17},
eprint = {1805.11799},
isbn = {9783030027674},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
title = {{Automated Proof Synthesis for the Minimal Propositional Logic with Deep Neural Networks}},
year = {2018}
}

@book{Seitz2014,
abstract = {Python hacker. Those are two words you really could use to describe me. At Immunity, I am lucky enough to work with people who actually, really, know how to code Python. I am not one of those people. I spend a great deal of my time penetration testing, and that requires rapid Python tool development, with a focus on execution and delivering results (not necessarily on prettiness, optimization, or even stability). Throughout this book you will learn that this is how I code, but I also feel as though it is part of what makes me a strong pentester. I hope that this philosophy and style helps you as well. As you progress through the book, you will also realize that I don't take deep dives on any single topic. This is by design. I want to give you the bare minimum, with a little flavor, so that you have some foundational knowledge. With that in mind, I've sprinkled ideas and homework assignments throughout the book to kickstart you in your own direction. I encourage you to explore these ideas, and I would love to hear back any of your own implementations, tooling, or homework assignments that you have done. As with any technical book, readers at different skill levels with Python (or information security in general) will experience this book differently. Some of you may simply grab it and nab chapters that are pertinent to a consulting gig you are on, while others may read it cover to cover. I would recommend that if you are a novice to intermediate Python programmer that you start at the beginning of the book and read it straight through in order. You will pick up some good building blocks along the way.},
author = {Seitz, Justin},
doi = {10.1016/S1353-4858(15)30025-8},
isbn = {1593275900},
pages = {192},
title = {{Black Hat Python: Python Programming for Hackers and Pentesters}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1353485815300258},
year = {2014}
}

@article{Segal2019,
abstract = {The prevalence of e-learning systems and on-line courses has made educational material widely accessible to students of varying abilities and backgrounds. There is thus a growing need to accommodate for individual differences in e-learning systems. This paper presents an algorithm called EduRank for personalizing educational content to students that combines a collaborative filtering algorithm with voting methods. EduRank constructs a difficulty ranking for each student by aggregating the rankings of similar students using different aspects of their performance on common questions. These aspects include grades, number of retries, and time spent solving questions. It infers a difficulty ranking directly over the questions for each student, rather than ordering them according to the student's predicted score. The EduRank algorithm was tested on two data sets containing thousands of students and a million records. It was able to outperform the state-of-the-art ranking approaches as well as a domain expert. EduRank was used by students in a classroom activity, where a prior model was incorporated to predict the difficulty rankings of students with no prior history in the system. It was shown to lead students to solve more difficult questions than an ordering by a domain expert, without reducing their performance.},
archivePrefix = {arXiv},
arxivId = {1907.12047},
author = {Segal, Avi and Gal, Kobi and Shani, Guy and Shapira, Bracha},
doi = {10.1016/j.ijhcs.2019.07.002},
eprint = {1907.12047},
month = {jul},
title = {{A difficulty ranking approach to personalization in E-learning}},
url = {http://arxiv.org/abs/1907.12047 http://dx.doi.org/10.1016/j.ijhcs.2019.07.002},
year = {2019}
}

@article{Scibior2015,
abstract = {The machine learning community has recently shown a lot of inter-est in practical probabilistic programming systems that target the problem of Bayesian inference. Such systems come in different forms, but they all express probabilistic models as computational processes using syntax resembling programming languages. In the functional programming community monads are known to offer a convenient and elegant abstraction for programming with probabil-ity distributions, but their use is often limited to very simple in-ference problems. We show that it is possible to use the monad abstraction to construct probabilistic models for machine learning, while still offering good performance of inference in challenging models. We use a GADT as an underlying representation of a prob-ability distribution and apply Sequential Monte Carlo-based meth-ods to achieve efficient inference. We define a formal semantics via measure theory. We demonstrate a clean and elegant implementa-tion that achieves performance comparable with Anglican, a state-of-the-art probabilistic programming system.},
author = {{\'{S}}cibior, Adam and Ghahramani, Zoubin and Gordon, Andrew D},
doi = {10.1145/2887747.2804317},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
number = {12},
title = {{Practical probabilistic programming with monads}},
volume = {50},
year = {2015}
}

@book{Schmidt2003,
abstract = {Fortunately for you, there's Schaum's Outlines. More than 40 million students have trusted Schaum's to help them succeed in the classroom and on exams. Schaum's is the key to faster learning and higher grades in every subject. Each Outline presents all the essential course information in an easy-to-follow, topic-by-topic format. You also get hundreds of examples, solved problems, and practice exercises to test your skills.},
author = {Schmidt, Philip and Ayres, Frank},
edition = {3},
isbn = {0071402276},
keywords = {Shelf 4},
pages = {400},
publisher = {McGraw Hill Professional},
title = {{Schaum's Outline of College Mathematics}},
year = {2003}
}

@article{Schmidt2018,
author = {Schmidt, Andreas},
number = {1},
pages = {1--66},
title = {{How to build a Search-Engine with Common Unix-Tools}},
url = {http://www.smiffy.de/dbkda-2018/},
year = {2018}
}

@article{Schilling2019,
abstract = {{\textless}p{\textgreater}The cooling of boiling water all the way down to freezing, by thermally connecting it to a thermal bath held at ambient temperature without external intervention, would be quite unexpected. We describe the equivalent of a “thermal inductor,” composed of a Peltier element and an electric inductance, which can drive the temperature difference between two bodies to change sign by imposing inertia on the heat flowing between them, and enable continuing heat transfer from the chilling body to its warmer counterpart without the need of an external driving force. We demonstrate its operation in an experiment and show that the process can pass through a series of quasi-equilibrium states while fully complying with the second law of thermodynamics. This thermal inductor extends the analogy between electrical and thermal circuits and could serve, with further progress in thermoelectric materials, to cool hot materials well below ambient temperature without external energy supplies or moving parts.{\textless}/p{\textgreater}},
author = {Schilling, A and Zhang, X and Bossen, O},
doi = {10.1126/sciadv.aat9953},
issn = {2375-2548},
journal = {Science Advances},
number = {4},
pages = {eaat9953},
publisher = {American Association for the Advancement of Science},
title = {{Heat flowing from cold to hot without external intervention by using a “thermal inductor”}},
url = {http://advances.sciencemag.org/lookup/doi/10.1126/sciadv.aat9953},
volume = {5},
year = {2019}
}

@article{Scheiman2019,
abstract = {The human gut microbiome is linked to many states of human health and disease1. The metabolic repertoire of the gut microbiome is vast, but the health implications of these bacterial pathways are poorly understood. In this study, we identify a link between members of the genus Veillonella and exercise performance. We observed an increase in Veillonella relative abundance in marathon runners postmarathon and isolated a strain of Veillonella atypica from stool samples. Inoculation of this strain into mice significantly increased exhaustive treadmill run time. Veillonella utilize lactate as their sole carbon source, which prompted us to perform a shotgun metagenomic analysis in a cohort of elite athletes, finding that every gene in a major pathway metabolizing lactate to propionate is at higher relative abundance postexercise. Using 13C3-labeled lactate in mice, we demonstrate that serum lactate crosses the epithelial barrier into the lumen of the gut. We also show that intrarectal instillation of propionate is sufficient to reproduce the increased treadmill run time performance observed with V. atypica gavage. Taken together, these studies reveal that V. atypica improves run time via its metabolic conversion of exercise-induced lactate into propionate, thereby identifying a natural, microbiome-encoded enzymatic process that enhances athletic performance.},
author = {Scheiman, Jonathan and Luber, Jacob M and Chavkin, Theodore A and MacDonald, Tara and Tung, Angela and Pham, Loc-Duyen and Wibowo, Marsha C and Wurth, Renee C and Punthambaker, Sukanya and Tierney, Braden T and Yang, Zhen and Hattab, Mohammad W and Avila-Pacheco, Julian and Clish, Clary B and Lessard, Sarah and Church, George M and Kostic, Aleksandar D},
doi = {10.1038/s41591-019-0485-4},
issn = {1078-8956},
journal = {Nature Medicine},
keywords = {Homeostasis,Metabolism,Microbiome,Translational research},
month = {jun},
pages = {1},
publisher = {Nature Publishing Group},
title = {{Meta-omics analysis of elite athletes identifies a performance-enhancing microbe that functions via lactate metabolism}},
url = {http://www.nature.com/articles/s41591-019-0485-4},
year = {2019}
}

@article{Sandfort2019,
abstract = {Labeled medical imaging data is scarce and expensive to generate. To achieve generalizable deep learning models large amounts of data are needed. Standard data augmentation is a method to increase generalizability and is routinely performed. Generative adversarial networks offer a novel method for data augmentation. We evaluate the use of CycleGAN for data augmentation in CT segmentation tasks. Using a large image database we trained a CycleGAN to transform contrast CT images into non-contrast images. We then used the trained CycleGAN to augment our training using these synthetic non-contrast images. We compared the segmentation performance of a U-Net trained on the original dataset compared to a U-Net trained on the combined dataset of original data and synthetic non-contrast images. We further evaluated the U-Net segmentation performance on two separate datasets: The original contrast CT dataset on which segmentations were created and a second dataset from a different hospital containing only non-contrast CTs. We refer to these 2 separate datasets as the in-distribution and out-of-distribution datasets, respectively. We show that in several CT segmentation tasks performance is improved significantly, especially in out-of-distribution (noncontrast CT) data. For example, when training the model with standard augmentation techniques, performance of segmentation of the kidneys on out-of-distribution non-contrast images was dramatically lower than for in-distribution data (Dice score of 0.09 vs. 0.94 for out-of-distribution vs. in-distribution data, respectively, p {\textless}0.001). When the kidney model was trained with CycleGAN augmentation techniques, the out-of-distribution (non-contrast) performance increased dramatically (from a Dice score of 0.09 to 0.66, p {\textless}0.001). Improvements for the liver and spleen were smaller, from 0.86 to 0.89 and 0.65 to 0.69, respectively. We believe this method will be valuable to medical imaging researchers to reduce manual segmentation effort and cost in CT imaging.},
author = {Sandfort, Veit and Yan, Ke and Pickhardt, Perry J and Summers, Ronald M},
doi = {10.1038/s41598-019-52737-x},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
publisher = {Nature Research},
title = {{Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks}},
volume = {9},
year = {2019}
}

@article{Sanchez-Stern2019,
abstract = {Foundational verification allows programmers to build software which has been empirically shown to have high levels of assurance in a variety of important domains. However, the cost of producing foundationally verified software remains prohibitively high for most projects,as it requires significant manual effort by highly trained experts. In this paper we present Proverbot9001 a proof search system using machine learning techniques to produce proofs of software correctness in interactive theorem provers. We demonstrate Proverbot9001 on the proof obligations from a large practical proof project,the CompCert verified C compiler,and show that it can effectively automate what was previously manual proofs,automatically solving 15.77{\%} of proofs in our test dataset. This corresponds to an over 3X improvement over the prior state of the art machine learning technique for generating proofs in Coq.},
archivePrefix = {arXiv},
arxivId = {1907.07794},
author = {Sanchez-Stern, Alex and Alhessi, Yousef and Saul, Lawrence and Lerner, Sorin},
eprint = {1907.07794},
number = {January},
title = {{Generating Correctness Proofs with Neural Networks}},
url = {http://arxiv.org/abs/1907.07794},
volume = {1},
year = {2019}
}

@book{SWilf2009,
abstract = {Generating functions, one of the most important tools in enumerative combinatorics, are a bridge between discrete mathematics and continuous analysis. Generating functions have numerous applications in mathematics, especially in: Combinatorics; Probability Theory; Statistics; Theory of Markov Chains; and Number Theory. One of the most important and relevant recent applications of combinatorics lies in the development of Internet search engines whose incredible capabilities dazzle even the mathematically trained user},
author = {{S Wilf}, Herbert},
pages = {1--231},
title = {{GeneratingFunctionology}},
url = {http://books.google.com/books?id=Ng0yH5L{\_}UX0C{\%}5Cnpapers2://publication/uuid/6733AEA5-E8A8-446B-A2CD-F23B9CBE666F},
year = {2009}
}

@article{Ruohonen2013,
author = {Ruohonen, Keijo},
title = {{Graph Theory}},
year = {2013}
}

@article{Roy2019,
author = {Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
doi = {10.1038/s41586-019-1677-2},
issn = {0028-0836},
journal = {Nature},
month = {nov},
number = {7784},
pages = {607--617},
title = {{Towards spike-based machine intelligence with neuromorphic computing}},
url = {http://www.nature.com/articles/s41586-019-1677-2},
volume = {575},
year = {2019}
}

@phdthesis{Rosario2017,
abstract = {Our focus is to test a new preprocessing approach that uses resampling, inspired by the bootstrap, combined with data augmentation, by treating each short text as a population and sampling similar words from a semantic space to create a longer text. We use blog post titles collected from the Technorati blog aggregator as experimental data with each title appearing in one of ten categories. We first test how well the raw short texts are classified using a variant of SVM designed specifically for short texts as well as a supervised topic model and an SVM model that uses semantic vectors as features. We then build a semantic space and augment each short text with related terms under a variety of experimental conditions. We test the classifiers on the augmented data and compare performance to the aforementioned baselines. The classifier performance on augmented test sets outperformed the baseline classifiers in most cases.},
author = {Rosario, Ryan Robert},
school = {UNIVERSITY OF CALIFORNIA},
title = {{A Data Augmentation Approach to Short Text Classification}},
year = {2017}
}

@article{Rocklin2012,
abstract = {Replacing symbols with random variables makes it possible to naturally add statistical operations to complex physical models. Three examples of symbolic statistical modeling are considered here, using new features from the popular SymPy project. {\textcopyright}2012 IEEE.},
author = {Rocklin, Matthew and Terrel, Andy R},
doi = {10.1109/MCSE.2012.56},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {Computational science,Python,SymPy,scientific computing,symbolic statistical modeling},
number = {3},
pages = {88--93},
title = {{Symbolic statistics with SymPy}},
volume = {14},
year = {2012}
}

@book{Rocha,
author = {Rocha, Rhuan},
isbn = {9781788830621},
title = {{Java EE 8 Design Patterns and Best Practices}}
}

@article{Robeva2019,
abstract = {In this article we show the duality between tensor networks and undirected graphical models with discrete variables. We study tensor networks on hypergraphs, which we call tensor hypernetworks. We show that the tensor hypernetwork on a hypergraph exactly corresponds to the graphical model given by the dual hypergraph.We translate various notions under duality. For example, marginalization in a graphical model is dual to contraction in the tensor network. Algorithms also translate under duality. We show that belief propagation corresponds to a known algorithm for tensor network contraction. This article is a reminder that the research areas of graphical models and tensor networks can benefit from interaction.},
archivePrefix = {arXiv},
arxivId = {1710.01437},
author = {Robeva, Elina and Seigal, Anna},
doi = {10.1093/imaiai/iay009},
eprint = {1710.01437},
issn = {20498772},
journal = {Information and Inference},
keywords = {Belief propagation,Graphical models,Hypergraphs,Tensor networks},
number = {2},
pages = {273--288},
title = {{Duality of graphical models and tensor networks}},
volume = {8},
year = {2019}
}

@book{RobertE.Moyer2008,
abstract = {This thorough review of standard college courses in trigonometry has been updated to reflect the latest course scope and sequences. The new edition includes expanded explanations of the aspects of each periodic function, and updated information for the curve graphing section.},
author = {{Robert E. Moyer}, Frank Ayres},
isbn = {0071543511},
keywords = {acute angle adjacent side airspeed amplitude andx},
mendeley-tags = {acute angle adjacent side airspeed amplitude andx},
pages = {211},
publisher = {McGraw Hill Professional},
title = {{Schaum's Outline of Trigonometry}},
year = {2008}
}

@book{Ries2011,
abstract = {Eric Ries defines a startup as an organization dedicated to creating something new under conditions of extreme uncertainty. This is just as true for one person in a garage or a group of seasoned professionals in a Fortune 500 boardroom. What they have in common is a mission to penetrate that fog of uncertainty to discover a successful path to a sustainable business. The Lean Startup approach fosters companies that are both more capital efficient and that leverage human creativity more effectively. Inspired by lessons from lean manufacturing, it relies on “validated learning,” rapid scientific experimentation, as well as a number of counter-intuitive practices that shorten product development cycles, measure actual progress without resorting to vanity metrics, and learn what customers really want. It enables a company to shift directions with agility, altering plans inch by inch, minute by minute. Rather than wasting time creating elaborate business plans, The Lean Startup offers entrepreneurs - in companies of all sizes - a way to test their vision continuously, to adapt and adjust before it's too late. Ries provides a scientific approach to creating and managing successful startups in a age when companies need to innovate more than ever.},
author = {Ries, Eric},
doi = {10.1111/j.1540-5885.2012.00920_2.x},
isbn = {9780307887917},
pages = {296},
publisher = {Crown Business},
title = {{The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses}},
year = {2011}
}

@article{Rienstra2017,
abstract = {In this paper we introduce RankPL, a modeling language that can be thought of as a qualitative variant of a probabilistic programming language with a semantics based on Spohn's ranking theory. Broadly speaking, RankPL can be used to represent and reason about processes that exhibit uncertainty expressible by distinguishing "normal" from" surprising" events. RankPL allows (iterated) revision of rankings over alternative program states and supports various types of reasoning, including abduction and causal inference. We present the language, its denotational semantics, and a number of practical examples. We also discuss an implementation of RankPL that is available for download.},
archivePrefix = {arXiv},
arxivId = {1705.07226},
author = {Rienstra, Tjitze},
eprint = {1705.07226},
month = {may},
title = {{RankPL: A Qualitative Probabilistic Programming Language}},
url = {http://arxiv.org/abs/1705.07226},
year = {2017}
}

@article{Richter-Gebert2009,
abstract = {Geometry and in particular projective geometry (and its corresponding invariant theory) deals a lot with structural properties of geometric objects and their interrelations. This papers describes how concepts of tensor calculus can be used to express geometric invariants and how, in particular, diagrammatic notation can be used to deal with invariants in a highly intuitive way. In particular we explain how geometries like euclidean or spherical geometry can be dealt with in this framework. {\textcopyright}2009 Springer Science+Business Media, LLC.},
author = {Richter-Gebert, J{\"{u}}rgen and Lebmeir, Peter},
doi = {10.1007/s00454-009-9188-9},
issn = {01795376},
journal = {Discrete and Computational Geometry},
keywords = {Diagrams,Incidence theorems,Invariant theory,Projective geometry,Tensors},
number = {2},
pages = {305--334},
title = {{Diagrams, tensors and geometric reasoning}},
volume = {42},
year = {2009}
}

@book{Richards2015,
abstract = {It's all too common for developers to start coding an application without a formal architecture in place. Without a clear and well- defined architecture, most developers and architects will resort to the de facto standard traditional layered architecture pattern (also called the n-tier architecture), creating implicit layers by separating source-code modules into packages. Unfortunately, what often results from this practice is a collection of unorganized source-code modules that lack clear roles, responsibilities, and relationships to one another. This is commonly referred to as the big ball of mud architecture anti-pattern.},
author = {Richards, Mark},
isbn = {978-1-491-92424-2},
publisher = {O'Reilly Media},
title = {{Software Architecture Patterns}},
year = {2015}
}

@article{Rekatsinas2017,
abstract = {We introduce HoloClean, a framework for holistic data repairing driven by probabilistic inference. HoloClean unifies existing qualitative data repairing approaches, which rely on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, HoloClean automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over HoloClean's probabilistic model scales to instances with millions of tuples. We show that HoloClean scales to instances with millions of tuples and find data repairs with an average precision of {\~{}}90{\%} and an average recall of above {\~{}}76{\%} across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2x against state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {1702.00820},
author = {Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F and R{\'{e}}, Christopher},
eprint = {1702.00820},
month = {feb},
title = {{HoloClean: Holistic Data Repairs with Probabilistic Inference}},
url = {http://arxiv.org/abs/1702.00820},
year = {2017}
}

@article{Raychev2014,
author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
doi = {10.1145/2666356.2594321},
isbn = {9781450327848},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
number = {6},
pages = {419--428},
title = {{Code Completion with Statistical Language Models}},
volume = {49},
year = {2014}
}

@misc{Ravi2016,
author = {Ravi, Sachin and Larochelle, Hugo},
month = {nov},
title = {{Optimization as a Model for Few-Shot Learning}},
url = {https://openreview.net/forum?id=rJY0-Kcll},
year = {2016}
}

@article{Rashkin2018,
abstract = {We investigate a new commonsense inference task: given an event described in a short free-form text ("X drinks coffee in the morning"), a system reasons about the likely intents ("X wants to stay awake") and reactions ("X feels alert") of the event's participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people's intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.},
archivePrefix = {arXiv},
arxivId = {1805.06939},
author = {Rashkin, Hannah and Sap, Maarten and Allaway, Emily and Smith, Noah A and Choi, Yejin},
eprint = {1805.06939},
month = {may},
title = {{Event2Mind: Commonsense Inference on Events, Intents, and Reactions}},
url = {http://arxiv.org/abs/1805.06939},
year = {2018}
}

@book{Ranson2008,
author = {Ranson, James F and Hamilton, Howard J and Fong, Philip W L},
isbn = {9780773106574},
keywords = {Project 2.2,Sub-project 2.2.a,Technical report,Theme 2},
title = {{A Semantics of Python in Isabelle/HOL (Techinical Report TR 2008-04)}},
url = {http://www.cs.uregina.ca/Research/Techreports/2008-04.pdf},
volume = {4},
year = {2008}
}

@book{Ramalho2014,
author = {Ramalho, L},
isbn = {978-1-491-94600-8},
title = {{Fluent Python}},
year = {2014}
}

@article{Raayoni2019,
abstract = {Fundamental mathematical constants like {\$}e{\$} and {\$}\backslashbackslashpi{\{}\backslash{\$}{\}} are ubiquitous in diverse fields of science, from abstract mathematics and geometry to physics, biology and chemistry. Nevertheless, for centuries new mathematical formulas relating fundamental constants have been scarce and usually discovered sporadically. In this paper we propose a novel and systematic approach that leverages algorithms for deriving new mathematical formulas for fundamental constants and help reveal their underlying structure. Our algorithms find dozens of well-known as well as previously unknown continued fraction representations of {\$}\backslashbackslashpi{\{}\backslash{\$}{\}}, {\$}e{\$}, and the Riemann zeta function values. Two new conjectures produced by our algorithm, along with many others, are: {\$}\backslash{\$}begin{\{}equation*{\}} e = 3 + {\$}\backslash{\$}frac{\{}-1{\}}{\{}4+{\$}\backslash{\$}frac{\{}-2{\}}{\{}5+{\$}\backslash{\$}frac{\{}-3{\}}{\{}6+{\$}\backslash{\$}frac{\{}-4{\}}{\{}7+{\$}\backslash{\$}ldots{\}}{\}}{\}}{\}} {\$}\backslash{\$}quad{\$}\backslash{\$}quad,{\$}\backslash{\$}quad{\$}\backslash{\$}quad {\$}\backslash{\$}frac{\{}4{\}}{\{}{\$}\backslash{\$}pi-2{\}} = 3+{\$}\backslash{\$}frac{\{}1{\$}\backslash{\$}cdot3{\}}{\{}5+{\$}\backslash{\$}frac{\{}2{\$}\backslash{\$}cdot 4{\}}{\{}7+{\$}\backslash{\$}frac{\{}3{\$}\backslash{\$}cdot 5{\}}{\{}9+{\$}\backslash{\$}frac{\{}4{\$}\backslash{\$}cdot 6{\}}{\{}11+{\$}\backslash{\$}ldots{\}}{\}}{\}}{\}} {\$}\backslash{\$}end{\{}equation*{\}} We present two algorithms that proved useful in finding new results: a variant of the Meet-In-The-Middle (MITM) algorithm and a Gradient Descent (GD) tailored to the recurrent structure of continued fractions. Both algorithms are based on matching numerical values and thus find new conjecture formulas without providing proofs and without requiring prior knowledge on any mathematical structure. This approach is especially attractive for fundamental constants for which no mathematical structure is known, as it reverses the conventional approach of sequential logic in formal proofs. Instead, our work presents a new conceptual approach for research: computer algorithms utilizing numerical data to unveil new internal structures and conjectures, thus playing the role of mathematical intuition of great mathematicians of the past, providing leads to new mathematical research.},
archivePrefix = {arXiv},
arxivId = {1907.00205},
author = {Raayoni, Gal and Pisha, George and Manor, Yahel and Mendlovic, Uri and Haviv, Doron and Hadad, Yaron and Kaminer, Ido},
eprint = {1907.00205},
month = {jun},
title = {{The Ramanujan Machine: Automatically Generated Conjectures on Fundamental Constants}},
url = {http://arxiv.org/abs/1907.00205},
year = {2019}
}

@article{Raab2013a,
abstract = {Symbolic integration deals with the evaluation of integrals in closed form. We present an overview of Risch's algorithm including recent developments. The algorithms discussed are suited for both indefinite and definite integration. They can also be used to compute linear relations among integrals and to find identities for special functions given by parameter integrals. The aim of this presentation is twofold: to introduce the reader to some basic ideas of differential algebra in the context of integration and to raise awareness in the physics community of computer algebra algorithms for indefinite and definite integration.},
archivePrefix = {arXiv},
arxivId = {arXiv:1305.1481v1},
author = {Raab, Clemens G},
doi = {10.1007/978-3-7091-1616-6_12},
eprint = {arXiv:1305.1481v1},
number = {July 2012},
pages = {285--304},
title = {{Generalization of Risch's Algorithm to Special Functions}},
year = {2013}
}

@article{Polyak2005,
abstract = {This is a simple mathematical introduction into Feynman diagram technique, which is a standard physical tool to write perturbative expansions of path integrals near a critical point of the action. I start from a rigorous treatment of a finite dimensional case (which actually belongs more to multivariable calculus than to physics), and then use a simple "dictionary" to translate these results to an infinite dimensional case. The standard methods such as gauge-fixing and Faddeev-Popov ghosts are also included. Resulting Feynman diagram series often may be used rigorously without any references to the initial physical theory (which one may "sweep under the carpet"). This idea is illustrated on an example of the Chern-Simons theory, which leads to universal finite type invariants of knots and 3-manifolds.},
archivePrefix = {arXiv},
arxivId = {math/0406251},
author = {Polyak, Michael},
doi = {10.1090/pspum/073/2131010},
eprint = {0406251},
keywords = {and phrases,chern-simons theory,con-,feynman diagrams,gauge-fixing,knots},
pages = {15--42},
primaryClass = {math},
title = {{Feynman diagrams for pedestrians and mathematicians}},
year = {2005}
}

@article{Polson2011,
abstract = {This paper presents a latent variable representation of regularized support vector machines (SVM's) that enables EM, ECME or MCMC algorithms to provide parameter estimates. We verify our representation by demonstrating that minimizing the SVM optimality criterion together with the parameter regularization penalty is equivalent to finding the mode of a mean-variance mixture of normals pseudo-posterior distribution. The latent variables in the mixture representation lead to EM and ECME point estimates of SVM parameters, as well as MCMC algorithms based on Gibbs sampling that can bring Bayesian tools for Gaussian linear models to bear on SVM's. We show how to implement SVM's with spike-and-slab priors and run them against data from a standard spam filtering data set.},
author = {Polson, Nicholas G and Scott, Steven L},
doi = {10.1214/11-BA601},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Bayesian inference,ECME,EM,L{\$}\alpha{\$}-norm,Lasso,MCMC,Regularization},
number = {1},
pages = {1--24},
title = {{Data augmentation for support vector machines}},
volume = {6},
year = {2011}
}

@article{Pluchino2018,
abstract = {The largely dominant meritocratic paradigm of highly competitive Western cultures is rooted on the belief that success is mainly due, if not exclusively, to personal qualities such as talent, intelligence, skills, smartness, efforts, willfulness, hard work or risk taking. Sometimes, we are willing to admit that a certain degree of luck could also play a role in achieving significant success. But, as a matter of fact, it is rather common to underestimate the importance of external forces in individual successful stories. It is very well known that intelligence (or, more in general, talent and personal qualities) exhibits a Gaussian distribution among the population, whereas the distribution of wealth-often considered as a proxy of success-follows typically a power law (Pareto law), with a large majority of poor people and a very small number of billionaires. Such a discrepancy between a Normal distribution of inputs, with a typical scale (the average talent or intelligence), and the scale-invariant distribution of outputs, suggests that some hidden ingredient is at work behind the scenes. In this paper, we suggest that such an ingredient is just randomness. In particular, our simple agent-based model shows that, if it is true that some degree of talent is necessary to be successful in life, almost never the most talented people reach the highest peaks of success, being overtaken by averagely talented but sensibly luckier individuals. As far as we know, this counterintuitive result- A lthough implicitly suggested between the lines in a vast literature-is quantified here for the first time. It sheds new light on the effectiveness of assessing merit on the basis of the reached level of success and underlines the risks of distributing excessive honors or resources to people who, at the end of the day, could have been simply luckier than others. We also compare several policy hypotheses to show the most efficient strategies for public funding of research, aiming to improve meritocracy, diversity of ideas and innovation.},
archivePrefix = {arXiv},
arxivId = {1802.07068},
author = {Pluchino, Alessandro and Biondo, Alessio Emanuele and Rapisarda, Andrea},
doi = {10.1142/S0219525918500145},
eprint = {1802.07068},
issn = {02195259},
journal = {Advances in Complex Systems},
keywords = {Success,agent-based models,luck,serendipity,talent},
number = {3-4},
pages = {1--28},
title = {{Talent versus luck: The role of randomness in success and failure}},
volume = {21},
year = {2018}
}

@article{Pham2006a,
abstract = {We present a novel algorithm for image fusion from irregularly sampled data. The method is based on the framework of normalized convolution (NC), in which the local signal is approximated through a projection onto a subspace. The use of polynomial basis functions in this paper makes NC equivalent to a local Taylor series expansion. Unlike the traditional framework, however, the window function of adaptive NC is adapted to local linear structures. This leads to more samples of the same modality being gathered for the analysis, which in turn improves signal-to-noise ratio and reduces diffusion across discontinuities. A robust signal certainty is also adapted to the sample intensities to minimize the influence of outliers. Excellent fusion capability of adaptive NC is demonstrated through an application of super-resolution image reconstruction. Copyright {\textcopyright}2006 Hindawi Publishing Corporation. All rights reserved.},
author = {Pham, Tuan Q and van Vliet, Lucas J and Schutte, Klamer},
doi = {10.1155/ASP/2006/83268},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in Signal Processing},
pages = {12},
title = {{Robust Fusion of Irregularly Sampled Data Using Adaptive Normalized Convolution}},
url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/83268},
year = {2006}
}

@article{Pfenning2007,
author = {Pfenning, F},
title = {{Logic Programming}},
year = {2007}
}

@article{Peterson,
abstract = {We present our experiences integrating Functional Reactive Programming (FRP) into a new host language, Python, and a variety of computational contexts: a game engine, a GUI system, and an em-bedded controller. We demonstrate FRP principles extended to a dy-namic environment and the integration of object-oriented libraries into the reactive environment. A number of FRP semantic issues are ad-dressed in a straight-forward way to produce an elegant combination of FRP and object-oriented programming. In particular, reactive proxies integrate traditional objects with the reactive world and event-triggered reactors allow reconfiguration of the network of reactive proxies and in-teraction with the outside world. This work demonstrates that FRP can serve as a unifying framework that simplifies programming and allows novices to build reactive systems. This system has been used to develop a wide variety of game programs and reactive animations in a summer camp which introduces programming concepts to teenagers.},
author = {Peterson, John and Roe, Ken and Cleary, Alan},
journal = {Cs.Jhu.Edu},
pages = {1--16},
title = {{Practical Functional Reactive Programming}},
url = {http://www.cs.jhu.edu/{\%}7B{~}{\%}7Droe/padl2014.pdf}
}

@article{Peterson2009a,
abstract = {This paper provides an introduction to trace diagrams at a level suitable for advanced undergraduates. Trace diagrams are a non-traditional notation for linear algebra. Vectors are represented by edges in a diagram, and matrices by markings along the edges of the diagram. The notation is rigorous and permits proofs more elegant than those written using traditional notation. We begin with the definition of trace diagrams, and move directly into two special cases that help orient the reader to the diagrammatic point-of-view. We then provide an explicit description of how they are calculated. Finally, we provide the diagrammatic perspective on some questions often posed by students seeing vectors and linear algebra for the first time. We also look at some questions inspired by the diagrammatic notation. We include several examples and exercises throughout, which are particularly important for adjusting to nonstandard notation.},
archivePrefix = {arXiv},
arxivId = {0910.1362},
author = {Peterson, Elisha},
eprint = {0910.1362},
month = {oct},
title = {{Unshackling Linear Algebra from Linear Notation}},
url = {http://arxiv.org/abs/0910.1362},
year = {2009}
}

@article{Petersen2012,
abstract = {These pages are a collection of facts (identities, approxima- tions, inequalities, relations, ...) about matrices and matters relating to them. It is collected in this form for the convenience of anyone who wants a quick desktop reference .},
author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
title = {{The Matrix Cookbook}},
year = {2012}
}

@article{Perez2015,
abstract = {There are at present two ways to write GUIs for functional code. One is to use standard GUI toolkits, with all the benefits they bring in terms of feature completeness, choice of platform, conformance to platform-specific look-and-feel, long-term viability, etc. However, such GUI APIs mandate an imperative programming style for the GUI and related parts of the application. Alternatively, we can use a functional GUI toolkit. The GUI can then be written in a functional style, but at the cost of foregoing many advantages of standard toolkits that often will be of critical importance. This paper introduces a light-weight framework structured around the notions of reactive values and reactive relations. It allows standard toolkits to be used from functional code written in a functional style. We thus bridge the gap between the two worlds, bringing the advantages of both to the developer. Our framework is available on Hackage and has been been validated through the development of non-trivial applications in a commercial context, and with different standard GUI toolkits.},
author = {Perez, Ivan and Nilsson, Henrik},
doi = {10.1145/2804302.2804316},
isbn = {9781450338080},
issn = {03621340},
journal = {Haskell 2015 - Proceedings of the 8th ACM SIGPLAN Symposium on Haskell, co-located with ICFP 2015},
title = {{Bridging the GUI gap with reactive values and relations}},
year = {2015}
}

@article{PerezGonzalez2018,
abstract = {Este libro est{\'{a}} escrito pensando en un estudiante real que tambi{\'{e}}n es, en algunos aspectos, un estudiante ideal. Es un estudiante llegado hace poco a la Universidad, quiz{\'{a}} reci{\'{e}}n llegado, que cursa estudios en alguna ingenier{\'{i}}a o licenciatura cient{\'{i}}fico – t{\'{e}}cnica y debe enfrentarse a una dif{\'{i}}cil asignatura de c{\'{a}}lculo diferencial e integral. Debe ser dif{\'{i}}cil, porque son muy pocos quienes logran aprobarla en un s{\'{o}}lo a{\~{n}}o y es muy alto el porcentaje de abandono. Con este libro quiero ayudarle en sus estudios de C{\'{a}}lculo o An{\'{a}}lisis Matem{\'{a}}tico, no solamente para que logre una buena calificaci{\'{o}}n sino para que saque de ellos el mayor provecho e incluso aprenda a disfrutarlos.},
author = {{P{\'{e}}rez Gonz{\'{a}}lez}, Francisco},
journal = {C{\'{a}}lculo diferencial ei ntegral},
number = {18},
pages = {87--93},
title = {{C{\'{a}}lculo Diferencial e Integral de Funciones de una Variable}},
url = {http://www.ugr.es/{\%}7B{~}{\%}7Dfjperez/textos/calculo{\_}diferencial{\_}integral{\_}func{\_}una{\_}var.pdf},
volume = {6},
year = {2018}
}

@article{Percival,
author = {Percival, C},
title = {{Stronger key derivation via sequential memory-hard functions colin percival}}
}

@article{Peng2019,
abstract = {Fully homomorphic encryption is a promising crypto primitive to encrypt your data while allowing others to compute on the encrypted data. But there are many well-known problems with fully homomorphic encryption such as CCA security and circuit privacy problem. Despite these problems, there are still many companies are currently using or preparing to use fully homomorphic encryption to build data security applications. It seems that the full homomorphic encryption is very close to practicality and these problems can be easily mitigated in implementation. Although the those problems are well known in theory, there is no public discussion of their actual impact on real application. Our research shows that there are many security pitfalls in fully homomorphic encryption from the perspective of practical application. The security problems of a fully homomorphic encryption in a real application is more severe than imagined. In this paper, we will take Microsoft SEAL as an examples to introduce the security pitfalls of fully homomorphic encryption from the perspective of implementation and practical application},
archivePrefix = {arXiv},
arxivId = {1906.07127},
author = {Peng, Zhiniang},
eprint = {1906.07127},
month = {jun},
title = {{Danger of using fully homomorphic encryption: A look at Microsoft SEAL}},
url = {http://arxiv.org/abs/1906.07127},
year = {2019}
}

@article{Pelt2017,
abstract = {Deep convolutional neural networks have been successfully applied to many image-processing problems in recent works. Popular network architectures often add additional operations and connections to the standard architecture to enable training deeper networks. To achieve accurate results in practice, a large number of trainable parameters are often required. Here, we introduce a network architecture based on using dilated convolutions to capture features at different image scales and densely connecting all feature maps with each other. The resulting architecture is able to achieve accurate results with relatively few parameters and consists of a single set of operations, making it easier to implement, train, and apply in practice, and automatically adapts to different problems. We compare results of the proposed network architecture with popular existing architectures for several segmentation problems, showing that the proposed architecture is able to achieve accurate results with fewer parameters, with a reduced risk of overfitting the training data.},
author = {Pelt, Dani{\"{e}}l M and Sethian, James A},
doi = {10.1073/pnas.1715832114},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {2},
pages = {254--259},
title = {{A mixed-scale dense convolutional neural network for image analysis}},
volume = {115},
year = {2017}
}

@article{Pease2009,
abstract = {Standard Upper Ontology Knowledge Interchange Format (SUO-KIF) is a language designed for use in the authoring and interchange of knowledge. SUO-KIF has declarative semantics. It is possible to understand the meaning of expressions in the language without appeal to an interpreter for manipulating those expressions. In this way, KIF differs from other languages that are based on specific interpreters, such as Emycin and Prolog. SUO-KIF is also logically comprehensive – at its most general, it provides for the expression of arbitrary logical sentences. In this way, it differs from relational database languages (like SQL) and logic programming languages (like Prolog). SUOKIF is intended primarily a first-order language, which is a good compromise between the computational demands of reasoning and richness of representation. In later sections of this document we describe ways in which SUO-KIF is made as expressive as possible without becoming higher-order.},
author = {Pease, Adam},
journal = {Standard Upper Ontology Knowledge Interchange Format},
title = {{Standard Upper Ontology Knowledge Interchange Format}},
year = {2009}
}

@book{Pearson,
editor = {Pearson},
publisher = {LONGMAN},
title = {{Essay Activator: Your Key to Writing Success}}
}

@article{Payan2015,
abstract = {Pattern recognition methods using neuroimaging data for the diagnosis of Alzheimer's disease have been the subject of extensive research in recent years. In this paper, we use deep learning methods, and in particular sparse autoencoders and 3D convolutional neural networks, to build an algorithm that can predict the disease status of a patient, based on an MRI scan of the brain. We report on experiments using the ADNI data set involving 2,265 historical scans. We demonstrate that 3D convolutional neural networks outperform several other classifiers reported in the literature and produce state-of-art results.},
archivePrefix = {arXiv},
arxivId = {1502.02506},
author = {Payan, Adrien and Montana, Giovanni},
eprint = {1502.02506},
month = {feb},
title = {{Predicting Alzheimer's disease: a neuroimaging study with 3D convolutional neural networks}},
url = {http://arxiv.org/abs/1502.02506},
year = {2015}
}

@article{Paulin-Mohring2012a,
abstract = {This paper is a tutorial on using the Coq proof-assistant for reasoning on software correctness. It illustrates features of Coq like inductive definitions and proof automation on a few examples including arithmetic, algorithms on functional and imperative lists and cryptographic protocols. Coq is not a tool dedicated to software verification but a general purpose environment for developing mathematical proofs. However, it is based on a powerful language including basic functional programming and high-level specifications. As such it offers modern ways to literally program proofs in a structured way with advanced data-types, proofs by computation, and general purpose libraries of definitions and lemmas. Coq is well suited for software verification of programs involving advanced specifications like language semantics and real numbers. The Coq architecture is also based on a small trusted kernel, making possible to use third-party libraries while being sure that proofs are not compromised. {\textcopyright}2012 Springer-Verlag.},
author = {Paulin-Mohring, Christine},
doi = {10.1007/978-3-642-35746-6_3},
isbn = {9783642357459},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {45--95},
title = {{Introduction to the Coq proof-assistant for practical software verification}},
volume = {7682 LNCS},
year = {2012}
}

@article{Parr2018,
abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way---just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here. See related articles at http://explained.ai},
archivePrefix = {arXiv},
arxivId = {1802.01528},
author = {Parr, Terence and Howard, Jeremy},
eprint = {1802.01528},
month = {feb},
title = {{The Matrix Calculus You Need For Deep Learning}},
url = {http://arxiv.org/abs/1802.01528},
year = {2018}
}

@article{Pan2006,
abstract = {We present a novel hybrid scheme called “hyper-resolution” that integrates image probability-filtering-based interpolation and improved Poisson maximum a posteriori (MAP) super-resolution to respectively enhance high spatial and spatial-frequency res- olutions ofa single down-sampled image. A new approach to interpolation is proposed for simultaneous image interpolation and smoothing by exploiting the probability filter coupled with a pyramidal decomposition and the Poisson MAP super-resolution is improved with the techniques of edge maps and pseudo-blurring. Simulation results demonstrate that this hyper-resolution scheme substantially improves the quality of a single gray-level, color, or noisy image, respectively.},
author = {Pan, Min Cheng},
doi = {10.1155/ASP/2006/97492},
issn = {11108657},
journal = {Eurasip Journal on Applied Signal Processing},
pages = {1--9},
title = {{Improving a Single Down-Sampled Image using Probability-Filtering-Based Interpolation and Improved Poisson Maximum APosteriori Super-Resolution}},
volume = {2006},
year = {2006}
}

@book{Palach2014,
abstract = {Develop efficient parallel systems using the robust Python environment},
author = {Palach, Jan},
isbn = {9781783288397},
pages = {124},
publisher = {Packt Publishing},
title = {{Parallel Programming with Python}},
url = {www.it-ebooks.info},
year = {2014}
}

@article{Ousterhout2010,
abstract = {Disk-oriented approaches to online storage are becoming increasingly problematic: they do not scale gracefully to meet the needs of large-scale Web applications, and improvements in disk capacity have far out-stripped improvements in access latency and bandwidth. This paper argues for a new approach to datacenter storage called RAMCloud, where information is kept entirely in DRAM and large-scale systems are created by aggregating the main memories of thousands of commodity servers. We believe that RAMClouds can provide durable and available storage with 100-1000x the throughput of disk-based systems and 100-1000x lower access latency. The combination of low latency and large scale will enable a new breed of data-intensive applications.},
author = {Ousterhout, John and Rosenblum, Mendel and Rumble, Stephen M and Stratmann, Eric and Stutsman, Ryan and Agrawal, Parag and Erickson, David and Kozyrakis, Christos and Leverich, Jacob and Mazi{\`{e}}res, David and Mitra, Subhasish and Narayanan, Aravind and Parulkar, Guru},
doi = {10.1145/1713254.1713276},
issn = {01635980},
journal = {ACM SIGOPS Operating Systems Review},
number = {4},
pages = {92},
title = {{The case for RAMClouds}},
url = {http://portal.acm.org/citation.cfm?doid=1713254.1713276},
volume = {43},
year = {2010}
}

@book{Osterwalder2010,
author = {Osterwalder, Alexander and Pigneur, Yves and Smith, Alan},
isbn = {9780470876411},
pages = {288},
title = {{Business Model Generation}},
year = {2010}
}

@article{Orus2014,
abstract = {This is a partly non-technical introduction to selected topics on tensor network methods, based on several lectures and introductory seminars given on the subject. It should be a good place for newcomers to get familiarized with some of the key ideas in the field, specially regarding the numerics. After a very general introduction we motivate the concept of tensor network and provide several examples. We then move on to explain some basics about Matrix Product States (MPS) and Projected Entangled Pair States (PEPS). Selected details on some of the associated numerical methods for 1. d and 2. d quantum lattice systems are also discussed. {\textcopyright}2014 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {1306.2164},
author = {Or{\'{u}}s, Rom{\'{a}}n},
doi = {10.1016/j.aop.2014.06.013},
eprint = {1306.2164},
issn = {1096035X},
journal = {Annals of Physics},
keywords = {Entanglement,MPS,PEPS,Tensor networks},
pages = {117--158},
title = {{A practical introduction to tensor networks: Matrix product states and projected entangled pair states}},
volume = {349},
year = {2014}
}

@article{Org,
author = {Org, The and Developers, Mode},
title = {{Org Mode Compact Guide}}
}

@book{Ørberg2003,
abstract = {Hans Oerberg's Lingua Latina per se illustrata is the world's premiere series for learning Latin via the Natural Method. Students first learn grammar and vocabulary intuitively through extended contextual reading and an innovative system of marginal notes. It is the only textbook currently available that gives students the opportunity to learn Latin without resorting to translation, but allows them to think in the language. It is also the most popular text for teachers, at both the secondary and collegiate levels, who wish to incorporate conversational skills into their classroom practice.},
author = {{\O}rberg, Hans H},
isbn = {8799701650},
pages = {328},
publisher = {Museum Tusculanum Press},
title = {{Lingua Latina per se illustrata}},
year = {2003}
}

@book{Orberg2003,
abstract = {Pars I - Familia Romana},
author = {Orberg, Hans H},
isbn = {8799701650},
pages = {331},
title = {{Lingua Latina Per Se Illustrata}},
year = {2003}
}

@article{OpenSourceModelicaConsortium2018a,
author = {{Open Source Modelica Consortium}},
title = {{OpenModelica User ' s Guide}},
year = {2018}
}

@article{OpenSourceModelicaConsortium2018a,
author = {{Open Source Modelica Consortium}},
title = {{OpenModelica User ' s Guide}},
year = {2018}
}

@article{OMG2017,
abstract = {This specification defines the Unified Modeling Language (UML), revision 2. The objective of UML is to provide system architects, software engineers, and software developers with tools for analysis, design, and implementation of softwarebased systems as well as for modeling business and similar processes. The initial versions of UML (UML 1) originated with three leading object-oriented methods (Booch, OMT, and OOSE), and incorporated a number of best practices from modeling language design, object-oriented programming, and architectural description languages. Relative to UML 1, this revision of UML has been enhanced with significantly more precise definitions of its abstract syntax rules and semantics, a more modular language structure, and a greatly improved capability for modeling large-scale systems. One of the primary goals of UML is to advance the state of the industry by enabling object visual modeling tool interoperability. However, to enable meaningful exchange of model information between tools, agreement on semantics and notation is required. UML meets the following requirements: A formal definition of a common MOF-based metamodel that specifies the abstract syntax of the UML. The abstract syntax defines the set of UML modeling concepts, their attributes and their relationships, as well as the rules for combining these concepts to construct partial or complete UML models. A detailed explanation of the semantics of each UML modeling concept. The semantics define, in a technologyindependent manner, how the UML concepts are to be realized by computers. A specification of the human-readable notation elements for representing the individual UML modeling concepts as well as rules for combining them into a variety of different diagram types corresponding to different aspects of modeled systems. A detailed definition of ways in which UML tools can be made compliant with this specification. This is supported (in a separate specification) with an XML-based specification of corresponding model interchange formats (XMI) that must be realized by compliant tools.},
author = {OMG},
journal = {Object Management Group},
number = {December},
pages = {796},
title = {{Unified modeling language (OMG UML)}},
url = {https://www.omg.org/spec/UML/2.5.1/PDF},
year = {2017}
}

@book{Oliveira2008,
author = {Oliveira, J N},
pages = {290},
title = {{Program Design By Calculation (Draft)}},
year = {2019}
}

@article{Norell2009,
abstract = {Dependently typed languages have for a long time been used to describe proofs about programs. Traditionally, dependent types are used mostly for stating and proving the properties of the programs and not in defining the programs themselves. An impressive example is the certified compiler by Leroy (2006) implemented and proved correct in Coq (Bertot and Cast{\'{e}}ran 2004). Recently there has been an increased interest in dependently typed programming, where the aim is to write programs that use the dependent type system to a much higher degree. In this way a lot of the properties that were previously proved separately can be integrated in the type of the program, in many cases adding little or no complexity to the definition of the program. New languages, such as Epigram (McBride and McKinna 2004), are being designed, and existing languages are being extended with new features to accomodate these ideas, for instance the work on dependently typed programming in Coq by Sozeau (2007). This talk gives an overview of the Agda programming language (Norell 2007), whose main focus is on dependently typed programming. Agda provides a rich set of inductive types with a powerful mechanism for pattern matching, allowing dependently typed programs to be written with minimal fuss. To read about programming in Agda, see the lecture notes from the Advanced Functional Programming summer school (Norell 2008) and the work by Oury and Swierstra (2008). In the talk a number of examples of interesting dependently typed programs chosen from the domain of programming language implementation are presented as they are implemented in Agda.},
author = {Norell, Ulf},
doi = {10.1007/978-3-642-04652-0_5},
isbn = {3642046517},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {Agda 2},
pages = {230--266},
title = {{Dependently typed programming in Agda}},
volume = {5832 LNCS},
year = {2009}
}

@article{Nogueira2006,
author = {Nogueira, Pablo},
number = {March},
pages = {1--25},
title = {{Bits of Category Theory}},
volume = {2},
year = {2006}
}

@article{Niknami2019,
abstract = {Constant-factor differences are frequently ignored when analyzing the complexity of algorithms and implementations, as they appear to be insignificant in practice In this paper, we demonstrate that this assumption can in fact have far more profound implications on time complexity than is obvious at first glance, and that a poor consideration of trade-offs can result in {\$}\backslash{\$}textit{\{}polynomially{\}} slower algorithms whose roots can be deeply and fundamentally ingrained into a programming language itself While the general observation may not be novel from a theoretical standpoint, it is rarely (if ever) presented in traditional computer science curricula or other settings, and appears to be far from common knowledge in practical software engineering We thus hope bring awareness to this issue and urge careful consideration of significant trade-offs that can result from trivial decisions made while programming.},
archivePrefix = {arXiv},
arxivId = {1911.12338},
author = {Niknami, Mehrdad},
eprint = {1911.12338},
month = {nov},
title = {{Inflationary Constant Factors and Why Python is Faster Than C++}},
url = {http://arxiv.org/abs/1911.12338},
year = {2019}
}

@article{Ngo2017,
abstract = {In the paper, we present a method for decomposing a discrete noisy curve into arcs and segments which are the frequent primitives in digital images. This method is based on two tools: dominant point detection using adaptive tangential cover and tangent space representation of the polygon issued from detected dominant points. The experiments demonstrate the robustness of the method w.r.t. noise.},
author = {Ngo, Phuc and Nasser, Hayat and Debled-Rennesson, Isabelle},
doi = {10.1007/978-3-319-54427-4_36},
isbn = {9783319544267},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Adaptive tangential cover,Curve reconstruction,Dominant point detection,Tangent space,Vectorization},
pages = {493--505},
title = {{A discrete approach for decomposing noisy digital contours into arcs and segments}},
volume = {10117 LNCS},
year = {2017}
}

@article{Nasonov2012,
author = {Nasonov, Andrey V and Krylov, Andrey S},
doi = {10.1109/CISP.2012.6469851},
isbn = {9781467309622},
journal = {2012 5th International Congress on Image and Signal Processing, CISP 2012},
number = {1},
pages = {617--620},
title = {{Text images superresolution and enhancement}},
year = {2012}
}

@article{Nakamoto2008,
abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
archivePrefix = {arXiv},
arxivId = {43543534534v343453},
author = {Nakamoto, Satoshi},
doi = {10.1007/s10838-008-9062-0},
eprint = {43543534534v343453},
isbn = {978-972-757-716-3},
issn = {09254560},
journal = {Journal for General Philosophy of Science},
number = {1},
pages = {1--9},
pmid = {14533183},
title = {{Bitcoin: A Peer-to-Peer Electronic Cash SyNakamoto, S. (2008). Bitcoin: A Peer-to-Peer Electronic Cash System. Consulted, 1–9.}},
year = {2008}
}

@book{MurrayR.Spiegel2009,
abstract = {Fully compatible with your classroom text, Schaum's highlights all the important facts you need to know. The book contains hundreds of examples, solved problems, and practice exercises to test your skills.},
author = {{Murray R. Spiegel} and Lipschutz, Seymour and Schiller, John J and Spellman, Dennis},
booktitle = {Schaum's Outline Series},
isbn = {9780071615709},
title = {{Schaum's Outline of Complex Variables}},
year = {2009}
}

@article{Murali2018,
abstract = {We study the problem of generating source code in a strongly typed, Java-like programming language, given a label (for example a set of API calls or types) carrying a small amount of information about the code that is desired. The generated programs are expected to respect a “realistic” relationship between programs and labels, as exemplified by a corpus of labeled programs available during training. Two challenges in such conditional program generation are that the generated programs must satisfy a rich set of syntactic and semantic constraints, and that source code contains many low-level features that impede learning. We address these problems by training a neural generator not on code but on program sketches, or models of program syntax that abstract out names and operations that do not generalize across programs. During generation, we infer a posterior distribution over sketches, then concretize samples from this distribution into type-safe programs using combinatorial techniques. We implement our ideas in a system for generating API-heavy Java code, and show that it can often predict the entire body of a method given just a few API calls or data types that appear in the method.},
archivePrefix = {arXiv},
arxivId = {1703.05698},
author = {Murali, Vijayaraghavan and Qi, Letao and Chaudhuri, Swarat and Jermaine, Chris},
eprint = {1703.05698},
pages = {1--17},
title = {{Neural Sketch Learning for Conditional Program Generation}},
url = {https://arxiv.org/abs/1703.05698},
year = {2018}
}

@article{Munn2019,
author = {Munn, Alan},
title = {{A one page, dictatorial guide to LATEX packages}},
year = {2019}
}

@book{Mqef2003,
abstract = {Mathmatics for economists},
address = {London},
author = {Mqef, Guillaume Carlier},
booktitle = {Quantitative literacy: Why numeracy matters for schools},
doi = {10.1007/b97511},
isbn = {1-85233-330-8},
number = {c},
pages = {533--540},
publisher = {Springer-Verlag},
series = {Springer Undergraduate Mathematics Series},
title = {{Mathematics for change Finance}},
url = {http://www.maa.org/external{\_}archive/QL/pgs75{\_}89.pdf http://link.springer.com/10.1007/b97511},
year = {2003}
}

@article{Moses1971,
abstract = {Three approaches to symbolic integration in the 1960's are described. The first, from artificial intelligence, led to Slagle's SAINT and to a large degree to Moses' SIN. The second, from algebraic manipulation, led to Manove's implementation and to Horowitz' and Tobey's reexamination of the Hermite algorithm for integrating rational functions. The third, from mathematics, led to Richardson's proof of the unsolvability of the problem for a class of functions and for Risch's decision procedure for the elementary functions. Generalizations of Risch's algorithm to a class of special functions and programs for solving differential equations and for finding the definite integral are also described. {\textcopyright}1971, ACM. All rights reserved.},
author = {Moses, Joel},
doi = {10.1145/362637.362651},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {definite integrals,integration,rational functions,symbolic integration},
number = {8},
pages = {548--560},
title = {{Symbolic integration: the stormy decade}},
url = {http://portal.acm.org/citation.cfm?doid=362637.362651},
volume = {14},
year = {1971}
}

@article{Moseley2006a,
abstract = {Complexity is the single major difficulty in the successful development of large-scale software systems. Following Brooks we distinguish accidental from essential difficulty, but disagree with his premise that most complexity remaining in contemporary systems is essential. We identify common causes of complexity and discuss general approaches which can be taken to eliminate them where they are accidental in nature. To make things more concrete we then give an outline for a potential complexity-minimizing approach based on functional programming and Codd's relational model of data.},
author = {Moseley, Ben and Marks, Peter},
doi = {10.1.1.93.8928},
journal = {Complexity},
pages = {1--66},
title = {{Out of the tar pit}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.8928{\&}rep=rep1{\&}type=pdf},
year = {2006}
}

@article{Morse2009,
abstract = {Trace diagrams are structured graphs with edges labeled by matrices. Each diagram has an interpretation as a particular multilinear function. We provide a rigorous combinatorial definition of these diagrams using a notion of signed graph coloring, and prove that they may be efficiently represented in terms of matrix minors. Using this viewpoint, we provide new proofs of several standard determinant formulas and a new generalization of the Jacobi determinant theorem.},
archivePrefix = {arXiv},
arxivId = {0903.1373v1},
author = {Morse, Steven and Peterson, Elisha},
doi = {10.2140/involve.2010.3.33},
eprint = {0903.1373v1},
journal = {Arxiv preprint arXiv09031373},
keywords = {combinatorics},
number = {1},
pages = {1--39},
title = {{Trace Diagrams, Matrix Minors, and Determinant Identities}},
url = {http://arxiv.org/abs/0903.1373},
year = {2009}
}

@book{Morgan,
author = {Morgan, Carroll},
edition = {2nd},
isbn = {9780137262250},
publisher = {Prentice Hall},
series = {Prentice Hall International Series in Computing Science},
title = {{Programming from Specifications}},
url = {http://gen.lib.rus.ec/book/index.php?md5=5c41c258c8fbe68d025074957fc0e5b3 http://www.cs.ox.ac.uk/publications/books/PfS/}
}

@book{Miller2013,
author = {Miller, Brad and Ranum, David},
isbn = {9781590282571},
pages = {240},
title = {{Problem Solving with Algorithms and Data Structures - Python}},
year = {2013}
}

@book{Milewski2019,
author = {Milewski, Bartosz},
title = {{Category Theory for Programmers}},
url = {https://github.com/hmemcpy/milewski-ctfp-pdf},
year = {2019}
}

@article{Mihalcea,
abstract = {In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innova- tive unsupervised methods for keyword and sentence extraction, and show that the results obtained com- pare favorably with previously published results on established benchmarks.},
author = {Mihalcea, R and Tarau, P},
keywords = {2,3-diphosphoglycerate},
title = {{TextRank: Bringing Order into Texts}}
}

@techreport{MichaelBarr1999,
author = {{Michael Barr} and {Charles Wells}},
keywords = {明治期の地方自治権論},
title = {{Category Theory Lecture Notes for ESSLLI}},
year = {1999}
}

@article{Metzger2001,
abstract = {Parallel computation will become the norm in the coming decades. Unfortunately, advances in parallel hardware have far outpaced parallel applications of software. There are currently two approaches to applying parallelism to applications. One is to write completely new applications in new languages. But abandoning applications that work is unacceptable to most nonacademic users of high-performance computers. The other approach is to convert existing applications to a parallel form. This can be done manually or automatically. Even partial success in doing the job automatically has obvious economic advantages. This book describes a fundamentally new theoretical framework for finding poor algorithms in an application program and replacing them with ones that parallelize the code.},
author = {Metzger, Robert and Wen, Zhaofang},
doi = {10.1108/k.2001.06730aae.005},
issn = {0368-492X},
journal = {Kybernetes},
number = {1},
title = {{Automatic Algorithm Recognition and Replacement: A New Approach to Program Optimisation}},
volume = {30},
year = {2001}
}

@book{Merritt2000,
author = {Merritt, Dennis},
isbn = {1461389119},
month = {jun},
publisher = {Springer Science {\&} Business Media},
title = {{Building Expert Systems in Prolog}},
year = {2000}
}

@article{Meijer1991,
abstract = {We develop a calculus for lazy functional programming based on recursion operators associated with data type definitions. For these operators we derive various algebraic laws that are useful in deriving and manipulating programs. We shall show that all example functions in Bird and Wadler's “Introduction to Functional Programming” can be expressed using these operators.},
author = {Meijer, Erik and Fokkinga, Maarten and Paterson, Ross},
doi = {10.1007/3540543961_7},
isbn = {9783540475996},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {124--144},
title = {{Functional programming with bananas, lenses, envelopes and barbed wire}},
volume = {523 LNCS},
year = {1991}
}

@inproceedings{McMinn2011,
author = {McMinn, Phil},
booktitle = {2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops},
doi = {10.1109/ICSTW.2011.100},
isbn = {978-1-4577-0019-4},
month = {mar},
pages = {153--163},
publisher = {IEEE},
title = {{Search-Based Software Testing: Past, Present and Future}},
url = {http://ieeexplore.ieee.org/document/5954405/},
year = {2011}
}

@article{McLaughlin2015,
abstract = {In this paper we explore ways to address the issue of dataset bias in person re-identification by using data augmentation to increase the variability of the available datasets, and we introduce a novel data augmentation method for re-identification based on changing the image background. We show that use of data augmentation can improve the cross-dataset generalisation of convolutional network based re-identification systems, and that changing the image background yields further improvements. {\textcopyright}2015 IEEE.},
author = {McLaughlin, Niall and {Del Rincon}, Jesus Martinez and Miller, Paul},
doi = {10.1109/AVSS.2015.7301739},
isbn = {9781467376327},
journal = {AVSS 2015 - 12th IEEE International Conference on Advanced Video and Signal Based Surveillance},
keywords = {Accuracy,Cameras,Image color analysis,Lighting,Standards,Testing,Training},
number = {2015},
title = {{Data-augmentation for reducing dataset bias in person re-identification}},
year = {2015}
}

@article{McDermott2004,
abstract = {MRI scanners enable fast, noninvasive, and high-resolution imaging of organs and soft tissue. The images are reconstructed from NMR signals generated by nuclear spins that precess in a static magnetic field B0 in the presence of magnetic field gradients. Most clinical MRI scanners operate at a magnetic field B0 = 1.5 T, corresponding to a proton resonance frequency of 64 MHz. Because these systems rely on large superconducting magnets, they are costly and demanding of infrastructure. On the other hand, low-field imagers have the potential to be less expensive, less confining, and more mobile. The major obstacle is the intrinsically low sensitivity of the low-field NMR experiment. Here, we show that prepolarization of the nuclear spins and detection with a superconducting quantum interference device (SQUID) yield a signal that is independent of B0, allowing acquisition of high-resolution MRIs in microtesla fields. Reduction of the strength of the measurement field eliminates inhomogeneous broadening of the NMR lines, resulting in enhanced signal-to-noise ratio and spatial resolution for a fixed strength of the magnetic field gradients used to encode the image. We present high-resolution images of phantoms and other samples and T1-weighted contrast images acquired in highly inhomogeneous magnetic fields of 132 microT; here, T1 is the spin-lattice relaxation time. These techniques could readily be adapted to existing multichannel SQUID systems used for magnetic source imaging of brain signals. Further potential applications include low-cost systems for tumor screening and imaging peripheral regions of the body.},
author = {McDermott, Robert and Lee, SeungKyun and ten Haken, Bennie and Trabesinger, Andreas H and Pines, Alexander and Clarke, John},
doi = {10.1073/pnas.0402382101},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {may},
number = {21},
pages = {7857--7861},
pmid = {15141077},
publisher = {National Academy of Sciences},
title = {{Microtesla MRI with a superconducting quantum interference device.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15141077 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC419521},
volume = {101},
year = {2004}
}

@inproceedings{McDermott2017,
abstract = {While tree-based Genetic Programming is often used with crossover, Cartesian Genetic Programming is mostly used only with mutation as genetic operator. In this paper, a new crossover technique is introduced which recombines subgraphs of two selected graphs. Experiments on symbolic regression, boolean functions and image operator design problems indicate that the use of the subgraph crossover improves the search performance of Cartesian Genetic Programming. A preliminary comparison to a former proposed crossover technique indicates that the subgraph crossover performs better on our tested problems.},
address = {Amsterdam},
booktitle = {20th European Conference, EuroGP 2017},
doi = {10.1007/978-3-319-55696-3},
editor = {McDermott, James and Castelli, Mauro and Sekanina, Lukas and Haasdijk, Evert and Garc{\'{i}}a-S{\'{a}}nchez, Pablo},
isbn = {978-3-319-55695-6},
issn = {16113349},
keywords = {Cartesian genetic programming,Crossover,Recombination},
pages = {359},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Genetic Programming}},
url = {http://link.springer.com/10.1007/978-3-319-55696-3},
year = {2017}
}

@book{Masenge,
author = {Masenge, Ralph W P},
title = {{Numerical Methods}}
}

@book{Martin2011,
author = {Martin, John C},
isbn = {9780073191461},
title = {{Introduction to Languages and the Theory of Computation}},
year = {2011}
}

@article{Marsaglia1968,
author = {Marsaglia, G},
doi = {10.1073/pnas.61.1.25},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {1},
pages = {25--28},
pmid = {16591687},
publisher = {National Academy of Sciences},
title = {{Random numbers fall mainly in the planes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16591687 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC285899},
volume = {61},
year = {1968}
}

@article{Marron2019,
author = {Marron, Mark},
title = {{Regularized Programming with the Bosque Language}},
year = {2019}
}

@article{Margalit2019,
author = {Margalit, Dan and Rabinoff, Joseph},
pages = {1--9},
title = {{Interactive Linear Algebra}},
year = {2019}
}

@article{Manhaeve2019,
abstract = {We introduce DeepProbLog, a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques of the underlying probabilistic logic programming language ProbLog can be adapted for the new language. We theoretically and experimentally demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
archivePrefix = {arXiv},
arxivId = {1907.08194},
author = {Manhaeve, Robin and Duman{\v{c}}i{\'{c}}, Sebastijan and Kimmig, Angelika and Demeester, Thomas and {De Raedt}, Luc},
eprint = {1907.08194},
month = {jul},
title = {{DeepProbLog: Neural Probabilistic Logic Programming}},
url = {http://arxiv.org/abs/1907.08194},
year = {2019}
}

@article{Mancas-Thillou2005,
abstract = {We propose a super-resolution technique specifically aimed at enhancing low-resolution text images from handheld devices. The Teager filter, a quadratic unsharp masking filter, is used to highlight high frequencies which are then combined with the warped and interpolated image sequence following motion estimation using Taylor series decomposition. Comparative performance evaluation is presented in the form of OCR results of the super-resolution output.},
author = {Mancas-Thillou, C and Mirmehdi, Majid and Copernic, Avenue},
journal = {First International Workshop on Camera- {\ldots}},
pages = {10--16},
title = {{Super-resolution text using the teager filter}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.3444{\&}rep=rep1{\&}type=pdf},
year = {2005}
}

@book{Maheshwari2016,
abstract = {This is a free textbook for an undergraduate course on the Theory of Com- putation, which we have been teaching at Carleton University since 2002. Until the 2011/2012 academic year, this course was offered as a second-year course (COMP 2805) and was compulsory for all Computer Science students. Starting with the 2012/2013 academic year, the course has been downgraded to a third-year optional course (COMP 3803).},
author = {Maheshwari, Anil and Smid, Michiel},
title = {{Introduction to theory of computation}},
year = {2016}
}

@book{MacKay2003,
abstract = {Information theory and inference, often taught separately, are here united in one entertaining textbook. These topics lie at the heart of many exciting areas of contemporary science and engineering - communication, signal processing, data mining, machine learning, pattern recognition, computational neuroscience, bioinformatics, and cryptography. This textbook introduces theory in tandem with applications. Information theory is taught alongside practical communication systems, such as arithmetic coding for data compression and sparse-graph codes for error-correction. A toolbox of inference techniques, including message-passing algorithms, Monte Carlo methods, and variational approximations, are developed alongside applications of these tools to clustering, convolutional codes, independent component analysis, and neural networks. The final part of the book describes the state of the art in error-correcting codes, including low-density parity-check codes, turbo codes, and digital fountain codes -- the twenty-first century standards for satellite communications, disk drives, and data broadcast. Richly illustrated, filled with worked examples and over 400 exercises, some with detailed solutions, David MacKay's groundbreaking book is ideal for self-learning and for undergraduate or graduate courses. Interludes on crosswords, evolution, and sex provide entertainment along the way. In sum, this is a textbook on information, communication, and coding for a new generation of students, and an unparalleled entry point into these subjects for professionals in areas as diverse as computational biology, financial engineering, and machine learning.},
author = {MacKay, David J C},
booktitle = {Advanced Science Letters},
isbn = {0521642981},
pages = {628},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference and Learning Algorithms}},
year = {2003}
}

@book{MacKay2005,
author = {MacKay, D},
title = {{Information Theory, Inference, and Learning Algorithms}},
url = {http://www.cambridge.org/0521642981},
year = {2005}
}

@article{Maass1997,
author = {Maass, W},
journal = {Neural Netw.},
pages = {1659--1671},
title = {{Networks of spiking neurons: the third generation of neural network models}},
volume = {10},
year = {1997}
}

@article{Lyuehh2018,
abstract = {数据科学是OSEMN（和 awesome 相同发音），它包括获取(Obtaining)、整理(Scrubbing)、探索(Exploring)、建模(Modeling)和翻译(iNterpreting)数据。作为一名数据科学家，我用命令行的时间非常长，尤其是要获取、整理和探索数据的时候。而且我也不是唯一一个这样做的人。},
author = {Lyuehh},
isbn = {9781484222522},
number = {703},
pages = {0--2},
title = {{Data science cheatsheet}},
url = {http://blog.jobbole.com/54308/},
year = {2018}
}

@article{Ly2018,
abstract = {Atrophy of neurons in the prefrontal cortex (PFC) plays a key role in the pathophysiology of depression and related disorders. The ability to promote both structural and functional plasticity in the PFC has been hypothesized to underlie the fast-acting antidepressant properties of the dissociative anesthetic ketamine. Here, we report that, like ketamine, serotonergic psychedelics are capable of robustly increasing neuritogenesis and/or spinogenesis both in vitro and in vivo. These changes in neuronal structure are accompanied by increased synapse number and function, as measured by fluorescence microscopy and electrophysiology. The structural changes induced by psychedelics appear to result from stimulation of the TrkB, mTOR, and 5-HT2A signaling pathways and could possibly explain the clinical effectiveness of these compounds. Our results underscore the therapeutic potential of psychedelics and, importantly, identify several lead scaffolds for medicinal chemistry efforts focused on developing plasticity-promoting compounds as safe, effective, and fast-acting treatments for depression and related disorders. Ly et al. demonstrate that psychedelic compounds such as LSD, DMT, and DOI increase dendritic arbor complexity, promote dendritic spine growth, and stimulate synapse formation. These cellular effects are similar to those produced by the fast-acting antidepressant ketamine and highlight the potential of psychedelics for treating depression and related disorders.},
author = {Ly, Calvin and Greb, Alexandra C and Cameron, Lindsay P and Wong, Jonathan M and Barragan, Eden V and Wilson, Paige C and Burbach, Kyle F and {Soltanzadeh Zarandi}, Sina and Sood, Alexander and Paddy, Michael R and Duim, Whitney C and Dennis, Megan Y and McAllister, A Kimberley and Ori-McKenney, Kassandra M and Gray, John A and Olson, David E},
doi = {10.1016/j.celrep.2018.05.022},
issn = {22111247},
journal = {Cell Reports},
keywords = {DMT,LSD,MDMA,depression,ketamine,neural plasticity,noribogaine,psychedelic,spinogenesis,synaptogenesis},
number = {11},
pages = {3170--3182},
publisher = {ElsevierCompany.},
title = {{Psychedelics Promote Structural and Functional Neural Plasticity}},
url = {https://doi.org/10.1016/j.celrep.2018.05.022},
volume = {23},
year = {2018}
}

@article{Luo2019,
author = {Luo, Yi-Han and Zhong, Han-Sen and Erhard, Manuel and Wang, Xi-Lin and Peng, Li-Chao and Krenn, Mario and Jiang, Xiao and Li, Li and Liu, Nai-Le and Lu, Chao-Yang and Zeilinger, Anton and Pan, Jian-Wei},
doi = {10.1103/PhysRevLett.123.070505},
issn = {0031-9007},
journal = {Physical Review Letters},
number = {7},
pages = {70505},
title = {{Quantum Teleportation in High Dimensions}},
url = {https://link.aps.org/doi/10.1103/PhysRevLett.123.070505},
volume = {123},
year = {2019}
}

@book{Reference,
abstract = {An essential one-stop resource-nine convenient minibooks in a single 840page volume-for network administrators everywhere This value-priced package includes sections on networking basics, building a network, network administration, TCP/IP and the Internet, wireless and home networking, Windows 2000 and 2003 servers, NetWare 6, Linux networking, and Mac OS X networking Written by the author of the perennial bestseller Networking For Dummies (0-7645-1677-9), this massive reference covers all the topics that administrators routinely handle Provides key information, explanations, and procedures for configuration, Internet connectivity, security, and wireless options on today's most popular networking platforms},
author = {Lowe, Doug},
edition = {For Dummie},
isbn = {0-7645-4260-5},
keywords = {adapter additional administrator allow appears aut},
pages = {840},
publisher = {John Wiley {\&} Sons},
title = {{Networking All-in-One Desk Reference For Dummies}},
year = {2004}
}

@article{Lopez2017,
abstract = {Convolutional Neural Network (CNNs) are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP.},
archivePrefix = {arXiv},
arxivId = {1703.03091},
author = {Lopez, Marc Moreno and Kalita, Jugal},
eprint = {1703.03091},
title = {{Deep Learning applied to NLP}},
url = {http://arxiv.org/abs/1703.03091},
year = {2017}
}

@article{Liu2020,
abstract = {Harvesting energy from the environment offers the promise of clean power for self-sustained systems1,2. Known technologies—such as solar cells, thermoelectric devices and mechanical generators—have specific environmental requirements that restrict where they can be deployed and limit their potential for continuous energy production3–5. The ubiquity of atmospheric moisture offers an alternative. However, existing moisture-based energy-harvesting technologies can produce only intermittent, brief (shorter than 50 seconds) bursts of power in the ambient environment, owing to the lack of a sustained conversion mechanism6–12. Here we show that thin-film devices made from nanometre-scale protein wires harvested from the microbe Geobacter sulfurreducens can generate continuous electric power in the ambient environment. The devices produce a sustained voltage of around 0.5 volts across a 7-micrometre-thick film, with a current density of around 17 microamperes per square centimetre. We find the driving force behind this energy generation to be a self-maintained moisture gradient that forms within the film when the film is exposed to the humidity that is naturally present in air. Connecting several devices linearly scales up the voltage and current to power electronics. Our results demonstrate the feasibility of a continuous energy-harvesting strategy that is less restricted by location or environmental conditions than other sustainable approaches.},
author = {Liu, Xiaomeng Xiaorong and Gao, Hongyan and Ward, Joy E. and Liu, Xiaomeng Xiaorong and Yin, Bing and Fu, Tianda and Chen, Jianhan and Lovley, Derek R. and Yao, Jun},
doi = {10.1038/s41586-020-2010-9},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {7796},
pages = {550--554},
publisher = {Springer US},
title = {{Power generation from ambient humidity using protein nanowires}},
url = {http://dx.doi.org/10.1038/s41586-020-2010-9 http://www.nature.com/articles/s41586-020-2010-9},
volume = {578},
year = {2020}
}

@article{Liu2019,
archivePrefix = {arXiv},
arxivId = {1910.02926},
author = {Liu, Hsueh-Ti Derek and Jacobson, Alec},
doi = {10.1145/3355089.3356495},
eprint = {1910.02926},
month = {oct},
title = {{Cubic Stylization}},
url = {https://arxiv.org/abs/1910.02926},
year = {2019}
}

@article{Littman1996,
abstract = {Sequential decision making is a fundamental task faced by any intelligent agent in an extended interaction with its environmen t; it is the act of answering the question "What should I do now?" In this thesis, I sho who w to answ er this question when "now" is one of a finite set of states, "do" is one of a finite set of actions, "should" is maximize a long-run measure of reward, and "I" is an automated planning or learning system (agent). In particular, I collect basic results concerning methods for finding optimal (or near-optimal) behavior in several different kinds of model environments: Markov decision processes, in which the agent always knows its state; partially observable Markov decision processes (POMDPs), in which the agent must piece together its state on the basis of observations it makes; and Markov games, in which the agent is in direct competition with an opponent. The thesis is written from a computer-science perspective, meaning that many mathematical details are not discussed, and descriptions of algorithms and the complexity of problems are emphasized. New results include an improved algorithm for solving POMDPs exactly over finite horizons, a method for learning minimax-optimal policies for Markov games, a pseudopolynomial bound for policy iteration, and a complete complexity theory for finding zero-reward POMDP policies.},
author = {Littman, Michael Lederman},
isbn = {9780591163506; 0591163500},
journal = {ProQuest Dissertations and Theses},
keywords = {0796:Operations research,0800:Artificial intelligence,0984:Computer science,Applied sciences,Artificial intelligence,Computer science,Markov decision process,Operations research,game theory,intelligent agents},
number = {March},
pages = {263},
title = {{Algorithms for sequential decision-making}},
url = {https://search.proquest.com/docview/304239223?accountid=188395},
year = {1996}
}

@article{Lin2010,
abstract = {This half-day tutorial introduces participants to data-intensive text processing with the MapReduce programming model 1, using the open-source Hadoop implementation. The focus will be on scalability and the tradeoffs associated with distributed processing of large datasets. Content will include general discussions about algorithm design, presentation of illustrative algorithms, case studies in HLT applications, as well as practical advice in writing Hadoop programs and running Hadoop clusters.},
author = {Lin, Jimmy and Dyer, Chris},
doi = {10.2200/S00274ED1V01Y201006HLT007},
issn = {1947-4040},
journal = {Synthesis Lectures on Human Language Technologies},
number = {1},
pages = {1--177},
title = {{Data-Intensive Text Processing with MapReduce}},
url = {http://www.morganclaypool.com/doi/abs/10.2200/S00274ED1V01Y201006HLT007},
volume = {3},
year = {2010}
}

@article{Lin2017,
abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through "cheap learning" with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine-learning, a deep neural network can be more efficient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various "no-flattening theorems" showing when efficient linear deep networks cannot be accurately approximated by shallow ones without efficiency loss, for example, we show that n variables cannot be multiplied using fewer than 2{\^{}}n neurons in a single hidden layer.},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.08225v4},
author = {Lin, Henry W and Tegmark, Max and Rolnick, David},
doi = {10.1007/s10955-017-1836-5},
eprint = {arXiv:1608.08225v4},
issn = {00224715},
journal = {Journal of Statistical Physics},
keywords = {Artificial neural networks,Deep learning,Statistical physics},
number = {6},
pages = {1223--1247},
title = {{Why Does Deep and Cheap Learning Work So Well?}},
volume = {168},
year = {2017}
}

@article{Lim2019,
abstract = {Data augmentation is an essential technique for improving generalization ability of deep learning models. Recently, AutoAugment has been proposed as an algorithm to automatically search for augmentation policies from a dataset and has significantly enhanced performances on many image recognition tasks. However, its search method requires thousands of GPU hours even for a relatively small dataset. In this paper, we propose an algorithm called Fast AutoAugment that finds effective augmentation policies via a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while achieves comparable performances on image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, SVHN, and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1905.00397},
author = {Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
eprint = {1905.00397},
month = {may},
title = {{Fast AutoAugment}},
url = {http://arxiv.org/abs/1905.00397},
year = {2019}
}

@article{Li2019,
abstract = {Various concepts related to parity-time symmetry, including anti-parity-time symmetry, have found broad applications in wave physics. Wave systems are fundamentally described by Hermitian operators, whereas their unusual properties are introduced by incorporation of gain and loss. We propose that the related physics need not be restricted to wave dynamics, and we consider systems described by diffusive dynamics. We study the heat transfer in two countermoving media and show that this system exhibits anti-parity-time symmetry. The spontaneous symmetry breaking results in a phase transition from motionless temperature profiles, despite the mechanical motion of the background, to moving temperature profiles. Our results extend the concepts of parity-time symmetry beyond wave physics and may offer opportunities to manipulate heat and mass transport.},
author = {Li, Ying and Peng, Yu-Gui and Han, Lei and Miri, Mohammad-Ali and Li, Wei and Xiao, Meng and Zhu, Xue-Feng and Zhao, Jianlin and Al{\`{u}}, Andrea and Fan, Shanhui and Qiu, Cheng-Wei},
doi = {10.1126/science.aaw6259},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
number = {6436},
pages = {170--173},
pmid = {30975886},
publisher = {American Association for the Advancement of Science},
title = {{Anti-parity-time symmetry in diffusive systems.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30975886},
volume = {364},
year = {2019}
}

@article{Li2017,
abstract = {Neural architectures are the foundation for improving performance of deep neural networks (DNNs). This paper presents deep compositional grammatical architectures which harness the best of two worlds: grammar models and DNNs. The proposed architectures integrate compositionality and reconfigurability of the former and the capability of learning rich features of the latter in a principled way. We utilize AND-OR Grammar (AOG) as network generator in this paper and call the resulting networks AOGNets. An AOGNet consists of a number of stages each of which is composed of a number of AOG building blocks. An AOG building block splits its input feature map into N groups along feature channels and then treat it as a sentence of N words. It then jointly realizes a phrase structure grammar and a dependency grammar in bottom-up parsing the "sentence" for better feature exploration and reuse. It provides a unified framework for the best practices developed in state-of-the-art DNNs. In experiments, AOGNet is tested in the CIFAR-10, CIFAR-100 and ImageNet-1K classification benchmark and the MS-COCO object detection and segmentation benchmark. In CIFAR-10, CIFAR-100 and ImageNet-1K, AOGNet obtains better performance than ResNet and most of its variants, ResNeXt and its attention based variants such as SENet, DenseNet and DualPathNet. AOGNet also obtains the best model interpretability score using network dissection. AOGNet further shows better potential in adversarial defense. In MS-COCO, AOGNet obtains better performance than the ResNet and ResNeXt backbones in Mask R-CNN.},
archivePrefix = {arXiv},
arxivId = {1711.05847},
author = {Li, Xilai and Song, Xi and Wu, Tianfu},
eprint = {1711.05847},
month = {nov},
title = {{AOGNets: Compositional Grammatical Architectures for Deep Learning}},
url = {http://arxiv.org/abs/1711.05847},
year = {2017}
}

@book{Levin2015,
author = {Levin, Oscar},
keywords = {academic literacy,examination p,metacognition,tertiary education},
pages = {292},
title = {{Discrete Mathematics: An Open Introduction}},
year = {2015}
}

@techreport{Levermore,
abstract = {We give algebraic formulas for the roots of polynomials of degree four or less. Specifically, we review the linear and quadratic formulas, give the cubic and quartic formulas, and then give derivations of those formulas. Some history around these formulas is sketched.},
author = {Levermore, C David},
title = {{Formulas for Roots of Polynomials}}
}

@article{Lei2018,
abstract = {Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, and so on. It has outperformed conventional methods in various fields and achieved great successes. Unfortunately, the understanding on how it works remains unclear. It has the central importance to lay down the theoretic foundation for deep learning. In this work, we give a geometric view to understand deep learning: we show that the fundamental principle attributing to the success is the manifold structure in data, namely natural high dimensional data concentrates close to a low-dimensional manifold, deep learning learns the manifold and the probability distribution on it. We further introduce the concepts of rectified linear complexity for deep neural network measuring its learning capability, rectified linear complexity of an embedding manifold describing the difficulty to be learned. Then we show for any deep neural network with fixed architecture, there exists a manifold that cannot be learned by the network. Finally, we propose to apply optimal mass transportation theory to control the probability distribution in the latent space.},
archivePrefix = {arXiv},
arxivId = {1805.10451},
author = {Lei, Na and Luo, Zhongxuan and Yau, Shing-Tung and Gu, David Xianfeng},
eprint = {1805.10451},
pages = {1--22},
title = {{Geometric Understanding of Deep Learning}},
url = {http://arxiv.org/abs/1805.10451},
year = {2018}
}

@article{Lehman2015,
author = {Lehman, E and Leighton, F T and Meyer, A R},
title = {{Mathematics for Computer Science}},
year = {2015}
}

@article{Le2017,
abstract = {Program synthesis from incomplete specifications (e.g. input-output examples) has gained popularity and found real-world applications, primarily due to its ease-of-use. Since this technology is often used in an interactive setting, efficiency and correctness are often the key user expectations from a system based on such technologies. Ensuring efficiency is challenging since the highly combinatorial nature of program synthesis algorithms does not fit in a 1-2 second response expectation of a user-facing system. Meeting correctness expectations is also difficult, given that the specifications provided are incomplete, and that the users of such systems are typically non-programmers. In this paper, we describe how interactivity can be leveraged to develop efficient synthesis algorithms, as well as to decrease the cognitive burden that a user endures trying to ensure that the system produces the desired program. We build a formal model of user interaction along three dimensions: incremental algorithm, step-based problem formulation, and feedback-based intent refinement. We then illustrate the effectiveness of each of these forms of interactivity with respect to synthesis performance and correctness on a set of real-world case studies.},
archivePrefix = {arXiv},
arxivId = {1703.03539},
author = {Le, Vu and Perelman, Daniel and Polozov, Oleksandr and Raza, Mohammad and Udupa, Abhishek and Gulwani, Sumit},
eprint = {1703.03539},
pages = {1--13},
title = {{Interactive Program Synthesis}},
url = {http://arxiv.org/abs/1703.03539},
year = {2017}
}

@article{LeGuennec2016,
abstract = {Time series classification has been around for decades in the data-mining and machine learning communities. In this paper, we investigate the use of convolutional neural networks (CNN) for time series classification. Such networks have been widely used in many domains like computer vision and speech recognition, but only a little for time series classification. We design a convolu-tional neural network that consists of two convolutional layers. One drawback with CNN is that they need a lot of training data to be efficient. We propose two ways to circumvent this problem: designing data-augmentation techniques and learning the network in a semi-supervised way using training time series from different datasets. These techniques are experimentally evaluated on a benchmark of time series datasets.},
author = {{Le Guennec}, Arthur and Malinowski, Simon and Tavenard, Romain},
journal = {ECML/PKDD Workshop on Advanced Analytics and Learning on Temporal Data},
keywords = {convolutional neural networks,time series},
title = {{Data Augmentation for Time Series Classification using Convolutional Neural Networks}},
url = {https://halshs.archives-ouvertes.fr/halshs-01357973/},
year = {2016}
}

@article{Lazard1992,
abstract = {It is shown that a good output for a solver of algebraic systems of dimension zero consists of a family of “triangular sets of polynomials”. Such an output is simple, readable and contains all information which may be wanted. Different algorithms are described for handling triangular systems and obtaining them from Gr{\"{o}}bner bases. These algorithms are practicable, and most of them are polynomial in the number of solutions. {\textcopyright}1991, Academic Press Limited. All rights reserved.},
author = {Lazard, D},
doi = {10.1016/S0747-7171(08)80086-7},
issn = {07477171},
journal = {Journal of Symbolic Computation},
number = {2},
pages = {117--131},
title = {{Solving zero-dimensional algebraic systems}},
volume = {13},
year = {1992}
}

@article{Lasemi2010,
abstract = {Freeform surfaces, also called sculptured surfaces, have been widely used in various engineering applications. Freeform surfaces are primarily manufactured by CNC machining, especially 5-axis CNC machining. Various methodologies and computer tools have been developed in the past to improve efficiency and quality of freeform surface machining. This paper aims at providing a state-of-the-art review on recent research development in CNC machining of freeform surfaces. This review primarily focuses on three aspects in freeform surface machining: tool path generation, tool orientation identification, and tool geometry selection. For each aspect, first concepts, requirements and fundamental research methods are briefly introduced. The major research methodologies developed in the past decade in each aspect are presented with details. Problems and future research directions are also discussed. {\textcopyright}2010 Elsevier Ltd. All rights reserved.},
author = {Lasemi, Ali and Xue, Deyi and Gu, Peihua},
doi = {10.1016/j.cad.2010.04.002},
issn = {00104485},
journal = {CAD Computer Aided Design},
keywords = {5-axis CNC machining,Freeform surface,Tool orientation,Tool parameters,Tool path},
number = {7},
pages = {641--654},
publisher = {Elsevier Ltd},
title = {{Recent development in CNC machining of freeform surfaces: A state-of-the-art review}},
url = {http://dx.doi.org/10.1016/j.cad.2010.04.002},
volume = {42},
year = {2010}
}

@book{Lanaro2013,
abstract = {Boost the performance of your Python programs using advanced techniques Overview Identify the bottlenecks in your applications and solve them using the best profiling techniques Write efficient numerical code in NumPy and Cython Adapt your programs to run on multiple processors with parallel programming In Detail Python is a programming language with a vibrant community known for its simplicity, code readability, and expressiveness. The massive selection of third party libraries make it suitable for a wide range of applications. This also allows programmers to express concepts in fewer lines of code than would be possible in similar languages. The availability of high quality numerically-focused tools has made Python an excellent choice for high performance computing. The speed of applications comes down to how well the code is written. Poorly written code means poorly performing applications, which means unsatisfied customers. This book is an example-oriented guide to the techniques used to dramatically improve the performance of your Python programs. It will teach optimization techniques by using pure python tricks, high performance libraries, and the python-C integration. The book will also include a section on how to write and run parallel code. This book will teach you how to take any program and make it run much faster. You will learn state-of the art techniques by applying them to practical examples. This book will also guide you through different profiling tools which will help you identify performance issues in your program. You will learn how to speed up your numerical code using NumPy and Cython. The book will also introduce you to parallel programming so you can take advantage of modern multi-core processors. This is the perfect guide to help you achieve the best possible performance in your Python applications. What you will learn from this book Assess the performance of your programs using benchmarks Spot the bottlenecks in your code using the Python profiling tools Speed up your code by replacing Python loops with NumPy Boost NumPy performance using the numexpr compiler Use Cython to reach performance on par with the C language Write code for multiple processors Profile, optimize, and rewrite an application from start to finish Approach An exciting, easy-to-follow guide illustrating the techniques to boost the performance of Python code, and their applications with plenty of hands-on examples. Who this book is written for If you are a programmer who likes the power and simplicity of Python and would like to use this language for performance-critical applications, this book is ideal for you. All that is required is a basic knowledge of the Python programming language. The book will cover basic and advanced topics so will be great for you whether you are a new or a seasoned Python developer. ** About the Author Gabriele Lanaro Gabriele Lanaro is a PhD student in Chemistry at the University of British Columbia, in the field of Molecular Simulation. He writes high performance Python code to analyze chemical systems in large-scale simulations. He is the creator of Chemlab—a high performance visualization software in Python—and emacs-for-python—a collection of emacs extensions that facilitate working with Python code in the emacs text editor. This book builds on his experience in writing scientific Python code for his research and personal projects.},
author = {Lanaro, Gabriele},
isbn = {9781783288458},
pages = {108},
title = {{Python High Performance Programming}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=4GRpAgAAQBAJ{\&}oi=fnd{\&}pg=PT7{\&}dq=Python+High+Performance+Programming{\&}ots=XTm0FxD6oV{\&}sig=bWGtDsU97cvSG-POrBoWuDIwopw},
year = {2013}
}

@book{Lamport2002a,
author = {Lamport, Leslie},
booktitle = {Computer},
isbn = {9780321143068},
pages = {364},
publisher = {Addison-Wesley},
title = {{Specifying Systems: The TLA+ Language and Tools for Hardware and Software Engineers}},
year = {2002}
}

@article{Lamport2009,
abstract = {Summary form only given. Algorithms are different from programs and should not be described with programming languages. For example, algorithms are usually best described in terms of mathematical objects like sets and graphs instead of the primitive objects like bytes and integers provided by programming languages. +CAL is an algorithm language based on TLA+. A +CAL algorithm is translated to a TLA+ specification that can then be checked with the TLC model checker},
author = {Lamport, Leslie},
doi = {10.1007/978-3-642-03466-4_2},
isbn = {3642034659},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {October},
pages = {36--60},
title = {{The PlusCal algorithm language}},
volume = {5684 LNCS},
year = {2009}
}

@book{Lamport2002,
author = {Lamport, Leslie},
isbn = {032114306X},
title = {{Specifying Sistems: The TLA+ Language and Tools for Hardware and Software Engineers}},
year = {2002}
}

@article{Lamport2017,
author = {Lamport, Leslie},
title = {{A PlusCal User's Manual}},
url = {http://lamport.azurewebsites.net/tla/p-manual.pdf},
year = {2016}
}

@article{Lamport2012,
abstract = {A method of writing proofs is described that makes it harder to prove things that are not true. The method, based on hierarchical structuring, is simple and practical. The author's twenty years of experience writing such proofs is discussed.},
author = {Lamport, Leslie},
doi = {10.1007/s11784-012-0071-6},
issn = {16617738},
journal = {Journal of Fixed Point Theory and Applications},
keywords = {Structured proofs,teaching proofs},
number = {1},
pages = {43--63},
title = {{How to Write a 21st Century Proof}},
volume = {11},
year = {2012}
}

@article{Lample2019,
abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
archivePrefix = {arXiv},
arxivId = {1912.01412},
author = {Lample, Guillaume and Charton, Fran{\c{c}}ois},
eprint = {1912.01412},
title = {{Deep Learning for Symbolic Mathematics}},
url = {http://arxiv.org/abs/1912.01412},
year = {2019}
}

@article{Lai1994,
abstract = {Edge-Valued Binary-Decision Diagrams (EVBDD's) are directed{\$}\backslash{\$}nacyclic graphs that can represent and manipulate integer functions as{\$}\backslash{\$}neffectively as Ordered Binary-Decision Diagrams OBDD's) do for Boolean{\$}\backslash{\$}nfunctions. They have been used in logic verification for showing the{\$}\backslash{\$}nequivalence between Boolean functions and arithmetic functions. In this{\$}\backslash{\$}npaper, we present EVBDD-based algorithms for solving integer linear{\$}\backslash{\$}nprograms, computing spectral coefficients of Boolean functions, and{\$}\backslash{\$}nperforming function decomposition. These algorithms have been{\$}\backslash{\$}nimplemented in C under the SIS environment and experimental results are{\$}\backslash{\$}nprovided},
author = {Lai, Yung Te and Pedram, Massoud and Vrudhula, Sarma B K},
doi = {10.1109/43.298033},
issn = {02780070},
journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
number = {8},
pages = {959--975},
title = {{EVBDD-Based Algorithms for Integer Linear Programming, Spectral Transformation, and Function Decomposition}},
volume = {13},
year = {1994}
}

@article{Lacroix2018,
abstract = {The problem of Knowledge Base Completion can be framed as a 3rd-order binary tensor completion problem. In this light, the Canonical Tensor Decomposition (CP) (Hitchcock, 1927) seems like a natural solution; however, current implementations of CP on standard Knowledge Base Completion benchmarks are lagging behind their competitors. In this work, we attempt to understand the limits of CP for knowledge base completion. First, we motivate and test a novel regularizer, based on tensor nuclear {\$}p{\$}-norms. Then, we present a reformulation of the problem that makes it invariant to arbitrary choices in the inclusion of predicates or their reciprocals in the dataset. These two methods combined allow us to beat the current state of the art on several datasets with a CP decomposition, and obtain even better results using the more advanced ComplEx model.},
archivePrefix = {arXiv},
arxivId = {1806.07297},
author = {Lacroix, Timoth{\'{e}}e and Usunier, Nicolas and Obozinski, Guillaume},
eprint = {1806.07297},
month = {jun},
title = {{Canonical Tensor Decomposition for Knowledge Base Completion}},
url = {http://arxiv.org/abs/1806.07297},
year = {2018}
}

@article{KUHLMAN2013,
abstract = {This document is a self­learning document for a course in Python programming. This course contains (1) a part for beginners, (2) a discussion of several advanced topics that are of interest to Python programmers, and (3) a Python workbook with lots of exercises.},
author = {KUHLMAN, Dave},
journal = {A Python Book},
keywords = {advanced python,and,beginning python,ython book},
pages = {1--227},
title = {{A Python Book}},
year = {2013}
}

@article{Krings2017,
abstract = {We present a CLP(FD)-based constraint solver able to deal with unbounded domains. It is based on constraint propagation, resorting to enumeration if all other methods fail. An important aspect is detecting when enumeration was complete and if this has an impact on the soundness of the result. We present a technique which guarantees soundness in the following way: if the constraint solver finds a solution it is guaranteed to be correct; if the constraint solver fails to find a solution it can either return the result "definitely false" in case it knows enumeration was exhaustive, or "unknown" in case it was aborted. The technique can deal with nested universal and existential quantifiers. It can easily be extended to set comprehensions and other operators introducing new quantified variables. We show applications in data validation and proof.},
author = {Krings, Sebastian and Leuschel, Michael},
doi = {10.4204/EPTCS.234.6},
issn = {2075-2180},
journal = {Electronic Proceedings in Theoretical Computer Science},
pages = {73--87},
title = {{Constraint Logic Programming over Infinite Domains with an Application to Proof}},
url = {http://arxiv.org/abs/1701.00629},
volume = {234},
year = {2017}
}

@article{Kriesel2007,
abstract = {Originally, this work has been prepared in the framework of a seminar of the University of Bonn in Germany, but it has been and will be extended (after being presented and published online under www.dkriesel.com on 5/27/2005). First and foremost, to provide a comprehensive overview of the subject of neural networks and, second, just to acquire more and more knowledge about LATEX. And who knows – maybe one day this summary will become a real preface!},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.3159v1},
author = {Kriesel, David},
doi = {10.1016/S0140-6736(95)92880-4},
eprint = {arXiv:1411.3159v1},
isbn = {9780849371943},
issn = {01406736},
journal = {Springer-Verlag, Berlin},
pages = {Second edition},
pmid = {7823875},
title = {{A brief Introduction on Neural Networks}},
year = {2007}
}

@incollection{Koopman2000,
abstract = {This paper forms the substance of a course of lectures given at the International Summer School in Computer Programming at Copenhagen in August, 1967. The lectures were originally given from notes and the paper was written after the course was finished. In spite of this, and only partly because of the shortage of time, the paper still retains many of the shortcomings of a lecture course. The chief of these are an uncertainty of aim—it is never quite clear what sort of audience there will be for such lectures—and an associated {\ldots}},
author = {Koopman, Pieter and Plasmeijer, Rinus},
booktitle = {Fundamental concepts in programming languages},
chapter = {3},
doi = {10.2168/LMCS-},
keywords = {{\$}\lambda{\$}-terms,and phrases,automatic programming,program synthesis,programming by},
pages = {11--49},
title = {{Systematic Synthesis of Functions}},
year = {2000}
}

@article{Kohler2018,
abstract = {Normalization helps us find a database schema at design time that can process the most frequent updates efficiently at run time. Unfortunately, relational normalization only works for idealized database instances in which duplicates and null markers are not present. On one hand, these features occur frequently in real-world data compliant with the industry standard SQL, and especially in modern application domains. On the other hand, the features impose challenges that make it difficult to extend the existing forty year old normalization framework to SQL, and any current extensions are fairly limited. We introduce a new class of functional dependencies and show that they provide the right notion for SQL schema design. Axiomatic and linear-time algorithmic characterizations of the associated implication problem are established. These foundations enable us to propose a Boyce–Codd normal form for SQL. We justify the normal form by showing that it permits precisely those SQL instances which are free from data redundancy. Unlike the relational case, there are SQL schemata that cannot be converted into Boyce–Codd normal form. Nevertheless, for an expressive sub-class of our functional dependencies we establish a normalization algorithm that always produces a schema in Value-Redundancy free normal form. This normal form permits precisely those instances which are free from any redundant data value occurrences other than the null marker. Experiments show that our functional dependencies occur frequently in real-world data and that they are effective in eliminating redundant values from these data sets without loss of information.},
author = {K{\"{o}}hler, Henning and Link, Sebastian},
doi = {10.1016/j.is.2018.04.001},
isbn = {9781450335317},
issn = {03064379},
journal = {Information Systems},
keywords = {Armstrong database,Axioms,Boyce–Codd normal form,Data redundancy,Database schema design,Functional dependency,Normalization,Reasoning,Update anomaly},
pages = {88--113},
title = {{SQL Schema Design: Foundations, Normal Forms, and Normalization}},
volume = {76},
year = {2018}
}

@inproceedings{Kodratoff1990,
abstract = {Among the several misunderstandings about Program Synthesis (PS), we particularly examine the one relative to Logic Programming alleged to have solved this problem. We exem- pli€{\~{}} how it is indeed quite possible to write down specifications in PROLOG. Nevertheless, well-known theoretical reasons limit this possibility, and we provide a detailed analysis of the practi- cal reasons why a formal specification may be hard to program in PROLOG. All that contributes to the clarification of the exact role of PS in AI and in Software Engineering, and its possible application to software certification.},
author = {Kodratoff, Yves and Franova, Marta and Partridge, Derek},
booktitle = {Systems Integration '90. Proceedings of the First International Conference on Systems Integration},
doi = {10.1109/ICSI.1990.138700},
isbn = {0-8186-9027-5},
keywords = {certification cycle,in- ductive theorem proving,program synthesis from formal specifications},
pages = {346--355},
publisher = {IEEE Comput. Soc. Press},
title = {{Logic Programming and Program Synthesis}},
url = {http://ieeexplore.ieee.org/document/138700/},
year = {1990}
}

@article{Kodratoff,
author = {Kodratoff, Y and Franova, M and Partridge, D},
title = {{Logic programming and program synthesis}}
}

@article{Knuth1981,
abstract = {This paper discusses a new approach to the problem of dividing the text of a paragraph into lines of approximately equal length. Instead of simply making decisions one line at a time, the method considers the paragraph as a whole, so that the final appearance of a given line might be influenced by the text on succeeding lines. A system based on three simple primitive concepts called ‘boxes', ‘glue', and ‘penalties' provides the ability to deal satisfactorily with a wide variety of typesetting problems in a unified framework, using a single algorithm that determines optimum breakpoints. The algorithm avoids backtracking by a judicious use of the techniques of dynamic programming. Extensive computational experience confirms that the approach is both efficient and effective in producing high-quality output. The paper concludes with a brief history of line-breaking methods, and an appendix presents a simplified algorithm that requires comparatively few resources.},
author = {Knuth, Donald E and Plass, Michael F},
doi = {10.1002/spe.4380111102},
issn = {1097024X},
journal = {Software: Practice and Experience},
keywords = {Box/glue/penalty algebra,Composition,Dynamic programming,History of printing,Justification,Layout,Line breaking,Shortest paths,Spacing,TEX (Tau Epsilon Chi),Typesetting,Word processing},
number = {11},
pages = {1119--1184},
title = {{Breaking Paragraphs into Lines}},
volume = {11},
year = {1981}
}

@book{Kim2015,
abstract = {Just as a professional athlete doesn't show up without a solid game plan, ethical hackers, IT professionals, and security researchers should not be unprepared, either. The Hacker Playbook provides them their own game plans. Written by a longtime security professional and CEO of Secure Planet, LLC, this step-by-step guide to the “game” of penetration hacking features hands-on examples and helpful advice from the top of the field. Through a series of football-style “plays,” this straightforward guide gets to the root of many of the roadblocks people may face while penetration testing—including attacking different types of networks, pivoting through security controls, privilege escalation, and evading antivirus software. From “Pregame” research to “The Drive” and “The Lateral Pass,” the practical plays listed can be read in order or referenced as needed. Either way, the valuable advice within will put you in the mindset of a penetration tester of a Fortune 500 company, regardless of your career or level of experience. This second version of The Hacker Playbook takes all the best "plays" from the original book and incorporates the latest attacks, tools, and lessons learned. Double the content compared to its predecessor, this guide further outlines building a lab, walks through test cases for attacks, and provides more customized code. Whether you're downing energy drinks while desperately looking for an exploit, or preparing for an exciting new job in IT security, this guide is an essential part of any ethical hacker's library—so there's no reason not to get in the game.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kim, Peter},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9781512214567},
issn = {16130073},
pages = {358},
pmid = {25246403},
title = {{The Hacker Playbook 2: Practical Guide To Penetration Testing}},
year = {2015}
}

@techreport{Kim,
archivePrefix = {arXiv},
arxivId = {1911.00892v1},
author = {Kim, Joon-Hwi and Oh, Maverick S H and Kim, Keun-Young},
eprint = {1911.00892v1},
title = {{Boosting Vector Calculus with the Graphical Notation}}
}

@misc{Khoury2013,
author = {Khoury, Marc},
title = {{Let's Draw a Graph : An Introduction with Graphviz}},
year = {2013}
}

@book{Kernighan1988,
abstract = {Introduces the features of the C programming language, discusses data types, variables, operators, control flow, functions, pointers, arrays, and structures, and looks at the UNIX system interface.},
author = {Kernighan, Brian W and Ritchie, Dennis M},
edition = {2},
isbn = {0131103709},
keywords = {alloc argc argument argv arithmetic array assignme},
mendeley-tags = {alloc argc argument argv arithmetic array assignme},
publisher = {Prentice Hall},
title = {{C Programming Language}},
year = {1988}
}

@book{Kernighan,
abstract = {The computing world has undergone a revolution since the publication ofThe C Programming Language in 1978. Big computers are much bigger, and personal computers have capabilities that rival mainframes of a decade ago. During this time, C has changed too, although only modestly, and it has spread far beyond its origins as the language of the UNIX operating system.},
author = {Kernighan, Brian W and Ritchie, Dennis M},
edition = {Second},
editor = {{Prentice Hall Software Series}},
pages = {217},
title = {{The ANSI C Programming Language}}
}

@book{KeithO.GeddesStephenR.Czapor1992,
abstract = {Algorithms for Computer Algebra is the first comprehensive textbook to be published on the topic of computational symbolic mathematics. The book first develops the foundational material from modern algebra that is required for subsequent topics. It then presents a thorough development of modern computational algorithms for such problems as multivariate polynomial arithmetic and greatest common divisor calculations, factorization of multivariate polynomials, symbolic solution of linear and polynomial systems of equations, and analytic integration of elementary functions. Numerous examples are integrated into the text as an aid to understanding the mathematical development. The algorithms developed for each topic are presented in a Pascal-like computer language. An extensive set of exercises is presented at the end of each chapter. Algorithms for Computer Algebra is suitable for use as a textbook for a course on algebraic algorithms at the third-year, fourth-year, or graduate level. Although the mathematical development uses concepts from modern algebra, the book is self-contained in the sense that a one-term undergraduate course introducing students to rings and fields is the only prerequisite assumed. The book also serves well as a supplementary textbook for a traditional modern algebra course, by presenting concrete applications to motivate the understanding of the theory of rings and fields.},
author = {{Keith O. Geddes, Stephen R. Czapor}, George Labahn},
isbn = {0-7923-9259-0},
keywords = {algebraic extension algebraic number Algorithm 6.1},
mendeley-tags = {algebraic extension algebraic number Algorithm 6.1},
publisher = {Kluwer Academic Publishers},
title = {{Algorithms for Computer Algebra}},
year = {1992}
}

@book{Kay2011,
author = {Kay, David},
isbn = {007034846},
title = {{Tensor Calculus}},
year = {2011}
}

@article{Kasim2020,
abstract = {Computer simulations are invaluable tools for scientific discovery. However, accurate simulations are often slow to execute, which limits their applicability to extensive parameter exploration, large-scale data analysis, and uncertainty quantification. A promising route to accelerate simulations by building fast emulators with machine learning requires large training datasets, which can be prohibitively expensive to obtain with slow simulations. Here we present a method based on neural architecture search to build accurate emulators even with a limited number of training data. The method successfully accelerates simulations by up to 2 billion times in 10 scientific cases including astrophysics, climate science, biogeochemistry, high energy density physics, fusion energy, and seismology, using the same super-architecture, algorithm, and hyperparameters. Our approach also inherently provides emulator uncertainty estimation, adding further confidence in their use. We anticipate this work will accelerate research involving expensive simulations, allow more extensive parameters exploration, and enable new, previously unfeasible computational discovery.},
archivePrefix = {arXiv},
arxivId = {2001.08055},
author = {Kasim, M F and Watson-Parris, D and Deaconu, L and Oliver, S and Hatfield, P and Froula, D H and Gregori, G and Jarvis, M and Khatiwala, S and Korenaga, J and Topp-Mugglestone, J and Viezzer, E and Vinko, S M},
eprint = {2001.08055},
title = {{Up to two billion times acceleration of scientific simulations with deep neural architecture search}},
url = {http://arxiv.org/abs/2001.08055},
year = {2020}
}

@inproceedings{Kanter2015,
author = {Kanter, James Max and Veeramachaneni, Kalyan},
booktitle = {2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
doi = {10.1109/DSAA.2015.7344858},
isbn = {978-1-4673-8272-4},
month = {oct},
pages = {1--10},
publisher = {IEEE},
title = {{Deep feature synthesis: Towards automating data science endeavors}},
url = {http://ieeexplore.ieee.org/document/7344858/},
year = {2015}
}

@book{Kamal2010,
address = {Berlin, Heidelberg},
author = {Kamal, Ahmad A},
doi = {10.1007/978-3-642-04333-8},
isbn = {978-3-642-04332-1},
publisher = {Springer Berlin Heidelberg},
title = {{1000 Solved Problems in Modern Physics}},
url = {http://link.springer.com/10.1007/978-3-642-04333-8},
year = {2010}
}

@article{Kaliszyk2018,
abstract = {We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40{\%} more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.},
archivePrefix = {arXiv},
arxivId = {1805.07563},
author = {Kaliszyk, Cezary and Michalewski, Henryk and Urban, Josef and Ol{\v{s}}{\'{a}}k, Mirek},
eprint = {1805.07563},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
title = {{Reinforcement learning of theorem proving}},
year = {2018}
}

@book{Juneau2018,
author = {Juneau, Josh},
booktitle = {Java EE 8 Recipes},
doi = {10.1007/978-1-4842-3594-2},
isbn = {9781484235935},
title = {{Java EE 8 Recipes}},
year = {2018}
}

@book{Judson2016,
author = {Judson, Thomas W and Austin, Stephen F},
pages = {471},
title = {{Abstract Algebra: Theory and Applications}},
year = {2016}
}

@article{Jordan2017,
abstract = {One version of the energy-time uncertainty principle states that the minimum time T{\_} for a quantum system to evolve from a given state to any orthogonal state is h/(4 E) where E is the energy uncertainty. A related bound called the Margolus-Levitin theorem states that T{\_} h/(2 E) where E is the expectation value of energy and the ground energy is taken to be zero. Many subsequent works have interpreted T{\_} as defining a minimal time for an elementary computational operation and correspondingly a fundamental limit on clock speed determined by a system's energy. Here we present local time-independent Hamiltonians in which computational clock speed becomes arbitrarily large relative to E and E{\$} as the number of computational steps goes to infinity. We argue that energy considerations alone are not sufficient to obtain an upper bound on computational speed, and that additional physical assumptions such as limits to information density and information transmission speed are necessary to obtain such a bound.},
archivePrefix = {arXiv},
arxivId = {arXiv:1701.01175v2},
author = {Jordan, Stephen P},
doi = {10.1103/PhysRevA.95.032305},
eprint = {arXiv:1701.01175v2},
issn = {24699934},
journal = {Physical Review A},
number = {3},
title = {{Fast quantum computation at arbitrarily low energy}},
volume = {95},
year = {2017}
}

@techreport{Johnson,
abstract = {We present CHORUS, a system with a novel architecture for providing differential privacy for statistical SQL queries. The key to our approach is to embed a differential privacy mechanism into the query before execution so the query automatically enforces differential privacy on its output. CHORUS is compatible with any SQL database that supports standard math functions, requires no user modifications to the database or queries, and simultaneously supports many differential privacy mechanisms. To the best of our knowledge, no existing system provides these capabilities. We demonstrate our approach using four general-purpose differential privacy mechanisms. In the first evaluation of its kind, we use CHORUS to evaluate these four mechanisms on real-world queries and data. The results demonstrate that our approach supports 93.9{\%} of statistical queries in our corpus, integrates with a production DBMS without any modifications, and scales to hundreds of millions of records. CHORUS is currently being deployed at Uber for its internal analytics tasks. CHORUS represents a significant part of the company's GDPR compliance efforts, and can provide both differential privacy and access control enforcement. In this capacity, CHORUS processes more than 10,000 queries per day.},
archivePrefix = {arXiv},
arxivId = {1809.07750v2},
author = {Johnson, Noah and Near, Joseph P and Hellerstein, Joseph M and Song, Dawn},
eprint = {1809.07750v2},
title = {{Chorus: Differential Privacy via Query Rewriting}}
}

@article{John2015,
abstract = {Given a propositional formula F(x,y), a Skolem function for x is a function {\$}\backslash{\$}Psi(y), such that substituting {\$}\backslash{\$}Psi(y) for x in F gives a formula semantically equivalent to {\$}\backslash{\$}exists F. Automatically generating Skolem functions is of significant interest in several applications including certified QBF solving, finding strategies of players in games, synthesising circuits and bit-vector programs from specifications, disjunctive decomposition of sequential circuits etc. In many such applications, F is given as a conjunction of factors, each of which depends on a small subset of variables. Existing algorithms for Skolem function generation ignore any such factored form and treat F as a monolithic function. This presents scalability hurdles in medium to large problem instances. In this paper, we argue that exploiting the factored form of F can give significant performance improvements in practice when computing Skolem functions. We present a new CEGAR style algorithm for generating Skolem functions from factored propositional formulas. In contrast to earlier work, our algorithm neither requires a proof of QBF satisfiability nor uses composition of monolithic conjunctions of factors. We show experimentally that our algorithm generates smaller Skolem functions and outperforms state-of-the-art approaches on several large benchmarks.},
archivePrefix = {arXiv},
arxivId = {1508.05497},
author = {John, Ajith K and Shah, Shetal and Chakraborty, Supratik and Trivedi, Ashutosh and Akshay, S},
eprint = {1508.05497},
title = {{Skolem Functions for Factored Formulas}},
url = {http://arxiv.org/abs/1508.05497},
year = {2015}
}

@article{Jin2018,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {1806.10282},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
eprint = {1806.10282},
month = {jun},
title = {{Auto-Keras: An Efficient Neural Architecture Search System}},
url = {http://arxiv.org/abs/1806.10282},
year = {2018}
}

@article{Jiji2006,
abstract = {We propose a learning-based, single-image super-resolution reconstruction technique using the contourlet transform, which is capable of capturing the smoothness along contoursmaking use of directional decompositions. The contourlet coefficients at finer ...},
author = {Jiji, C V and Chaudhuri, Subhasis},
doi = {10.1155/ASP/2006/73767},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in Signal Processing},
number = {1},
pages = {73767},
title = {{Single-Frame Image Super-resolution through Contourlet Learning}},
url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/73767},
volume = {2006},
year = {2006}
}

@article{Jelonek2012,
abstract = {Let k be a field and f : kn → kn be a polynomial isomorphism. We give aformula for f-1. In particular we show how to solve the equation f = 0.},
author = {Jelonek, Zbigniew},
doi = {10.1515/dema-2013-0419},
issn = {23914661},
journal = {Demonstratio Mathematica},
keywords = {Polynomial equations,Polynomial isomorphis,Polynomial mappings},
number = {4},
pages = {797--805},
title = {{Solving polynomial equations}},
volume = {45},
year = {2012}
}

@article{Jakubczyk2012,
abstract = {The paper is concerned with the problem of approximating smooth curves in the plane by circular arc splines. The algorithm based on fitting a biarc to a segment of a given function or parametric curve of monotonic cur-vature is presented. The biarc formed of two tangentially joined circular arcs is constructed by assuming that it matches two end points and two tangents of the segment. However, such an interpolation problem imposes four conditions whereas the biarc has five degrees of freedom, and there exists a one parameter family of interpolating biarcs, which is searched iteratively by bisection method for finding a suitable one. If a given ac-curacy has not been achieved the segment is subdivided and this process is continued on smaller pieces. Finally, the fitting circular arc spline is obtained as the result of sequential applying the local biarc interpolation procedure.},
author = {Jakubczyk, Kazimierz},
journal = {Kaj.Pr.Radom.Pl},
keywords = {approx-,arc spline,biarc,circular arc spline,circular spline,imation by circular splines,interpolation by biarcs},
pages = {1--13},
title = {{Approximation of Smooth Planar Curves by Circular Arc Splines}},
url = {http://www.kaj.pr.radom.pl/prace/Biarcs.pdf},
volume = {2010},
year = {2012}
}

@inproceedings{Iyer2016a,
abstract = {High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely data-driven approach for generating high level summaries of source code. Our model, CODE-NN, uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C{\#} code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C{\#} benchmark by a large margin.},
address = {Stroudsburg, PA, USA},
author = {Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
doi = {10.18653/v1/P16-1195},
isbn = {9781510827585},
pages = {2073--2083},
publisher = {Association for Computational Linguistics},
title = {{Summarizing Source Code using a Neural Attention Model}},
url = {http://aclweb.org/anthology/P16-1195},
volume = {4},
year = {2016}
}

@article{Iworiso2019,
abstract = {This paper applies a plethora of machine learning techniques to forecast the direction of the US equity premium. Our techniques include benchmark binary probit models, classification and regression trees, along with penalized binary probit models. Our empirical analysis reveals that the sophisticated machine learning techniques significantly outperformed the benchmark binary probit forecasting models, both statistically and economically. Overall, the discriminant analysis classifiers are ranked first among all the models tested. Specifically, the high-dimensional discriminant analysis classifier ranks first in terms of statistical performance, while the quadratic discriminant analysis classifier ranks first in economic performance. The penalized likelihood binary probit models (least absolute shrinkage and selection operator, ridge, elastic net) also outperformed the benchmark binary probit models, providing significant alternatives to portfolio managers.},
author = {Iworiso, Jonathan and Vrontos, Spyridon},
doi = {10.1002/for.2632},
issn = {1099131X},
journal = {Journal of Forecasting},
keywords = {CART,binary probit,directional predictability,forecasting,penalized binary probit,recursive window},
title = {{On the directional predictability of equity premium using machine learning techniques}},
year = {2019}
}

@article{IvorraCastillo2011,
abstract = {Tradicionalmente se ha dicho que la l{\'{o}}gica se ocupa del estudio del razonamiento. Esto hoy en d{\'{i}}a puede considerarse desbordado por la enorme extensi{\'{o}}n y diversidad que ha alcanzado esta disciplina, pero puede servirnos como primera aproximaci{\'{o}}n a su contenido. Un matem{\'{a}}tico competente distingue sin dificultad una demostraci{\'{o}}n correcta de una incorrecta, o mejor dicho, una demostraci{\'{o}}n de otra cosa que aparenta serlo pero que no lo es. Sin embargo, no le pregunt{\'{e}}is qu{\'{e}} es lo que entiende por demostraci{\'{o}}n, pues —a menos que adem{\'{a}}s sepa l{\'{o}}gica— no os sabr{\'{a}} responder, ni falta que le hace. El matem{\'{a}}tico se las arregla para reconocer la validez de un argumento o sus defectos posibles de una forma improvisada pero, al menos en principio, de total fiabilidad. No necesita para su tarea contar con un concepto preciso de demostraci{\'{o}}n. Eso es en cambio lo que ocupa al l{\'{o}}gico: El matem{\'{a}}tico demuestra, el l{\'{o}}gico estudia lo que hace el matem{\'{a}}tico cuando demuestra.},
author = {{Ivorra Castillo}, Carlos},
pages = {446},
title = {{L{\'{o}}gica Y Teor{\'{i}}a de Conjuntos}},
url = {https://www.uv.es/ivorra/Libros/Logica.pdf},
year = {2011}
}

@article{Isaza2014,
author = {Isaza, Juan Pedro Villa},
journal = {Unversidad EAFIT},
pages = {131},
title = {{Category Theory Applied to Functional Programming}},
year = {2014}
}

@article{InternationalSoftwareTestingQualificationsBoard2011,
abstract = {certification; foundation level; agile tester; istqb; syllabus},
author = {{International Software Testing Qualifications Board}},
pages = {85},
title = {{Certified Tester Foundation Level Syllabus}},
year = {2011}
}

@article{Innes2019,
abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe Zygote, a Differentiable Programming system that is able to take gradients of general program structures. We implement this system in the Julia programming language. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning, but more importantly, it enables us to incorporate a large ecosystem of libraries in our models in a straightforward way. We discuss our approach to automatic differentiation, including its support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present several examples of differentiating programs.},
archivePrefix = {arXiv},
arxivId = {1907.07587},
author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B and Tebbutt, Will},
eprint = {1907.07587},
month = {jul},
title = {{A Differentiable Programming System to Bridge Machine Learning and Scientific Computing}},
url = {http://arxiv.org/abs/1907.07587},
year = {2019}
}

@misc{Indyk,
author = {Indyk, Piotr},
title = {{Tutorial on Compressed Sensing (or Compressive Sampling, or Linear Sketching)}}
}

@article{Humblot2006,
abstract = {This paper presents a new method for super-resolution ( SR) reconstruction of a high-resolution (HR) image from several low-resolution (LR) images. The HR image is assumed to be composed of homogeneous regions. Thus, the a priori distribution of the pixels is modeled by a finite mixture model (FMM) and a Potts Markov model (PMM) for the labels. The whole a priori model is then a hierarchical Markov model. The LR images are assumed to be obtained from the HR image by lowpass filtering, arbitrarily translation, decimation, and finally corruption by a random noise. The problem is then put in a Bayesian detection and estimation framework, and appropriate algorithms are developed based on Markov chain Monte Carlo (MCMC) Gibbs sampling. At the end, we have not only an estimate of the HR image but also an estimate of the classification labels which leads to a segmentation result.},
author = {Humblot, Fabrice and Mohammad-Djafari, Ali},
doi = {10.1155/ASP/2006/36971},
issn = {11108657},
journal = {Eurasip Journal on Applied Signal Processing},
pages = {1--16},
title = {{Super-Resolution Using Hidden Markov Model and Bayesian Detection Estimation Framework}},
volume = {2006},
year = {2006}
}

@article{Hughes1989,
abstract = {As software becomes more and more complex, it is more and more important to structure it well. Well-structured software is easy to write, easy to debug, and provides a collection of modules that can be re-used to reduce future programming costs. Conventional languages place con- ceptual limits on the way problems can be modularised. Functional lan- guages push those limits back. In this paper we show that two features of functional languages in particular, higher-order functions and lazy eval- uation, can contribute greatly to modularity. As examples, we manipu- late lists and trees, program several numerical algorithms, and implement the alpha-beta heuristic (an algorithm from Artificial Intelligence used in game-playing programs). Since modularity is the key to successful pro- gramming, functional languages are vitally important to the real world.},
author = {Hughes, John},
doi = {10.1093/comjnl/32.2.98},
isbn = {0201172364},
issn = {0010-4620},
journal = {Research Topics in Functional Programming},
number = {April 1989},
pages = {1--23},
title = {{Why functional programming matters}},
url = {http://comjnl.oxfordjournals.org/content/32/2/98.short},
volume = {32},
year = {1989}
}

@book{Hubbard2003,
author = {Hubbard, John H and Hubbard, Barbara Burke},
doi = {10.2307/3647874},
issn = {00029890},
pages = {698},
title = {{Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach}},
url = {http://www.jstor.org/stable/3647874?origin=crossref},
year = {2003}
}

@misc{Hu2015,
abstract = {In 1989 when functional programming was still considered a niche topic, Hughes wrote a visionary paper arguing convincingly " why functional programming matters " . More than two decades have pas-sed. Has functional programming really mattered? Our answer is a resounding " Yes! " . Functional programming is now at the forefront of a new generation of programming technologies, and enjoying incre-asing popularity and influence. In this paper, we review the impact of functional programming, focusing on how it has changed the way we may construct programs, the way we may verify programs, and fundamentally the way we may think about programs.},
author = {Hu, Zhenjiang and Hughes, John and Wang, Meng},
booktitle = {National Science Review},
doi = {10.1093/nsr/nwv042},
issn = {2053714X},
number = {3},
title = {{How functional programming mattered}},
volume = {2},
year = {2015}
}

@article{Hu2019,
abstract = {We present DiffTaichi, a new differentiable programming language tailored for building high-performance differentiable physical simulators. Based on an imperative programming language, DiffTaichi generates gradients of simulation steps using source code transformations that preserve arithmetic intensity and parallelism. A light-weight tape is used to record the whole simulation program structure and replay the gradient kernels in a reversed order, for end-to-end backpropagation. We demonstrate the performance and productivity of our language in gradient-based learning and optimization tasks on 10 different physical simulators. For example, a differentiable elastic object simulator written in our language is 4.2x shorter than the hand-engineered CUDA version yet runs as fast, and is 188x faster than the TensorFlow implementation. Using our differentiable programs, neural network controllers are typically optimized within only tens of iterations.},
archivePrefix = {arXiv},
arxivId = {1910.00935},
author = {Hu, Yuanming and Anderson, Luke and Li, Tzu-Mao and Sun, Qi and Carr, Nathan and Ragan-Kelley, Jonathan and Durand, Fr{\'{e}}do},
eprint = {1910.00935},
pages = {1--20},
title = {{DiffTaichi: Differentiable Programming for Physical Simulation}},
url = {http://arxiv.org/abs/1910.00935},
year = {2019}
}

@article{Howard2020,
abstract = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4–5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching.},
archivePrefix = {arXiv},
arxivId = {2002.04688},
author = {Howard, Jeremy and Gugger, Sylvain},
doi = {10.3390/info11020108},
eprint = {2002.04688},
issn = {2078-2489},
journal = {Information},
month = {feb},
title = {{Fastai: A Layered API for Deep Learning}},
url = {http://arxiv.org/abs/2002.04688 http://dx.doi.org/10.3390/info11020108 https://www.mdpi.com/2078-2489/11/2/108},
year = {2020}
}

@book{Houtven2013,
abstract = {This book is intended as an introduction to cryptography for pro- grammers of any skill level. It's a continuation of a talk of the same name, which was given by the author at PyCon 2013. The structure of this book is very similar: it starts with very simple primitives, and gradually introduces new ones, demonstrating why they're necessary. Eventually, all of this is put together into complete, practical cryptosystems, such as TLS, GPG and OTR.},
author = {Houtven, Laurens Van},
pages = {254},
title = {{Crypto 101}},
year = {2013}
}

@article{Horstmann2008,
abstract = {This refcard gives you an overview of key aspects of the Java language and cheat sheets on the core library (formatted output, collections, regular expressions, logging, properties) as well as the most commonly used tools (javac, java, jar).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Horstmann, Cay},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {7976931348623},
issn = {1098-6596},
pages = {6},
pmid = {25246403},
title = {{Core Java Cheatsheet}},
url = {www.dzone.com},
year = {2008}
}

@article{Horsch2013,
abstract = {We present an anytime algorithm which computes policies for decision problems represented as multi-stage influence diagrams. Our algorithm constructs policies incrementally, starting from a policy which makes no use of the available information. The incremental process constructs policies which includes more of the information available to the decision maker at each step. While the process converges to the optimal policy, our approach is designed for situations in which computing the optimal policy is infeasible. We provide examples of the process on several large decision problems, showing that, for these examples, the process constructs valuable (but sub-optimal) policies before the optimal policy would be available by traditional methods.},
archivePrefix = {arXiv},
arxivId = {1301.7384},
author = {Horsch, M C and Poole, D},
eprint = {1301.7384},
journal = {Proceedings of the Fourteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI–98)},
title = {{An anytime algorithm for decision making under uncertainty}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1880{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.21.1880{\&}rep=rep1{\&}type=pdf},
year = {2013}
}

@incollection{Hoare1996,
author = {Hoare, C A R},
doi = {10.1007/3-540-60973-3_77},
pages = {1--17},
publisher = {Springer, Berlin, Heidelberg},
title = {{How did software get so reliable without proof?}},
url = {http://link.springer.com/10.1007/3-540-60973-3{\_}77},
year = {1996}
}

@book{Hertzog2017a,
abstract = {Kali Linux has not only become the information security professional's platform of choice, but evolved into an industrial-grade, and world-class operating system distribution--mature, secure, and enterprise-ready. Through the decade-long development process, Muts and his team, along with countless volunteers from the hacker community, have taken on the burden of streamlining and organizing our work environment, freeing us from much of the drudgery. They provided a secure and reliable foundation, allowing us to concentrate on securing our digital world. An amazing community has built up around Kali Linux. Every month, more than 300,000 of us download a version of Kali. We come together in online and real-world training rooms and grind through the sprawling Offensive Security Penetration Testing Labs, pursuing the near-legendary Offensive Security certifications. We come together on the Kali forums, some 40,000 strong, and hundreds of us at a time can be found on the Kali IRC channel. We gather at conferences and attend Kali Dojos to learn from the developers themselves how to best leverage Kali. However, the Kali team has never released an official Kali Linux manual, until now. In this book, we'll focus on the Kali Linux platform itself, and help you understand and maximize Kali from the ground up. The developers will walk you through Kali Linux features and fundamentals, provide a crash course in basic Linux commands and concepts, and then walk you through the most common Kali Linux installation scenarios. You'll learn how to configure, troubleshoot and secure Kali Linux and then dive into the powerful Debian package manager. Throughout this expansive section, you'll learn how to install and configure packages, how to update and upgrade your Kali installation, and how to create your own custom packages. Then you'll learn how to deploy your custom installation across massive enterprise networks. Finally, you'll be guided through advanced topics such as kernel compilation, custom ISO creation, industrial-strength encryption, and even how to install crypto kill switches to safeguard your sensitive information. Whether you're a veteran or an absolute n00b, this is the best place to start with Kali Linux, the security professional's platform of choice.},
author = {Hertzog, Raphael and O'Gorman, Jim and Aharoni, Mati},
isbn = {9780997615609},
keywords = {DIVA model,speech acquisition and production,speech recognition system,speech target},
pages = {314},
title = {{Kali Linux Revealed: Mastering the Penetration Testing Distribution}},
year = {2017}
}

@article{Herbert1999,
abstract = {Software architectures address the high-level specification, design and analysis of software systems. Formal models can provide the essential underpinning for architectural description languages (ADLs), and formal techniques can play an important role in systems analysis. While formal models and formal analysis may always enhance conventional notations and methods, they are of the greatest benefit when they employ tractable models and efficient, mechanizable techniques. The novelty in our work has been in our effort to find and mechanize a general semantic framework for software architectures that can provide tractable models and support architectural formal analysis. The resultant semantic framework is a layered one: the core is a simple model of elements and topology, which provides the basis for general architectural theorems and proof techniques; the structural core is augmented by semantic layers representing the semantics of relevant properties of the design. The model has been implemented in the higher-order logic proof tool PVS (Prototype Verification System), and has been used in correctness proofs during a case study of a distributed transaction protocol. (17 References).},
author = {Herbert, John and Dutertre, Bruno and Riemenschneider, Robert and Stavridou, Victoria},
doi = {10.1007/3-540-48119-2_9},
isbn = {3540665870},
issn = {16113349},
pages = {116--133},
title = {{A formalization of Software Architecture}},
year = {1999}
}

@article{Henaff2017,
abstract = {In this work we introduce a new framework for performing temporal predictions in the presence of uncertainty. It is based on a simple idea of disentangling components of the future state which are predictable from those which are inherently unpredictable, and encoding the unpredictable components into a low-dimensional latent variable which is fed into a forward model. Our method uses a supervised training objective which is fast and easy to train. We evaluate it in the context of video prediction on multiple datasets and show that it is able to consistently generate diverse predictions without the need for alternating minimization over a latent space or adversarial training.},
archivePrefix = {arXiv},
arxivId = {1711.04994},
author = {Henaff, Mikael and Zhao, Junbo and LeCun, Yann},
eprint = {1711.04994},
month = {nov},
title = {{Prediction Under Uncertainty with Error-Encoding Networks}},
url = {http://arxiv.org/abs/1711.04994},
year = {2017}
}

@book{Healey2003,
author = {Healey, Alan},
booktitle = {Kivung},
title = {{ENGLISH IDIOMS Dictionary}},
year = {2003}
}

@incollection{Hayashi1994,
author = {Hayashi, Susumu},
doi = {10.1007/3-540-58085-9_74},
pages = {108--126},
publisher = {Springer, Berlin, Heidelberg},
title = {{Logic of refinement types}},
url = {http://link.springer.com/10.1007/3-540-58085-9{\_}74},
year = {1994}
}

@article{Caballero2010,
abstract = {Goal Programming (GP) can be regarded as one of the most widely used multicriteria decision-making techniques. In this paper, two surveys are carried out. First, the evolution of GP since its birth to the present time, in terms of number of publications, references, journals, etc., has been studied. Second, a more in-depth survey has been carried out, which covers the publications from year 2000 to the present time. All the references are listed, and some conclusions and future research lines have been extracted about the late trends of GP. Copyright copyright 2010 John Wiley {\&} Sons, Ltd.},
author = {Hansen, Paul and Ombler, Franz},
doi = {10.1002/mcda.428},
isbn = {1099-1360},
issn = {10579214},
journal = {Journal of Multi-Criteria Decision Analysis},
keywords = {goal programming,historical survey,multiple objective programming},
month = {may},
number = {3-4},
pages = {87--107},
title = {{A new method for scoring additive multi-attribute value models using pairwise rankings of alternatives}},
url = {http://dx.doi.org/10.1002/mcda.442 http://doi.wiley.com/10.1002/mcda.428},
volume = {15},
year = {2008}
}

@article{Hall,
author = {Hall, Patrick A V},
title = {{Relational Algebras, Logic, And Functional Programming}}
}

@inproceedings{Gupta2019,
author = {Gupta, Amit and Xu, Weijia and Jaiswal, Pankaj and Taylor, Crispin and Regala, Jennifer},
doi = {10.1145/3332186.3332255},
pages = {1--7},
publisher = {Association for Computing Machinery (ACM)},
title = {{Extracting Domain Information using Deep Learning}},
year = {2019}
}

@article{Guibas2011,
abstract = {See, stats, and : https : / / www . researchgate. net / publication / 228390416 Basic Computational Article CITATION 1 READS 19 1 : Leonidas . Guibas Stanford 518 , 063 SEE All . Guibas . The . All - text and , letting .},
author = {Guibas, Leonidas},
journal = {Stanford University, Technical Report},
pages = {1--51},
title = {{Basic Algorithms and Combinatorics in Computational Arrangements of Lines in the Plane}},
year = {2011}
}

@article{Guerraoui2019,
abstract = {Byzantine reliable broadcast is a powerful primitive that allows a set of processes to agree on a message from a designated sender, even if some processes (including the sender) are Byzantine. Existing broadcast protocols for this setting scale poorly, as they typically build on quorum systems with strong intersection guarantees, which results in linear per-process communication and computation complexity. We generalize the Byzantine reliable broadcast abstraction to the probabilistic setting, allowing each of its properties to be violated with a fixed, arbitrarily small probability. We leverage these relaxed guarantees in a protocol where we replace quorums with stochastic samples. Compared to quorums, samples are significantly smaller in size, leading to a more scalable design. We obtain the first Byzantine reliable broadcast protocol with logarithmic per-process communication and computation complexity. We conduct a complete and thorough analysis of our protocol, deriving bounds on the probability of each of its properties being compromised. During our analysis, we introduce a novel general technique we call adversary decorators. Adversary decorators allow us to make claims about the optimal strategy of the Byzantine adversary without having to make any additional assumptions. We also introduce Threshold Contagion, a model of message propagation through a system with Byzantine processes. To the best of our knowledge, this is the first formal analysis of a probabilistic broadcast protocol in the Byzantine fault model. We show numerically that practically negligible failure probabilities can be achieved with realistic security parameters.},
archivePrefix = {arXiv},
arxivId = {1908.01738},
author = {Guerraoui, Rachid and Kuznetsov, Petr and Monti, Matteo and Pavlovic, Matej and Seredinschi, Dragos-Adrian},
doi = {10.4230/LIPIcs.DISC.2019.22},
eprint = {1908.01738},
title = {{Scalable Byzantine Reliable Broadcast (Extended Version)}},
url = {http://arxiv.org/abs/1908.01738 http://dx.doi.org/10.4230/LIPIcs.DISC.2019.22},
year = {2019}
}

@misc{Griffiths2015,
abstract = {This paper is a conversion optimisation framework tailored to “Smartphone” mobile applications (not tablet applications). It defines the key principles and considerations - i.e. the important things to think about - when assessing Smartphone apps, in order to identify what and how to improve the user experience, optimise conversion and better measure app performance. While this paper can inform the creation of a new app, it is intended for the optimisation of existing apps.},
author = {Griffiths, Stephen},
number = {April},
pages = {49},
publisher = {Google},
title = {{Mobile App UX Principles Mobile App UX Principles}},
year = {2015}
}

@inproceedings{Gray1996,
author = {Gray, Jim and Bosworth, A and Lyaman, A and Pirahesh, H},
booktitle = {Proceedings of the Twelfth International Conference on Data Engineering},
doi = {10.1109/ICDE.1996.492099},
isbn = {0-8186-7240-4},
pages = {152--159},
publisher = {IEEE Comput. Soc. Press},
title = {{Data cube: a relational aggregation operator generalizing GROUP-BY, CROSS-TAB, and SUB-TOTALS}},
url = {http://ieeexplore.ieee.org/document/492099/},
year = {1996}
}

@article{Gray,
abstract = {Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N -dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, cross-tabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N -space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N -dimensional cube. Super-aggregates are computed by aggregating the N -cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.},
author = {Gray, Jim},
keywords = {aggregation,analysis,data cube,data mining,database,query,summarization},
title = {{Data Cube : A Relational Aggregation Operator}}
}

@article{Grasby2019,
author = {Grasby, Katrina L and Little, Callie W and Byrne, Brian and Coventry, William L and Olson, Richard K and Larsen, Sally and Samuelsson, Stefan},
doi = {10.1037/edu0000418},
issn = {1939-2176},
journal = {Journal of Educational Psychology},
month = {oct},
title = {{Estimating classroom-level influences on literacy and numeracy: A twin study.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/edu0000418},
year = {2019}
}

@techreport{Granville2019a,
author = {Granville, Vincent},
title = {{Statistics: New Foundations, Toolbox, and Machine Learning Recipes}},
url = {www.DataScienceCentral.com},
year = {2019}
}

@article{Graham2002,
author = {Graham, Paul},
title = {{The Roots of Lisp}},
year = {2002}
}

@article{Graham,
author = {Graham, Daniel},
title = {{The Ethical Hacking Manual}}
}

@article{Graf1994,
abstract = {The performance of a theorem prover crucially depends on the speed of the basic retrieval operations, such as finding terms that are unifiable with (instances of, or more general than) a given query term. In this paper a new indexing method is presented, which outperforms traditional methods such as path indexing, discrimination tree indexing and abstraction trees. Additionally, the new index not only supports term indexing but also provides maintenance and efficient retrieval of substitutions. As confirmed in multiple experiments, substitution trees combine maximal search speed and minimal memory requirements.},
author = {Graf, Peter},
doi = {10.1007/3-540-59200-8_52},
pages = {39},
title = {{Substitution Tree Indexing}},
year = {1994}
}

@book{Govindaraj2015,
abstract = {Develop high-quality and maintainable Python applications using the principles of test-driven development About This BookWrite robust and easily maintainable code using the principles of test driven developmentGet solutions to real-world problems faced by Python developersGo from a unit testing beginner to a master through a series of step-by-step tutorials that are easy to followWho This Book Is ForThis book is intended for Python developers who want to use the principles of test-driven development (TDD) to create efficient and robust applications. In order to get the best out of this book, you should have development experience with Python.What You Will Learn Implement the test-driven development process in Python applications Fully leverage Python's powerful built-in unittest and doctest modules Effectively add features to existing code bases that do not have any tests Safely resolve problems with the code and design, without breaking any functionality Utilize Python's powerful mock and patch functionality to test complex interactions Integrate unit testing into the overall software delivery process Use doctest to test code with examples Enhance TDD with the nose2 test runner In DetailThis book starts with a look at the test-driven development process, and how it is different from the traditional way of writing code. All the concepts are presented in the context of a real application that is developed in a step-by-step manner over the course of the book. While exploring the common types of smelly code, we will go back into our example project and clean up the smells that we find.Additionally, we will use mocking to implement the parts of our example project that depend on other systems. Towards the end of the book, we'll take a look at the most common patterns and anti-patterns associated with test-driven development, including integration of test results into the development process.},
author = {Govindaraj, Siddharta},
isbn = {978-1-78398-792-4},
pages = {300},
title = {{Test-Driven Python Development}},
url = {https://www.amazon.com/Driven-Python-Development-Siddharta-Govindaraj/dp/1783987928/ref=sr{\_}1{\_}1?s=books{\&}ie=UTF8{\&}qid=1499943042{\&}sr=1-1{\&}keywords=Test-Driven+Python+Development{\%}0Ahttp://hym-2017.corpuscodea.es/pdf/Siddharta Govindaraj - Test-Driven Python Dev},
year = {2015}
}

@techreport{Goss,
author = {Goss, Nuzzo-Jones and Walonoski, Jason A and Heffernan, Neil T and Livak, Tom and Luengo, Vanda and Vadcard, Lucile and Goldrei, Simon and Kay, Judy and Kummerfeld, Bob and Libbrecht, Paul and Machuca, Enrique and Spanbroek, Mark and Blank, Glenn and Parvez, Shahida and Moritz, Sally and Turner, Terence E and Macasek, Michael A},
title = {{Table of contents Preface The eXtensible Tutor Architecture: A New Foundation for ITS Design of Adaptive Feedback in a Web Educational System Exploiting User Models to Automate the Harvesting of Metadata for Learning Objects MEDEA: an Open Service-Based L}}
}

@book{Goransson2016,
abstract = {Software Defined Networks: A Comprehensive Approach, Second Edition provides in-depth coverage of the technologies collectively known as Software Defined Networking (SDN). The book shows how to explain to business decision-makers the benefits and risks in shifting parts of a network to the SDN model, when to integrate SDN technologies in a network, and how to develop or acquire SDN applications. In addition, the book emphasizes the parts of the technology that encourage opening up the network, providing treatment for alternative approaches to SDN that expand the definition of SDN as networking vendors adopt traits of SDN to their existing solutions. Since the first edition was published, the SDN market has matured, and is being gradually integrated and morphed into something more compatible with mainstream networking vendors. This book reflects these changes, with coverage of the OpenDaylight controller and its support for multiple southbound protocols, the Inclusion of NETCONF in discussions on controllers and devices, expanded coverage of NFV, and updated coverage of the latest approved version (1.5.1) of the OpenFlow specification. - Contains expanded coverage of controllers - Includes a new chapter on NETCONF and SDN - Presents expanded coverage of SDN in optical networks - Provides support materials for use in computer networking courses},
author = {Goransson, Paul and Black, Chuck and Culver, Timothy},
edition = {2},
isbn = {9780128045794},
keywords = {Computers › Networking › General,Technology {\&} Engineering / Mobile {\&} Wireless Commu,Technology {\&} Engineering / Telecommunications},
mendeley-tags = {Computers › Networking › General,Technology {\&} Engineering / Mobile {\&} Wireless Commu,Technology {\&} Engineering / Telecommunications},
pages = {436},
publisher = {Morgan Kaufmann},
title = {{Software Defined Networks: A Comprehensive Approach}},
year = {2016}
}

@book{Goodfellow2016,
abstract = {Deep Learning provides a truly comprehensive look at the state of the art in deep learning and some developing areas of research. The authors are Ian Goodfellow, along with his Ph.D. advisor Yoshua Bengio, and Aaron Courville. All three are widely published experts in the field of artificial intelligence (AI). In addition to being available in both hard cover and Kindle the authors also make the individual chapter PDFs available for free on the Internet.Footnote1 The book is aimed at an academic research audience with prior knowledge of calculus, linear algebra, probability, and some programming capabilities. A non-mathematical reader will find this book difficult. A comprehensive, well cited coverage of the field makes this book a valuable reference for any researcher. The book provides a mathematical description of a comprehensive set of deep learning algorithms, but could benefit from more pseudocode examples. The authors provide an adequate explanation for the many mathematical formulas that are used to communicate the ideas expressed in this book. The lack of both exercises and examples in any of the major machine learning software packages makes this book difficult as a primary undergraduate textbook.},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
isbn = {0262035618},
pages = {775},
publisher = {MIT Press},
title = {{Deep Learning}},
year = {2016}
}

@article{Gonzalez-Soto2019,
abstract = {Decision making under uncertain conditions has been well studied when uncertainty can only be considered at the associative level of information. The classical Theorems of von Neumann-Morgenstern and Savage provide a formal criterion for rationally making choices using associative information. We provide here a previous result from Pearl and show that it can be considered as a causal version of the von Neumann-Morgenstern Theorem; furthermore, we consider the case when the true causal mechanism that controls the environment is unknown to the decision maker and propose a causal version of the Savage Theorem. As applications, we argue how previous optimal action learning methods for causal environments fit within the Causal Savage Theorem we present thus showing the utility of our result in the justification and design of learning algorithms; furthermore, we define a Causal Nash Equilibria for a strategic game in a causal environment in terms of the preferences induced by our Causal Decision Making Theorem.},
archivePrefix = {arXiv},
arxivId = {1907.11752},
author = {Gonzalez-Soto, Mauricio and Sucar, Luis E and Escalante, Hugo J},
eprint = {1907.11752},
month = {jul},
title = {{von Neumann-Morgenstern and Savage Theorems for Causal Decision Making}},
url = {http://arxiv.org/abs/1907.11752},
year = {2019}
}

@book{Gonzalez-Diaz2013a,
abstract = {This book constitutes the thoroughly refereed proceedings of the 17th International Conference on Discrete Geometry for Computer Imagery, DGCI 2013, held in Seville, Spain, in March 2013. The 34 revised full papers presented were carefully selected from 56 submissions and focus on geometric transforms, discrete and combinatorial tools for image segmentation and analysis, discrete and combinatorial topology, discrete shape representation, recognition and analysis, models for discrete geometry, morphological analysis and discrete tomography.},
address = {Berlin, Heidelberg},
booktitle = {17th IAPR International Conference, DGCI 2013},
doi = {10.1007/978-3-642-37067-0},
editor = {Gonzalez-Diaz, Rocio and Jimenez, Maria-Jose and Medrano, Belen},
isbn = {978-3-642-37066-3},
pages = {408},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Discrete Geometry for Computer Imagery}},
url = {http://link.springer.com/10.1007/978-3-642-37067-0},
volume = {7749},
year = {2013}
}

@article{Goldstein2009,
abstract = {Conway and Kochen have presented a "free will theorem" (Notices of the AMS 56, pgs. 226-232 (2009)) which they claim shows that "if indeed we humans have free will, then [so do] elementary particles." In a more precise fashion, they claim it shows that for certain quantum experiments in which the experimenters can choose between several options, no deterministic or stochastic model can account for the observed outcomes without violating a condition "MIN" motivated by relativistic symmetry. We point out that for stochastic models this conclusion is not correct, while for deterministic models it is not new.},
archivePrefix = {arXiv},
arxivId = {0905.4641},
author = {Goldstein, Sheldon and Tausk, Daniel V and Tumulka, Roderich and Zanghi, Nino},
eprint = {0905.4641},
number = {December},
pages = {1451--1453},
title = {{What Does the Free Will Theorem Actually Prove?}},
url = {http://arxiv.org/abs/0905.4641},
year = {2009}
}

@techreport{Glen2019,
abstract = {www.DataScienceCentral.com This online book is intended for beginners, college students and professionals confronted with statistical analyses. It is also a refresher for professional statisticians. The book covers over 600 concepts, chosen out of more than 1,500 for their popularity.},
author = {Glen, Stephanie},
title = {{Online Encyclopedia of Statistical Science}},
url = {www.DataScienceCentral.com},
year = {2019}
}

@article{Gheorghies2019,
abstract = {MetaUML is a MetaPost [1] library for creating UML [2] diagrams by means of a textual notation. While presenting the inner workings of MetaUML, this manual doubles as a step-by-step tutorial. More impor- tantly, its source code contains many useful examples of diagrams, ranging from the very basic to the more advanced and customized.},
author = {Gheorghies, Ovidiu},
title = {{MetaUML: A Manual and Test Suite}},
year = {2019}
}

@book{GennadiyZlobin2013,
abstract = {This book takes a tutorial-based and user-friendly approach to covering Python design patterns. Its concise presentation means that in a short space of time, you will get a good introduction to various design patterns.If you are an intermediate level Python user, this book is for you. Prior knowledge of Python programming is essential. Some knowledge of UML is also required to understand the UML diagrams which are used to describe some design patterns.},
author = {{Gennadiy Zlobin}},
isbn = {978-1-78328-337-8},
keywords = {Computer Books: General,Computer science,Computers,Computers - General Information,Computers / Data Processing,Computers : Data Modeling {\&} Design,Computers : Programming Languages - Python,Computing: Textbooks {\&} Study Guides,Data Modeling {\&} Design,Data Processing,Gennadiy Zlobin,Learning Python Design Patterns,Packt Publishing,Programming {\&} scripting languages: general,Programming Languages - Python,Programming Languages / Python},
pages = {100},
publisher = {Packt Publishing},
title = {{Learning Python Design Patterns}},
year = {2013}
}

@article{Ge2009,
abstract = {First order logic provides a convenient formalism for describing a wide variety of verification conditions. Two main approaches to checking such conditions are pure first order automated theorem proving (ATP) and automated theorem proving based on satisfiability modulo theories (SMT). Traditional ATP systems are designed to handle quantifiers easily, but often have difficulty reasoning with respect to theories. SMT systems, on the other hand, have built-in support for many useful theories, but have a much more difficult time with quantifiers. One clue on how to get the best of both worlds can be found in the legacy system Simplify which combines built-in theory reasoning with quantifier instantiation heuristics. Inspired by Simplify and motivated by a desire to provide a competitive alternative to ATP systems, this paper describes a methodology for reasoning about quantifiers in SMT systems. We present the methodology in the context of the Abstract DPLL Modulo Theories framework. Besides adapting many of Simplify's techniques, we also introduce a number of new heuristics. Most important is the notion of instantiation level which provides an effective mechanism for prioritizing and managing the large search space inherent in quantifier instantiation techniques. These techniques have been implemented in the SMT system CVC3. Experimental results show that our methodology enables CVC3 to solve a significant number of benchmarks that were not solvable with any previous approach.},
author = {Ge, Yeting and Barrett, Clark and Tinelli, Cesare},
doi = {10.1007/s10472-009-9153-6},
issn = {10122443},
journal = {Annals of Mathematics and Artificial Intelligence},
keywords = {First-order logic,Quantified verification conditions,Quantifier instantiation,Satisfiability modulo theories},
number = {1-2},
pages = {101--122},
title = {{Solving quantified verification conditions using satisfiability modulo theories}},
volume = {55},
year = {2009}
}

@article{Gavin1964,
abstract = {This MMWR supplement summarizes the deliberations of CDC/ATSDR scientists and managers who met in September 2009 in Atlanta as part of the 2009 Consultation on CDC/ATSDR's Vision for Public Health Surveillance in the 21st Century. The meeting was convened to reflect on domestic and global public health surveillance practice and to recommend a strategic framework to advance public health surveillance to meet continuing and new challenges. The first report is an adaptation of the keynote address for the meeting, which summarized the history of public health surveillance, the need to reassess its usefulness, the rationale for topics selected for discussion, and the charge to participants. Subsequent reports summarize the discussions of workgroups that addressed specific topics in surveillance science and practices.},
author = {Gavin, M R},
doi = {10.1088/0031-9112/15/10/010},
issn = {0031-9112},
journal = {Physics Bulletin},
number = {10},
pages = {246},
title = {{Introduction to Electronics}},
volume = {15},
year = {1964}
}

@article{Gaunt2016,
abstract = {We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TerpreT, a domain-specific language for expressing program synthesis problems. A TerpreT model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of input-output examples and infer the underlying program. From a TerpreT model we automatically perform inference using four different back-ends: gradient descent (thus each TerpreT model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference. We illustrate the value of TerpreT by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models. To our knowledge, this is the first work to compare gradient-based search over program space to traditional search-based alternatives. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. This is a workshop summary of a longer report at arXiv:1608.04428},
archivePrefix = {arXiv},
arxivId = {1612.00817},
author = {Gaunt, Alexander L and Brockschmidt, Marc and Singh, Rishabh and Kushman, Nate and Kohli, Pushmeet and Taylor, Jonathan and Tarlow, Daniel},
eprint = {1612.00817},
pages = {1--7},
title = {{Summary - TerpreT: A Probabilistic Programming Language for Program Induction}},
url = {http://arxiv.org/abs/1612.00817},
year = {2016}
}

@article{Gaunt2016a,
abstract = {We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community. Our key contribution is the proposal of TerpreT, a domain-specific language for expressing program synthesis problems. TerpreT is similar to a probabilistic programming language: a model is composed of a specification of a program representation (declarations of random variables) and an interpreter describing how programs map inputs to outputs (a model connecting unknowns to observations). The inference task is to observe a set of input-output examples and infer the underlying program. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing like-to-like comparisons between different approaches to inference. From a single TerpreT specification we automatically perform inference using four different back-ends. These are based on gradient descent, linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. We illustrate the value of TerpreT by developing several interpreter models and performing an empirical comparison between alternative inference algorithms. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. We conclude with suggestions for the machine learning community to make progress on program synthesis.},
archivePrefix = {arXiv},
arxivId = {1608.04428},
author = {Gaunt, Alexander L and Brockschmidt, Marc and Singh, Rishabh and Kushman, Nate and Kohli, Pushmeet and Taylor, Jonathan and Tarlow, Daniel},
eprint = {1608.04428},
title = {{TerpreT: A Probabilistic Programming Language for Program Induction}},
url = {http://arxiv.org/abs/1608.04428},
year = {2016}
}

@unpublished{Gassmann2013,
abstract = {We have looked at 250 business model innovators and found out that about 90{\%} of their innovations are recombinations of previously existing concepts, ideas, or business models. This insight can be used proactively: We identified 55 repetitive patterns, which allow you to construct your own business model. Click on the Business Model Innovation Map on the left to download a summary which explains the process.},
author = {Gassmann, Oliver and Frankenberger, Karolin and Csik, Michaela},
institution = {University of St.Gallen},
pages = {18},
title = {{The St . Gallen Business Model Navigator}},
year = {2013}
}

@article{Ganesalingam2017,
abstract = {This paper describes a program that solves elementary mathematical problems, mostly in metric space theory, and presents solutions that are hard to distinguish from solutions that might be written by human mathematicians.},
author = {Ganesalingam, M and Gowers, W T},
doi = {10.1007/s10817-016-9377-1},
issn = {15730670},
journal = {Journal of Automated Reasoning},
keywords = {ATP,Automated theorem proving,Human-like output,Human-oriented,Human-oriented theorem proving},
number = {2},
pages = {253--291},
publisher = {Springer Netherlands},
title = {{A Fully Automatic Theorem Prover with Human-Style Output}},
volume = {58},
year = {2017}
}

@article{Gallier2018,
abstract = {Summary Stress-induced diseases in modern life are on an alarming rise not only in developed countries but also in developing ones. To alleviate stress, one practice that is being commonly and increasingly adapted to is meditation. Limited studies on meditation have reported occurrence of mental calmness along with apparently favorable changes in certain autonomic functional parameters like heart rate, blood pressure, respiration and skin resistance. Recently, meditation is also being practiced and advised for alleviation of epilepsy; however, very little work is available to comprehend effect and utility of meditation on epilepsy. Neuro-imaging and in-depth studies during the course and attainment of meditational state have revealed alteration in neuro-chemistry and neuro-physiology of brain environment that could favor epileptogenesis. The rise in brain glutamate and serotonin along with development of ‘hypersynchrony' of EEG activity (which occur during the course and attainment of meditational state) are well documented to form the underlying basis of epilepsy. Each of the above-mentioned factors is individually capable of inducing susceptibility and decreasing threshold to epilepsy. Based on these changes in brain, this paper raises a grave possibility and risk of meditation in developing epilepsy or increasing the severity and frequency of attacks in an already epileptic state, contrary to the popular belief of its remedial role in alleviating epilepsy.},
author = {Gallier, Jean and Quaintance, Jocelyn},
doi = {10.1016/j.mehy.2004.09.012},
isbn = {0306-9877},
issn = {0306-9877},
title = {{Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Engineering}},
url = {http://www.cis.upenn.edu/{\%}7B{~}{\%}7Djean/math-basics.pdf},
year = {2018}
}

@book{Gallier2011,
author = {Gallier, Jean},
pages = {88},
title = {{Basics of Algebra and Analysis For Computer Science}},
year = {2011}
}

@book{Fritzson2010,
abstract = {Provides an introduction to modern object-oriented design principles and applications for the fast-growing area of modeling and simulation Covers the topic of multi-domain system modeling and design with applications that have components from several areas Serves as a reference for the Modelica language as well as a comprehensive overview of application model libraries for a number of application domains},
author = {Fritzson, Peter},
isbn = {0470937610},
keywords = {acceleration algorithm algorithm section angle ang},
mendeley-tags = {acceleration algorithm algorithm section angle ang},
pages = {944},
publisher = {John Wiley {\&} Sons},
title = {{Principles of object-oriented modeling and simulation with Modelica 2.1}},
year = {2010}
}

@article{Fried2009,
abstract = {Getting Real details the business, design, programming, and marketing principles of 37signals. The book is packed with keep-it-simple insights, contrarian points of view, and unconventional approaches to software design. This is not a technical book or a design tutorial, it's a book of ideas.Anyone working on a web app ' including entrepreneurs, designers, programmers, executives, or marketers ' will find value and inspiration in this book.37signals used the Getting Real process to launch five successful web-based applications (Basecamp, Campfire, Backpack, Writeboard, Ta-da List), and Ruby on Rails, an open-source web application framework, in just two years with no outside funding, no debt, and only 7 people (distributed across 7 time zones). Over 500,000 people around the world use these applications to get things done.Now you can find out how they did it and how you can do it too. It's not as hard as you think if you Get Real.},
author = {Fried, Jason and Heinemeier, David and Linderman, Matthew},
isbn = {9780578012810},
journal = {Time},
pages = {194},
title = {{Getting Real — The smarter, faster, easier way to build a successful web application}},
url = {http://books.google.pt/books?id=NIZNQwAACAAJ{\&}dq=37signals+{\%}22getting+real{\%}22{\&}hl=pt-PT{\&}ei=SVvOTrvFAtP78QO46qm8Dw{\&}sa=X{\&}oi=book{\_}result{\&}ct=result{\&}resnum=1{\&}ved=0CC4Q6AEwAA},
year = {2009}
}

@article{Freeman1991,
author = {Freeman, Tim and Pfenning, Frank},
doi = {10.1145/113446.113468},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
month = {jun},
number = {6},
pages = {268--277},
title = {{Refinement types for ML}},
url = {http://portal.acm.org/citation.cfm?doid=113446.113468},
volume = {26},
year = {1991}
}

@article{Fowler1997,
abstract = {Written for those already grounded in object-oriented analysis and design, this concise overview introduces you to UML, highlighting the key elements of its notation, semantics, and processes. Included is a brief explanation of UML's history, development, and rationale, as well as discussions on how UML can be integrated into the object-oriented development process. In addition, the book profiles various modeling techniques associated with UML - use cases, CRC cards, design by contract, dynamic classification, interfaces, and abstract classes - along with concise descriptions of notation and semantics and numerous insightful tips for effective use based on the authors' experience. In addition, the authors offer the first look at the emerging Objectory Software Development Process derived from the methodologies of Grady Booch, Ivar Jacobson, and James Rumbaugh. To give you a feel for the UML in action, the book includes a Java programming example that outlines the implementation of a UML-based design. You will come away with an excellent understanding of UML essentials, insight into how UML functions within the software development process, and a firm foundation upon which to expand and build your knowledge of the Unified Modeling Language. Foreword -- Preface -- Acknowledgments -- Ch. 1. Introduction -- Ch. 2. An Outline Development Process -- Ch. 3. Use Cases -- Ch. 4. Class Diagrams: The Essentials -- Ch. 5. Class Diagrams: Advanced Concepts -- Ch. 6. Interaction Diagrams -- Ch. 7. Package Diagrams -- Ch. 8. State Diagrams -- Ch. 9. Activity Diagrams -- Ch. 10. Deployment Diagrams -- Ch. 11. UML and Programming -- App. Techniques and Their Uses -- Bibliography -- Index.},
author = {Fowler, Martin and Scott, Kendall},
isbn = {0201325632},
pages = {179},
title = {{UML distilled : applying the standard object modeling language}},
url = {https://dl.acm.org/citation.cfm?id=270005},
year = {1997}
}

@book{Fouladi,
abstract = {We present gg, a framework and a set of command-line tools that helps people execute everyday applications-e.g., software compilation, unit tests, video encoding, or object recognition-using thousands of parallel threads on a cloud-functions service to achieve near-interactive completion times. In the future, instead of running these tasks on a laptop, or keeping a warm cluster running in the cloud, users might push a button that spawns 10,000 parallel cloud functions to execute a large job in a few seconds from start. gg is designed to make this practical and easy. With gg, applications express a job as a composition of lightweight OS containers that are individually transient (life-times of 1-60 seconds) and functional (each container is hermetically sealed and deterministic). gg takes care of instantiating these containers on cloud functions, loading dependencies, minimizing data movement, moving data between containers, and dealing with failure and stragglers. We ported several latency-sensitive applications to run on gg and evaluated its performance. In the best case, a distributed compiler built on gg outperformed a conventional tool (icecc) by 2-5×, without requiring a warm cluster running continuously. In the worst case, gg was within 20{\%} of the hand-tuned performance of an existing tool for video encoding (ExCamera).},
author = {Fouladi, Sadjad and Romero, Francisco and Iter, Dan and Li, Qian and Chatterjee, Shuvo and Kozyrakis, Christos and Zaharia, Matei and Winstein, Keith},
isbn = {978-1-939133-03-8},
title = {{From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient Functional Containers}},
url = {https://snr.stanford.edu/gg.}
}

@book{Forouzan2012,
abstract = {Technologies related to networks and internetworking may be the fastest growing in our culture today. The appearance of some new social networking applications every year is a testimony to this claim. People use the Internet more and more every day. They use the Internet for research, shopping, airline reservations, checking the latest news, weather, and so on. In this Internet-oriented society, specialists need be trained to run and manage the Internet, part of the Internet, or an organization's network that is connected to the Internet. This book is designed to help students understand the basics of networking in general and the protocols used in the Internet in particular.},
author = {Forouzan, Behrouz A. and Mosharraf, Firouz},
editor = {International, McGraw-Hill},
isbn = {9780071315159},
pages = {931},
title = {{Computer Networks: A Top-down Approach}},
year = {2012}
}

@article{Fong2019,
abstract = {A supervised learning algorithm searches over a set of functions A → B parametrised by a space P to find the best approximation to some ideal function f:A → B. It does this by taking examples (a, f(a)) ϵ A × B, and updating the parameter according to some rule. We define a category where these update rules may be composed, and show that gradient descent-with respect to a fixed step size and an error function satisfying a certain property-defines a monoidal functor from a category of parametrised functions to this category of update rules. A key contribution is the notion of request function. This provides a structural perspective on backpropagation, giving a broad generalisation of neural networks and linking it with structures from bidirectional programming and open games.},
archivePrefix = {arXiv},
arxivId = {1711.10455},
author = {Fong, Brendan and Spivak, David and Tuyeras, Remy},
doi = {10.1109/LICS.2019.8785665},
eprint = {1711.10455},
isbn = {9781728136080},
issn = {10436871},
journal = {Proceedings - Symposium on Logic in Computer Science},
pages = {1--17},
title = {{Backprop as Functor: A compositional perspective on supervised learning}},
volume = {2019-June},
year = {2019}
}

@article{Fong2018a,
abstract = {This book is an invitation to discover advanced topics in category theory through concrete, real-world examples. It aims to give a tour: a gentle, quick introduction to guide later exploration. The tour takes place over seven sketches, each pairing an evocative application, such as databases, electric circuits, or dynamical systems, with the exploration of a categorical structure, such as adjoint functors, enriched categories, or toposes. No prior knowledge of category theory is assumed. A feedback form for typos, comments, questions, and suggestions is available here: https://docs.google.com/document/d/160G9OFcP5DWT8Stn7TxdVx83DJnnf7d5GML0{\_}FOD5Wg/edit},
archivePrefix = {arXiv},
arxivId = {1803.05316},
author = {Fong, Brendan and Spivak, David I},
eprint = {1803.05316},
title = {{Seven Sketches in Compositionality: An Invitation to Applied Category Theory}},
url = {http://arxiv.org/abs/1803.05316},
year = {2018}
}

@article{Fokkinga1994,
abstract = {In these notes we present the important notions from category theory. The intention is to provide a fairly good skill in manipulating with those concepts formally. What you probably will not acquire from these notes is the ability to recognise the concepts in your daily work when that differs from algorithmics, since we give only a few examples and those are taken from algorithmics. For such an ability you need to work through many, very many examples, in diverse fields of applications.This text differs from most other introductions to category theory in the calculational style of the proofs (especially in and the restriction to applications within algorithmics, and the omission of many additional concepts and facts that I consider not helpful in a first introduction to category theory.},
author = {Fokkinga, Maarten},
issn = {03050041},
journal = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
pages = {80},
title = {{A Gentle Introduction to Category Theory — the calculational approach}},
url = {http://doc.utwente.nl/66620/1/db-utwente-0000003535.pdf},
year = {1994}
}

@book{Flajolet2009,
abstract = {Analytic Combinatorics is a self-contained treatment of the mathematics underlying the analysis of discrete structures, which has emerged over the past several decades as an essential tool in the understanding of properties of computer programs and scientific models with applications in physics, biology and chemistry. Thorough treatment of a large number of classical applications is an essential aspect of the presentation. Written by the leaders in the field of analytic combinatorics, this text is certain to become the definitive reference on the topic. The text is complemented with exercises, examples, appendices and notes to aid understanding therefore, it can be used as the basis for an advanced undergraduate or a graduate course on the subject, or for self-study.},
author = {Flajolet, Philippe and Sedgewick, Robert},
booktitle = {Analytic Combinatorics},
doi = {10.1017/CBO9780511801655},
isbn = {9780511801655},
pages = {1--810},
title = {{Analytic combinatorics}},
year = {2009}
}

@techreport{Flach,
author = {Flach, Peter},
title = {{Simply Logical Intelligent Reasoning by Example}}
}

@article{Fiorenza2001,
abstract = {This paper is an introduction to the language of Feynman Diagrams. We use Reshetikhin-Turaev graphical calculus to define Feynman diagrams and prove that asymptotic expansions of Gaussian integrals can be written as a sum over a suitable family of graphs. We discuss how different kind of interactions give rise to different families of graphs. In particular, we show how symmetric and cyclic interactions lead to ``ordinary'' and ``ribbon'' graphs respectively. As an example, the 't Hooft-Kontsevich model for 2D quantum gravity is treated in some detail.},
archivePrefix = {arXiv},
arxivId = {math/0106001},
author = {Fiorenza, Domenico and Murri, Riccardo},
eprint = {0106001},
month = {may},
primaryClass = {math},
title = {{Feynman Diagrams via Graphical Calculus}},
url = {http://arxiv.org/abs/math/0106001},
year = {2001}
}

@book{Finch2011,
abstract = {This is a beginner's guide with clear step-by-step instructions, explanations, and advice. Each concept is illustrated with a complete example that you can use as a starting point for your own work. If you are an engineer, scientist, mathematician, or student, this book is for you. To get the most from Sage by using the Python programming language, we'll give you the basics of the language to get you started. For this, it will be helpful if you have some experience with basic programming concepts.},
author = {Finch, Craig},
isbn = {184951447X},
keywords = {{\_}{\_}init{\_}{\_} algebra algorithm armor{\_}values base class},
mendeley-tags = {{\_}{\_}init{\_}{\_} algebra algorithm armor{\_}values base class},
pages = {364},
publisher = {Packt Publishing Ltd},
title = {{Sage Beginner's Guide}},
year = {2011}
}

@article{Feurer2015,
abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML.We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn. 1},
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost Tobias and Blum, Manuel and Hutter, Frank},
doi = {10.1016/j.sbspro.2015.09.090},
isbn = {0863775349},
issn = {1877-0428},
journal = {Procedia - Social and Behavioral Sciences},
pages = {315--320},
title = {{Efficient and Robust Automated Machine Learning}},
url = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning},
volume = {205},
year = {2015}
}

@article{Fawaz2018,
abstract = {Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.},
archivePrefix = {arXiv},
arxivId = {1808.02455},
author = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
eprint = {1808.02455},
title = {{Data augmentation using synthetic data for time series classification with deep residual networks}},
url = {http://arxiv.org/abs/1808.02455},
year = {2018}
}

@inproceedings{Farhan2008,
abstract = {The past few years has witnessed an increasing in both academic and industry to integrate the expert systems with database management systems (DBMS) technologies to develop more intelligent applications. This integration gives the expert systems the power of database management features and ability to access large volume of information in real time. The strength of the DBMS is storing and manipulating large quantities of data. Similarly, expert systems benefits from having capabilities to infer new data (or facts) from the existing database, as well as, easily access a large quantities of data. This data can be used to produces expert advice, ranging from helpful suggestions to outright decisions. Understanding the limitations of the DBMS and expert systems helps to explain the need for their integration. In this paper we design and implementing a visual environment for building and integrating expert systems with various DBMS. It consists of two independent but complementary subsystems. The first subsystem, knowledge base builder (KBB), is used to build the knowledge base for an expert system from various DBMS, such as, Oracle, MS-SQL Server, MS-Access, ⋯ etc. The second subsystem, visual expert system builder (VESB), is used to generate a complete expert system in PROLOG language, and integrate it with the knowledge base that generated by the KBB subsystem. Finally, the main characteristics of the system are: efficiency, simplicity of use, and high degree of portability, which make it an ideal choice for generating various types of expert systems.},
author = {Farhan, H A},
booktitle = {Proceedings of the 2008 International Conference on Artificial Intelligence, ICAI 2008 and Proceedings of the 2008 International Conference on Machine Learning; Models, Technologies and Applications},
isbn = {1601320728 (ISBN); 9781601320728 (ISBN)},
keywords = {Applications,Artificial intelligence,Complementary subsystems,Database management,Database management systems,Database managements,Database systems,Expert advice,Expert advices,Expert systems,Information management,Integration,Intelligent applications,Knowledge base,Learning systems,Main characteristics,Management information systems,Problem solving,Real time,Real time systems,Robot learning,SQL servers,Structural design,System builders,Visual environments,Visual languages},
title = {{Design and implementation of a visual environment for building and integrating expert systems with various DBMS}},
year = {2008}
}

@article{Fadous,
author = {Fadous, Raymond and Forsyth, John},
doi = {10.1145/500108.500109},
title = {{Finding Candidate Keys For Relational Data Bases}}
}

@techreport{Everitt,
author = {Everitt, Brent},
title = {{Symmetries of Equations: An Introduction to Galois Theory}}
}

@article{Eriksson2004,
author = {Eriksson, Kenneth and Johnson, Claes and Estep, Donald and Eriksson, Kenneth and Johnson, Claes and Estep, Donald},
doi = {10.1007/978-3-662-05800-8_11},
journal = {Applied Mathematics: Body and Soul},
pages = {911--928},
title = {{Double Integrals}},
year = {2004}
}

@book{Erdman2014,
abstract = {This textbook is suitable for a course in advanced calculus that promotes active learning through problem solving. It can be used as a base for a Moore method or inquiry based class, or as a guide in a traditional classroom setting where lectures are organized around the presentation of problems and solutions. This book is appropriate for any student who has taken (or is concurrently taking) an introductory course in calculus. The book includes sixteen appendices that review some indispensable prerequisites on techniques of proof writing with special attention to the notation used the course.},
author = {Erdman, John M},
isbn = {1470442469},
pages = {377},
publisher = {Portland State University},
title = {{A Problems Based Course in Advanced Calculus}},
year = {2014}
}

@book{Elmqvist1999,
abstract = {This document is a tutorial for the Modelica language, version 1.4, which is developed by the Modelica Association, a non-profit organization with seat in Link{\"{o}}ping, Sweden. Modelica is a freely available, object-oriented language for modeling of large, complex, and heterogeneous physical systems. It is suited for multi-domain modeling, for example, mechatronic models in robotics, automotive and aerospace applications involving mechanical, electrical, hydraulic and control subsystems, process oriented applications and generation and distribution of electric power. Models in Modelica are mathematically described by differential, algebraic and discrete equations. No particular variable needs to be solved for manually. A Modelica tool will have enough information to decide that automatically. Modelica is designed such that available, specialized algorithms can be utilized to enable efficient handling of large models having more than hundred thousand equations. Modelica is suited and used for hardware-in-the- loop simulations and for embedded control systems. More information is available at http://www.Modelica.org/ ModelicaTM},
author = {Elmqvist, H and Boudaud, F and Broenink, J and Br{\"{u}}ck, D and Ernst, T and Fritzson, P and Jeandel, A},
doi = {10.1016/S0928-4869(97)84257-7},
issn = {09284869},
keywords = {Tutorial and Rationale},
title = {{Modelica — A unified object-oriented language for physical systems modeling}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0928486997842577},
year = {1999}
}

@article{Elmqvist,
author = {Elmqvist, H and Bachmann, B and Boudaud, F and Broenink, J and Br{\"{u}}ck, D and Ernst, T and Franke, R and Fritzson, P and Jeandel, A and Grozman, P and Juslin, K and K{\aa}gedahl, D and Klose, M and Loubere, N and Mattsson, S E and Mostermann, P and Nilsson, H and Otter, M and Sahlin, P and Schneider, A and Tummescheit, H and Vangheluwe, H},
title = {{Modelica- A Unified Object-Oriented Language for Physical Systems Modeling}}
}

@book{Elmasri,
author = {Elmasri, Ramez and Navathe, Shamkant B},
edition = {4th},
isbn = {0-321-12226-7},
publisher = {Addison Wesley},
title = {{Fundamentals Of Database Systems}}
}

@article{Elliott2018,
abstract = {Continuous-time stochastic processes pervade everyday experience, and the simulation of models of these processes is of great utility. Classical models of systems operating in continuous-time must typically track an unbounded amount of information about past behaviour, even for relatively simple models, enforcing limits on precision due to the finite memory of the machine. However, quantum machines can require less information about the past than even their optimal classical counterparts to simulate the future of discrete-time processes, and we demonstrate that this advantage extends to the continuous-time regime. Moreover, we show that this reduction in the memory requirement can be unboundedly large, allowing for arbitrary precision even with a finite quantum memory. We provide a systematic method for finding superior quantum constructions, and a protocol for analogue simulation of continuous-time renewal processes with a quantum machine.},
archivePrefix = {arXiv},
arxivId = {1704.04231},
author = {Elliott, Thomas J and Gu, Mile},
doi = {10.1038/s41534-018-0064-4},
eprint = {1704.04231},
issn = {20566387},
journal = {npj Quantum Information},
number = {1},
pages = {1--10},
publisher = {Springer US},
title = {{Superior memory efficiency of quantum devices for the simulation of continuous-time stochastic processes}},
url = {http://dx.doi.org/10.1038/s41534-018-0064-4},
volume = {4},
year = {2018}
}

@article{El-Masri2015,
abstract = {Recent research shows that gamification is a valuable tool to improve students' learning effectiveness. However, its application continues to be limited. Educators remain reluctant to use games due to fac-tors like limited resources, game complexity, inadaptability to various learning-outcomes, weak stu-dent involvement, and difficulty to integrate in course structures. In this article we argue that, when purposefully designed, educational games can address those factors that hinder adoption. According-ly, we identified seven design principles that, if satisfied, are expected to yield educational games that are useful to both educators and students. The principles accentuate the importance of designing edu-cational platforms over which games can be created and played. Game platforms must (1) adapt to various educational purposes, (2) enable educators control over student engagement (switching on/off game features like rewards, personifications, etc.), (3) scale up/down to achieve the desired level of complexity, and (4) maintain student arousal by dynamically balancing the challenge level with skill level. To evaluate the design principles, we intend to design a business game platform with educators that reflect the seven proposed principles and evaluate it in class settings. If fruitful, this research will advance extant knowledge on learning strategies and specifically the design of educational games.},
author = {El-Masri, Mazen and Tarhini, Ali},
isbn = {9783000502842},
journal = {Twenty-Third European Conference on Information Systems (ECIS)},
pages = {1--10},
title = {{A Design Science Approach to Gamify Education: From Games to Platforms}},
url = {http://aisel.aisnet.org/ecis2015{\_}rip/48/},
year = {2015}
}

@techreport{Eisenstein,
abstract = {Design problems involve issues of stylistic preference and flexible standards of success; human designers often proceed by intuition and are unaware of following any strict rule-based procedures. These features make design tasks especially difficult to automate. Adaptation is proposed as a means to overcome these challenges. We describe a system that applies an adaptive algorithm to automated user interface design within the framework of the MOBI-D (Model-Based Interface Designer) interface development environment. Preliminary experiments indicate that adaptation improves the performance of the automated user interface design system. Keywords Model-based interface development, machine learning, decision trees, theory refinement, user interface development tools, interface models, theory refinement INTRODUCTION},
author = {Eisenstein, Jacob and Puerta, Angel},
title = {{Adaptation in Automated User-Interface Design}}
}

@article{Egi2018b,
abstract = {Non-free data types are data types whose data have no canonical forms. For example, multisets are non-free data types because the multiset {\$}\backslashbackslash{\{}\backslash{\{}{\}}a,b,b\backslashbackslash{\{}\backslash{\}}{\}}{\$} has two other equivalent but literally different forms {\$}\backslashbackslash{\{}\backslash{\{}{\}}b,a,b\backslashbackslash{\{}\backslash{\}}{\}}{\$} and {\$}\backslashbackslash{\{}\backslash{\{}{\}}b,b,a\backslashbackslash{\{}\backslash{\}}{\}}{\$}. Pattern matching is known to provide a handy tool set to treat such data types. Although many studies on pattern matching and implementations for practical programming languages have been proposed so far, we observe that none of these studies satisfy all the criteria of practical pattern matching, which are as follows: i) efficiency of the backtracking algorithm for non-linear patterns, ii) extensibility of matching process, and iii) polymorphism in patterns. This paper aims to design a new pattern-matching-oriented programming language that satisfies all the above three criteria. The proposed language features clean Scheme-like syntax and efficient and extensible pattern matching semantics. This programming language is especially useful for the processing of complex non-free data types that not only include multisets and sets but also graphs and symbolic mathematical expressions. We discuss the importance of our criteria of practical pattern matching and how our language design naturally arises from the criteria. The proposed language has been already implemented and open-sourced as the Egison programming language.},
archivePrefix = {arXiv},
arxivId = {1808.10603},
author = {Egi, Satoshi and Nishiwaki, Yuichi},
doi = {10.1007/978-3-030-02768-1_1},
eprint = {1808.10603},
title = {{Non-linear Pattern Matching with Backtracking for Non-free Data Types}},
url = {http://arxiv.org/abs/1808.10603 http://dx.doi.org/10.1007/978-3-030-02768-1{\_}1},
year = {2018}
}

@article{Egi2018,
archivePrefix = {arXiv},
arxivId = {1809.03252},
author = {Egi, Satoshi},
eprint = {1809.03252},
title = {{Loop Patterns: Extension of Kleene Star Operator for More Expressive Pattern Matching against Arbitrary Data Structures}},
url = {https://arxiv.org/abs/1809.03252},
year = {2018}
}

@book{Eberly2005,
author = {Eberly, D E},
isbn = {1558607323},
title = {{Real-Time Collision Detection}},
year = {2005}
}

@article{Duchateau,
author = {Duchateau, P and Zachmann, D},
isbn = {981238815X},
journal = {Computing in Science Engineering},
title = {{Partial Differential Equations}}
}

@article{DuanLian;HuTao;ChengEn;ZhuJianfeng;Gao2017,
abstract = {-Crime, as a long-term global problem, has been showing the complex interactions with space, time and environments. Extracting effective features to reveal such entangled relationships to predict where and when crimes will occur, is becoming a hot topic and also a bottleneck for researchers. We, therefore, proposed a novel Spatiotemporal Crime Network (STCN), in an attempt to apply deep Convolutional Neural Networks (CNNs) for automatically crime-referenced feature extraction. This model can forecast the crime risk of each region in the urban area for the next day from the retrospective volume of high-dimension data. We evaluated the STCN using felony and 311 datasets in New York City from 2010 to 2015. The results showed STCN achieved 88{\%} and 92{\%} on F1 and AUC respectively, confirming the performances of STCN exceeded those of four baselines. Finally, the predicted results was visualized to help people understanding its linking with the ground truth.},
author = {{Duan, Lian;Hu, Tao;Cheng, En;Zhu, Jianfeng;Gao}, Chao},
isbn = {1601324634},
journal = {International Conf. Information and Knowledge Engineering},
keywords = {CNN (Convolution Neural Networks),Crime Risk Estimation,Crime Spatiotemporal Prediction,Deep Learning,Urban Computing},
pages = {61--67},
title = {{Deep Convolutional Neural Networks for Spatiotemporal Crime Prediction}},
url = {http://csce.ucmss.com/books/LFS/CSREA2017/IKE3704.pdf},
year = {2017}
}

@article{Drake1999,
abstract = {Background: Data suggest that androgenetic alopecia is a process dependent on dihydrotestosterone (DHT) and type 2 5{\$}\alpha{\$}-reductase. Finasteride is a type 2 5{\$}\alpha{\$}-reductase inhibitor that has been shown to slow further hair loss and improve hair growth in men with androgenetic alopecia. Objective: We attempted to determine the effect of finasteride on scalp skin and serum androgens. Methods: Men with androgenetic alopecia (N = 249) underwent scalp biopsies before and after receiving 0.01, 0.05, 0.2, 1, or 5 mg daily of finasteride or placebo for 42 days. Results: Scalp skin DHT levels declined significantly by 13.0{\%} with placebo and by 14.9{\%}, 61.6{\%}, 56.5{\%}, 64.1{\%}, and 69.4{\%} with 0.01, 0.05, 0.2, 1, and 5 mg doses of finasteride, respectively. Serum DHT levels declined significantly (P {\textless}.001) by 49.5{\%}, 68.6{\%}, 71.4{\%}, and 72.2{\%} in the 0.05, 0.2, 1, and 5 mg finasteride treatment groups, respectively. Conclusion: In this study, doses of finasteride as low as 0.2 mg per day maximally decreased both scab skin and serum DHT levels. These data support the rationale used to conduct clinical trials in men with male pattern hair loss at doses of finasteride between 0.2 and 5 mg.},
author = {Drake, L and Hordinsky, M and Fiedler, V and Swinehart, J and Unger, W P and Cotterill, P C and Thiboutot, D M and Lowe, N and Jacobson, C and Whiting, D and Stieglitz, S and Kraus, S J and Griffin, E I and Weiss, D and Carrington, P and Gencheff, C and Cole, G W and Pariser, D M and Epstein, E S and Tanaka, W and Dallob, A and Vandormael, K and Geissler, L and Waldstreicher, J},
doi = {10.1016/S0190-9622(99)80051-6},
issn = {01909622},
journal = {Journal of the American Academy of Dermatology},
number = {4},
pages = {550--554},
pmid = {10495374},
title = {{The effects of finasteride on scalp skin and serum androgen levels in men with androgenetic alopecia}},
volume = {41},
year = {1999}
}

@book{Downey2013,
abstract = {The premise of this book, and the other books in the Think X series, is that if you know how to program, you can use that skill to learn other topics. Most books on Bayesian statistics use mathematical notation and present ideas in terms of mathematical concepts like calculus. This book uses Python code instead of math, and discrete approximations instead of con- tinuous mathematics. As a result, what would be an integral in a math book becomes a summation, and most operations on probability distributions are simple loops.},
author = {Downey, Allen B},
edition = {Version 1.},
isbn = {1491945435},
pages = {214},
publisher = {O'Reilly Media},
title = {{Think Bayes: Bayesian Statistics in Python}},
year = {2013}
}

@article{Doets2004,
author = {Doets, Kees and van Eijck, Jan},
title = {{The Haskell Road to Logic, Maths and Programming}},
year = {2004}
}

@book{Dixon2011,
abstract = {Title from resource description page (Recorded Books, viewed November 10, 2014). Explores wide-ranging applications of modeling and simulation techniques that allow readers to conduct research and ask "What if''" Principles of Modeling and Simulation: A Multidisciplinary Approach is the first book to provide an introduction to modeling and simulation techniques across diverse areas of study. Numerous researchers from the fields of social science, engineering, computer science, and business have collaborated on this work to explore the multifaceted uses of computational modeling while illustrating their applications in common spreadsheets. The book is organized into three succinct parts: Principles of Modeling and Simulation provides a brief history of modeling and simulation, outlines its many functions, and explores the advantages and disadvantages of using models in problem solving. Two major reasons to employ modeling and simulation are illustrated through the study of a specific problem in conjunction with the use of related applications, thus gaining insight into complex concepts. Theoretical Underpinnings examines various modeling techniques and introduces readers to two significant simulation concepts: discrete event simulation and simulation of continuous systems. This section details the two primary methods in which humans interface with simulations, and it also distinguishes the meaning, importance, and significance of verification and validation. Practical Domains delves into specific topics related to transportation, business, medicine, social science, and enterprise decision support. The challenges of modeling and simulation are discussed, along with advanced applied principles of modeling and simulation such as representation techniques, integration into the application infrastructure, and emerging technologies. With its accessible style and wealth of real-world examples, Principles of Modeling and Simulation: A Multidisciplinary Approach is a valuable book for modeling and simulation courses at the upper-undergraduate and graduate levels. It is also an indispensable reference for researchers and practitioners working in statistics, mathematics, engineering, computer science, economics, and the social sciences who would like to further develop their understanding and knowledge of the field. ""Title""; ""Copyright""; ""Preface""; ""Contributors""; ""Part One: Principles of Modeling and Simulation: A Multidisciplinary Approach""; ""Chapter 1: What Is Modeling and Simulation?""; ""INTRODUCTION""; ""MODELS: APPROXIMATIONS OF REAL-WORLD EVENTS""; ""A BRIEF HISTORY OF MODELING AND SIMULATION""; ""APPLICATION AREAS""; ""USING MODELING AND SIMULATION: ADVANTAGES AND DISADVANTAGES""; ""CONCLUSION""; ""KEY TERMS""; ""FURTHER READING""; ""REFERENCES""; ""Chapter 2: The Role of Modeling and Simulation""; ""INTRODUCTION""; ""USING SIMULATIONS TO SOLVE PROBLEMS"" ""UNCERTAINTY AND ITS EFFECTS""""GAINING INSIGHT""; ""A SIMULATIONâ€™S LIFETIME""; ""CONCLUSION""; ""KEY TERMS""; ""FURTHER READING""; ""Part Two: Theoretical Underpinnings""; ""Chapter 3: Simulation: Models That Vary over Time""; ""INTRODUCTION""; ""DISCRETE EVENT SIMULATION""; ""CONTINUOUS SIMULATION""; ""CONCLUSION""; ""KEY TERMS""; ""REFERENCE""; ""Chapter 4: Queue Modeling and Simulation""; ""INTRODUCTION""; ""ANALYTICAL SOLUTION""; ""QUEUING MODELS""; ""SEQUENTIAL SIMULATION""; ""SIMPACK QUEUING IMPLEMENTATION""; ""PARALLEL SIMULATION""; ""CONCLUSION""; ""KEY TERMS"" ""FURTHER READING""""REFERENCES""; ""Chapter 5: Human Interaction with Simulations""; ""INTRODUCTION""; ""SIMULATION AND DATA DEPENDENCY""; ""VISUAL REPRESENTATION""; ""CONCLUSION""; ""KEY TERMS""; ""REFERENCES""; ""Chapter 6: Verification and Validation""; ""INTRODUCTION""; ""PERFORMING VERIFICATION AND VALIDATION""; ""VERIFICATION AND VALIDATION EXAMPLES""; ""CONCLUSION""; ""KEY TERMS""; ""REFERENCES""; ""Part Three: Practical Domains""; ""Chapter 7: Uses of Simulation""; ""INTRODUCTION""; ""THE MANY FACETS OF SIMULATION""; ""EXPERIMENTATION ASPECT OF SIMULATION"" ""EXPERIENCE ASPECT OF SIMULATION""""EXAMPLES OF USES OF SIMULATION""; ""ETHICS IN THE USE OF SIMULATION""; ""SOME EXCUSES TO AVOID USING SIMULATION""; ""CONCLUSION""; ""KEY TERMS""; ""FURTHER EXPLORATION""; ""APPENDIX A SIMULATION ASSOCIATIONS/GROUPS/RESEARCH CENTERS""; ""REFERENCES""; ""Chapter 8: Modeling and Simulation: Real-World Examples""; ""INTRODUCTION""; ""TRANSPORTATION MODELING AND SIMULATION""; ""BUSINESS MODELING AND SIMULATION""; ""MEDICAL MODELING AND SIMULATION""; ""SOCIAL SCIENCE MODELING AND SIMULATION""; ""CONCLUSION""; ""KEY TERMS""; ""FURTHER READING""; ""REFERENCES"" ""Chapter 9: The Future of Simulation""""INTRODUCTION""; ""A BRIEF ... AND SELECTIVE ... HISTORY OF SIMULATION""; ""CONVERGENT SIMULATIONS""; ""SERIOUS GAMES""; ""HUMAN-SIMULATOR INTERFACES""; ""COMPUTING TECHNOLOGY""; ""THE ROLE OF EDUCATION IN SIMULATION""; ""THE FUTURE OF SIMULATION""; ""KEY TERMS""; ""Index""},
author = {Dixon, Kenneth},
booktitle = {Modeling and Simulation in Ecotoxicology with Applications in MATLAB and Simulink},
doi = {10.1201/b11089-3},
isbn = {0471471631},
pages = {9--29},
title = {{Principles of Modeling and Simulation}},
year = {2011}
}

@article{Dietz2003,
abstract = {A novel microprocessor interface circuit is described which can alternately emit and detect light using only an LED, two digital I/O pins and a single current limiting resistor. This technique is first applied to create a smart illumination system that uses a single LED as both light source and sensor. We then present several devices that use an LED as a generic wireless serial data port. An important implication of this work is that every LED connected to a microprocessor can be thought of as a wireless two-way communication port. We present this technology as a solution to the "last centimeter problem", because it permits disparate devices to communicate with each other simply and cheaply with minimal design modification. {\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
author = {Dietz, Paul and Yerazunis, William and Leigh, Darren},
doi = {10.1007/978-3-540-39653-6_14},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {175--191},
title = {{Very low-cost sensing and communication using bidirectional LEDs}},
volume = {2864},
year = {2003}
}

@book{Diehl2020,
author = {Diehl, Stephen},
title = {{What I Wish I Knew When Learning Haskell}},
url = {https://github.com/sdiehl/wiwinwlh},
year = {2020}
}

@article{DiTano2020,
abstract = {Fasting-mimicking diets delay tumor progression and sensitize a wide range of tumors to chemotherapy, but their therapeutic potential in combination with non-cytotoxic compounds is poorly understood. Here we show that vitamin C anticancer activity is limited by the up-regulation of the stress-inducible protein heme-oxygenase-1. The fasting-mimicking diet selectivity reverses vitamin C-induced up-regulation of heme-oxygenase-1 and ferritin in KRAS-mutant cancer cells, consequently increasing reactive iron, oxygen species, and cell death; an effect further potentiated by chemotherapy. In support of a potential role of ferritin in colorectal cancer progression, an analysis of The Cancer Genome Atlas Database indicates that KRAS mutated colorectal cancer patients with low intratumor ferritin mRNA levels display longer 3- and 5-year overall survival. Collectively, our data indicate that the combination of a fasting-mimicking diet and vitamin C represents a promising low toxicity intervention to be tested in randomized clinical trials against colorectal cancer and possibly other KRAS mutated tumors.},
author = {{Di Tano}, Maira and Raucci, Franca and Vernieri, Claudio and Caffa, Irene and Buono, Roberta and Fanti, Maura and Brandhorst, Sebastian and Curigliano, Giuseppe and Nencioni, Alessio and de Braud, Filippo and Longo, Valter D.},
doi = {10.1038/s41467-020-16243-3},
issn = {2041-1723},
journal = {Nature Communications},
month = {dec},
number = {1},
pages = {2332},
title = {{Synergistic effect of fasting-mimicking diet and vitamin C against KRAS mutated cancers}},
url = {http://dx.doi.org/10.1038/s41467-020-16243-3 http://www.nature.com/articles/s41467-020-16243-3},
volume = {11},
year = {2020}
}

@article{Deville1994,
abstract = {This paper presents an overview and a survey of logic program synthesis. Logic program synthesis is interpreted here in a broad way; it is concerned with the following question: given a specification, how do we get a logic program satisfying the specification? Logic programming provides a uniquely nice and uniform framework for program synthesis since the specification, the synthesis process, and the resulting program can all be expressed in logic. Three main approaches to logic program synthesis by formal methods are described: constructive synthesis, deductive synthesis, and inductive synthesis. Related issues such as correctness and verification, as well as synthesis by informal methods, are briefly presented. Our presentation is made coherent by employing a unified framework of terminology and notation, and by using the same running example for all the approaches covered. This paper thus intends to provide an assessment of existing work and a framework for future research in logic program synthesis. {\textcopyright}1994.},
author = {Deville, Yves and Lau, Kung Kiu},
doi = {10.1016/0743-1066(94)90029-9},
issn = {07431066},
journal = {The Journal of Logic Programming},
number = {SUPPL. 1},
pages = {321--350},
title = {{Logic program synthesis}},
volume = {19-20},
year = {1994}
}

@incollection{Devai2010,
author = {D{\'{e}}vai, Gergely},
doi = {10.1007/978-3-642-17685-2_10},
pages = {354--371},
title = {{Embedding a Proof System in Haskell}},
url = {http://link.springer.com/10.1007/978-3-642-17685-2{\_}10},
year = {2010}
}

@article{Detterman2016,
abstract = {{\textless}p{\textgreater}Education has not changed from the beginning of recorded history. The problem is that focus has been on schools and teachers and not students. Here is a simple thought experiment with two conditions: 1) 50 teachers are assigned by their teaching quality to randomly composed classes of 20 students, 2) 50 classes of 20 each are composed by selecting the most able students to fill each class in order and teachers are assigned randomly to classes. In condition 1, teaching ability of each teacher and in condition 2, mean ability level of students in each class is correlated with average gain over the course of instruction. Educational gain will be best predicted by student abilities (up to {\textless}italic{\textgreater}r{\textless}/italic{\textgreater}= 0.95) and much less by teachers' skill (up to {\textless}italic{\textgreater}r{\textless}/italic{\textgreater}= 0.32). I argue that seemingly immutable education will not change until we fully understand students and particularly human intelligence. Over the last 50 years in developed countries, evidence has accumulated that only about 10{\%} of school achievement can be attributed to schools and teachers while the remaining 90{\%} is due to characteristics associated with students. Teachers account for from 1{\%} to 7{\%} of total variance at every level of education. For students, intelligence accounts for much of the 90{\%} of variance associated with learning gains. This evidence is reviewed. {\textless}/p{\textgreater}},
author = {Detterman, Douglas K},
doi = {10.1017/sjp.2016.88},
issn = {1138-7416},
journal = {The Spanish Journal of Psychology},
keywords = {education,intelligence,schooling,teachers},
pages = {E93},
publisher = {Cambridge University Press},
title = {{Education and Intelligence: Pity the Poor Teacher because Student Characteristics are more Significant than Teachers or Schools}},
url = {https://www.cambridge.org/core/product/identifier/S1138741616000883/type/journal{\_}article},
volume = {19},
year = {2016}
}

@article{Denney1998,
author = {Denney, E and Denney, E},
journal = {IFIP WORKING CONFERENCE ON PROGRAMMING CONCEPTS AND METHODS (PROCOMET '98), SHELTER ISLAND},
pages = {148--166},
title = {{Refinement Types for Specification}},
url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.4988},
volume = {125},
year = {1998}
}

@article{Denes2014a,
author = {D{\'{e}}n{\`{e}}s, Maxime and Hritcu, Catalin and Lampropoulos, Leonidas and Paraskevopoulou, Zoe and Pierce, Benjamin C},
journal = {Coq Workshop},
pages = {1--2},
title = {{{\{}QuickChick{\}}: Property-Based Testing for {\{}Coq{\}}}},
url = {http://prosecco.gforge.inria.fr/personal/hritcu/talks/coq6{\_}submission{\_}4.pdf},
year = {2014}
}

@article{Denes2014,
author = {D{\'{e}}n{\`{e}}s, Maxime and Hritcu, Catalin and Lampropoulos, Leonidas and Paraskevopoulou, Zoe and Pierce, Benjamin C},
journal = {Coq Workshop},
pages = {1--2},
title = {{QuickChick: Property-Based Testing for Coq}},
year = {2014}
}

@article{Delaware2015,
author = {Delaware, Benjamin and Delaware, Benjamin and Gross, Jason and Chlipala, Adam},
journal = {IN PROC. POPL},
title = {{Fiat: Deductive synthesis of abstract data types in a proof assistant}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.668.3066},
year = {2015}
}

@article{Deisenroth2019,
abstract = {Machine learning is the latest in a long line of attempts to distill human knowledge and reasoning into a form that is suitable for constructing ma- chines and engineering automated systems. As machine learning becomes more ubiquitous and its software packages become easier to use, it is nat- ural and desirable that the low-level technical details are abstracted away and hidden from the practitioner. However, this brings with it the danger that a practitioner becomes unaware of the design decisions and, hence, the limits of machine learning algorithms. The enthusiastic practitioner who is interested to learn more about the magic behind successful machine learning algorithms currently faces a daunting set of pre-requisite knowledge: Programming languages and data analysis tools Large-scale computation and the associated frameworks Mathematics and statistics and how machine learning builds on it},
author = {Deisenroth, Marc P and Faisal, A Aldo and Ong, C Soon},
journal = {Cambridge University Press},
month = {oct},
title = {{Mathematics for Machine Learning}},
year = {2019}
}

@article{Deary2007,
author = {Deary, Ian J and Strand, Steve and Smith, Pauline and Fernandes, Cres},
doi = {10.1016/j.intell.2006.02.001},
issn = {01602896},
journal = {Intelligence},
number = {1},
pages = {13--21},
title = {{Intelligence and educational achievement}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0160289606000171},
volume = {35},
year = {2007}
}

@incollection{DeSa2017,
author = {de S{\'{a}}, Alex G C and Pinto, Walter Jos{\'{e}} G S and Oliveira, Luiz Otavio V B and Pappa, Gisele L},
doi = {10.1007/978-3-319-55696-3_16},
pages = {246--261},
publisher = {Springer, Cham},
title = {{RECIPE: A Grammar-Based Framework for Automatically Evolving Classification Pipelines}},
url = {http://link.springer.com/10.1007/978-3-319-55696-3{\_}16},
year = {2017}
}

@article{DeCabo2019,
author = {{De Cabo}, Rafael and Mattson, Mark P},
doi = {10.1056/NEJMra1905136},
issn = {15334406},
journal = {New England Journal of Medicine},
number = {26},
pages = {2541--2551},
pmid = {31881139},
title = {{Effects of intermittent fasting on health, aging, and disease}},
volume = {381},
year = {2019}
}

@book{Davidson,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Davidson, Neil},
booktitle = {Journal of Chemical Information and Modeling},
eprint = {arXiv:1011.1669v3},
isbn = {9780957179127},
issn = {1098-6596},
pmid = {25246403},
title = {{Don't Just Roll the Dice = Software Pricing}},
volume = {53}
}

@book{DavidBeazley2013,
abstract = {If you need help writing programs in Python 3, or want to update older Python 2 code, this book is just the ticket. Packed with practical recipes written and tested with Python 3.3, this unique cookbook is for experienced Python programmers who want to focus on modern tools and idioms. Inside, you'll find complete recipes for more than a dozen topics, covering the core Python language as well as tasks common to a wide variety of application domains. Each recipe contains code samples you can use in your projects right away, along with a discussion about how and why the solution works.},
author = {{David Beazley} and Jones, Brian K},
isbn = {9781449340377},
keywords = {Data Structures and AlgorithmsStrings and TextNumb,Dates,Debugging,and ExceptionsC Extensions,and TimesIterators and GeneratorsFiles and I/OData},
mendeley-tags = {Data Structures and AlgorithmsStrings and TextNumb,Dates,Debugging,and ExceptionsC Extensions,and TimesIterators and GeneratorsFiles and I/OData},
pages = {706},
publisher = {O'Reilly Media},
title = {{Python Cookbook: 3rd Edition}},
year = {2013}
}

@book{Daschner2017,
abstract = {Find out how to craft effective, business-oriented Java EE 8 applications that target customer's demands in the age of Cloud platforms and container technology. About This Book Understand the principles of modern Java EE and how to realize effective architectures Gain knowledge of how to design enterprise software in the age of automation, Continuous Delivery and Cloud platforms Learn about the reasoning and motivations behind state-of-the-art enterprise Java technology, that focuses on business Who This Book Is For This book is for experienced Java EE developers who are aspiring to become the architects of enterprise-grade applications, or software architects who would like to leverage Java EE to create effective blueprints of applications. What You Will Learn What enterprise software engineers should focus on Implement applications, packages, and components in a modern way Design and structure application architectures Discover how to realize technical and cross-cutting aspects Get to grips with containers and container orchestration technology Realize zero-dependency, 12-factor, and Cloud-native applications Implement automated, fast, reliable, and maintainable software tests Discover distributed system architectures and their requirements In Detail Java EE 8 brings with it a load of features, mainly targeting newer architectures such as microservices, modernized security APIs, and cloud deployments. This book will teach you to design and develop modern, business-oriented applications using Java EE 8. It shows how to structure systems and applications, and how design patterns and Domain Driven Design aspects are realized in the age of Java EE 8. You will learn about the concepts and principles behind Java EE applications, and how to effect communication, persistence, technical and cross-cutting concerns, and asynchronous behavior. This book covers Continuous Delivery, DevOps, infrastructure-as-code, containers, container orchestration technologies, such as Docker and Kubernetes, and why and especially how Java EE fits into this world. It also covers the requirements behind containerized, zero-dependency applications and how modern Java EE application servers support these approaches. You will also learn about automated, fast, and reliable software tests, in different test levels, scopes, and test technologies. This book covers the prerequisites and challenges of distributed systems that lead to microservice, shared-nothing architectures. The challenges and solutions of consistency versus scalability will further lead us to event sourcing, event-driven architectures, and the CQRS principle. This book also includes the nuts and bolts of application performance as well as how to realize resilience, logging, monitoring and tracing in a modern enterprise world. Last but not least the demands of securing enterprise systems are covered. By the end, you will understand the ins and outs of Java EE so that you can make critical design decisions that not only live up to, but also surpass your clients' expectations. Style and approach This book focuses on solving business problems and meeting customer demands in the enterprise world. It covers how to create enterprise applications with reasonable technology choices, free of cargo-cult and over-engineering. The aspects shown in this book not only demonstrate how to realize a certain solution, but also explain its motivations and reasoning.},
author = {Daschner, Sebastian},
isbn = {1788393856},
pages = {442},
publisher = {Packt Publishing},
title = {{Architecting modern Java EE applications}},
year = {2017}
}

@article{Danvy2001,
author = {Danvy, Olivier and Nielsen, Lasse R},
issn = {0909-0878},
title = {{Defunctionalization at work}},
year = {2001}
}

@article{Dandekar2017,
author = {Dandekar, Ashish and Zen, Remmy A M},
keywords = {2019,acm reference format,and st{\'{e}}phane bressan,ashish dandekar,compar-,data privacy,m,remmy a,synthetic data generation,utility,zen},
title = {{Comparative Evaluation of Synthetic Dataset Generation Methods Open Data vs Data Privacy}},
year = {2017}
}

@article{Damiand2003,
abstract = {Split-and-merge algorithms define a class of image segmentation methods. Topological maps are a mathematical model that represents image subdivisions in 2D and 3D. This paper discusses a split-and-merge method for 3D image data based on the topological map model. This model allows representations of states of segmentations and of merge and split operations. Indeed, it can be used as data structure for dynamic changes of segmentation. The paper details such an algorithmic approach and analyzes its time complexity. A general introduction into combinatorial and topological maps is given to support the understanding of the proposed algorithms.},
author = {Damiand, Guillaume and Resch, Patrick},
doi = {10.1016/S1524-0703(03)00009-2},
issn = {15240703},
journal = {Graphical Models},
keywords = {Image processing,Image segmentation,Split-and-merge,Topological maps},
number = {1-3},
pages = {149--167},
title = {{Split-and-merge algorithms defined on topological maps for 3D image segmentation}},
volume = {65},
year = {2003}
}

@article{Czajka2018,
abstract = {Hammers provide most powerful general purpose automation for proof assistants based on HOL and set theory today. Despite the gaining popularity of the more advanced versions of type theory, such as those based on the Calculus of Inductive Constructions, the construction of hammers for such foundations has been hindered so far by the lack of translation and reconstruction components. In this paper, we present an architecture of a full hammer for dependent type theory together with its implementation for the Coq proof assistant. A key component of the hammer is a proposed translation from the Calculus of Inductive Constructions, with certain extensions introduced by Coq, to untyped first-order logic. The translation is " sufficiently " sound and complete to be of practical use for automated theorem provers. We also introduce a proof reconstruction mechanism based on an eauto-type algorithm combined with limited rewriting, congruence closure and some forward reasoning. The algorithm is able to re-prove in the Coq logic most of the theorems established by the ATPs. Together with machine-learning based selection of relevant premises this consti-tutes a full hammer system. The performance of the whole procedure is evaluated in a bootstrapping scenario emulating the development of the Coq standard library. For each theorem in the library only the previous theorems and proofs can be used. We show that 40.8{\%} of the theorems can be proved in a push-button mode in about 40 seconds of real time on a 8-CPU system.},
author = {Czajka, {\L}ukasz and Kaliszyk, Cezary},
doi = {10.1007/s10817-018-9458-4},
issn = {15730670},
journal = {Journal of Automated Reasoning},
keywords = {Calculus of inductive constructions,Coq,Hammer,Proof automation},
number = {1-4},
pages = {423--453},
publisher = {Springer Netherlands},
title = {{Hammer for Coq: Automation for Dependent Type Theory}},
url = {https://doi.org/10.1007/s10817-018-9458-4},
volume = {61},
year = {2018}
}

@article{Czajka,
author = {Czajka, {\L}ukasz and Kaliszyk, Cezary},
title = {{Hammer for Coq}}
}

@inproceedings{Cusumano-Towner2019,
address = {New York, New York, USA},
author = {Cusumano-Towner, Marco F and Saad, Feras A and Lew, Alexander K and Mansinghka, Vikash K},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation - PLDI 2019},
doi = {10.1145/3314221.3314642},
isbn = {9781450367127},
keywords = {Markov chain Monte Carlo,Probabilistic programming,sequential Monte Carlo,variational inference},
pages = {221--236},
publisher = {ACM Press},
title = {{Gen: a general-purpose probabilistic programming system with programmable inference}},
url = {http://dl.acm.org/citation.cfm?doid=3314221.3314642},
year = {2019}
}

@article{Curtis,
author = {Curtis, Sharon and Lowe, Gavin},
title = {{A Graphical Calculus}}
}

@book{Csikszentmihalyi2009,
abstract = {Legendary psychologist Mihaly Csikszentmihalyi's famous investigations of "optimal experience" have revealed that what makes an experience genuinely satisfying is a state of consciousness called flow. During flow, people typically experience deep enjoyment, creativity, and a total involvement with life. In this new edition of his groundbreaking classic work, Csikszentmihalyi ("the leading researcher into ‘flow states'" —Newsweek) demonstrates the ways this positive state can be controlled, not just left to chance. Flow: The Psychology of Optimal Experience teaches how, by ordering the information that enters our consciousness, we can discover true happiness, unlock our potential, and greatly improve the quality of our lives. "Explores a happy state of mind called flow, the feeling of complete engagement in a creative or playful activity." —Time},
author = {Csikszentmihalyi, Mihaly},
isbn = {0061876720},
pages = {336},
publisher = {Harper Collins},
title = {{Flow: The Psychology of Optimal Experience}},
year = {2009}
}

@book{Csikszentmihalyi2008,
abstract = {THIS BOOK SUMMARIZES, for a general audience, decades of research on the positive aspects of human experience—joy, creativity, the process of total involvement with life I call flow. To take this step is somewhat dangerous, because as soon as one strays from the stylized constraints of academic prose, it is easy to become careless or overly enthusiastic about such a topic. What follows, however, is not a popular book that gives insider tips about how to be happy. To do so would be impossible in any case, since a joyful life is an individual creation that cannot be copied from a recipe. This book tries instead to present general principles, along with concrete examples of how some people have used these principles, to transform boring and meaningless lives into ones full of enjoyment. There is no promise of easy short-cuts in these pages. But for readers who care about such things, there should be enough information to make possible the transition from theory to practice.},
author = {Csikszentmihalyi, Mihaly},
isbn = {978-0-06-154812-3},
pages = {314},
title = {{Flow - The Psychology of Optimal Experience}},
year = {2008}
}

@article{Courtois2002,
abstract = {Several recently proposed ciphers are built with layers of small S-boxes, interconnected by linear key-dependent layers. Their security relies on the fact, that the classical methods of cryptanalysis (e.g. linear or differential attacks) are based on probabilistic characteristics, which makes their security grow exponentially with the number of rounds Nr. In this paper we study the security of such ciphers under an additional hypothesis: the S-box can be described by an overdefined system of algebraic equations (true with probability 1).We show that this hypothesis is true for both Serpent (due to a small size of S-boxes) and Rijndael (due to unexpected algebraic properties). We study general methods known for solving overdefined systems of equations, such as XL from Euro- crypt'00, and show their inefficiency. Then we introduce a new method called XSL that uses the sparsity of the equations and their specific structure. The XSL attack has a parameter P, and in theory we show that P should be a constant. The XSL attack would then be polynomial in Nr, with a huge constant that is double- exponential in the size of the S-box. We demonstrated by computer simulations that the XSL attack works well enough on a toy cipher. It seems however that P will rather increase very slowly with Nr. More simulations are needed for bigger ciphers. Our optimistic evaluation shows that the XSL attack might be able to break Rijndael 256 bits and Serpent for key lengths 192 and 256 bits. However if only P is increased by 2 (respectively 4) the XSL attack on Rijndael (respectively Serpent) would become slower than the exhaustive search. At any rate, it seems that the security of these ciphers does not grow exponentially with the number of rounds.},
author = {Courtois, Nicolas T and Pieprzyk, Josef},
doi = {10.1007/3-540-36178-2_17},
isbn = {3540001719},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {AES,Block ciphers,Camellia,Gr¨obner bases,MQ problem,Multivariate cryptanalysis,Multivariate quadratic equations,Overdefined systems of multivariate equations,Rijndael,Serpent,Sparse multivariate polynomials,Square,XL algorithm},
number = {256},
pages = {267--287},
title = {{Cryptanalysis of block ciphers with overdefined systems of equations}},
volume = {2501},
year = {2002}
}

@article{Costa1997,
author = {Costa, Carlos A Bana E and Vansnick, Jean-claude},
keywords = {applications,cardinal value functions,multicriteria analysis,scaling constants},
number = {July 1995},
pages = {107--114},
title = {{Applications of the MACBETH approach in the Framework of an additive aggregation model. Journal of Multi-Criteria Decision Analysis, 107-114.}},
volume = {6},
year = {1997}
}

@book{Corral2009,
abstract = {This book covers elementary trigonometry. It is suitable for a one-semester course at the college level, though it could also be used in high schools. The prerequisites are high school algebra and geometry.},
author = {Corral, Michael},
isbn = {1475074573},
pages = {180},
title = {{Trigonometry}},
year = {2009}
}

@inproceedings{Conforti1992,
abstract = {Several problems of propositional logic, such as satisfiability, MAXSAT and logical inference, can be formulated as integer programs. The authors consider sets of clauses for which these integer programs can be solved as linear programs. They prove that balanced sets of clauses have this property.},
author = {Conforti, Michele and Cornuejols, Gerard},
booktitle = {Proceedings., 33rd Annual Symposium on Foundations of Computer Science},
doi = {10.1109/SFCS.1992.267784},
isbn = {0-8186-2900-2},
issn = {02725428},
pages = {670--675},
publisher = {IEEE},
title = {{A Class of Logic Problems Solvable by Linear Programming}},
url = {http://ieeexplore.ieee.org/document/267784/},
volume = {1992-Octob},
year = {1992}
}

@book{Cometti2000a,
abstract = {Partiendo de una afirmaci{\'{o}}n com{\'{u}}n es las ciencias (humanas y biol{\'{o}}gicas), el individuo tiene una estructura que pone en juego y moviliza la energ{\'{i}}a. Esta estructura est{\'{a}} constituida por las palancas, las articulaciones y los m{\'{u}}sculos; pero s{\'{o}}lo los m{\'{u}}sculos son los elementos sobre los cuales puede actuar directamente el entrenamiento. Cuando el m{\'{u}}sculo funciona produce la fuerza, que depende de su estiramiento. El libro, Los m{\'{e}}todos modernos de musculaci{\'{o}}n, se presenta divido en dos partes. La primera, que incluye las bases te{\'{o}}ricas, trata: Los mecanismos de la fuerza -factores estructurales y nerviosos-; Los m{\'{e}}todos de desarrollo de la fuerza -m{\'{e}}todos de fuerza m{\'{a}}xima, m{\'{e}}todo por repeticiones, m{\'{e}}todo din{\'{a}}mico, m{\'{e}}todo de la pir{\'{a}}mide-; Los reg{\'{i}}menes de contracci{\'{o}}n -reg{\'{i}}menes isom{\'{e}}tricos, anisom{\'{e}}tricos, exc{\'{e}}ntrico y pliom{\'{e}}trico-. La segunda parte, que incluye los datos pr{\'{a}}cticos, trata: Los m{\'{e}}todos de desarrollo de la masa muscular -los m{\'{e}}todos post-fatiga, la planificaci{\'{o}}n de los m{\'{e}}todos-; Los m{\'{e}}todos conc{\'{e}}ntricos -la l{\'{o}}gica de Zatsiorski, el m{\'{e}}todo B{\'{u}}lgaro, el principio de carga ascendente y descendente-; Los m{\'{e}}todos isom{\'{e}}tricos -las particularidades de m{\'{e}}todo isom{\'{e}}trico, planificaci{\'{o}}n del m{\'{e}}todo isom{\'{e}}trico-; Los m{\'{e}}todos exc{\'{e}}ntricos -principio de contraste, la prefatiga-; Los m{\'{e}}todos pliom{\'{e}}tricos -particularidades y planificaci{\'{o}}n-; La electroestimulaci{\'{o}}n -el trabajo por electroestimulaci{\'{o}}n, efecto inmediato, efecto retardado-; Los m{\'{e}}todos combinados; Ejemplos concretos aplicados a diferentes deportes. Todo el libro est{\'{a}} claramente ilustrado por medio de numerosas tablas, diagramas y gr{\'{a}}ficos.},
author = {Cometti, Gilles},
isbn = {8499108563},
pages = {294},
publisher = {Paidotribo},
title = {{Los M{\'{e}}todos Modernos de Musculaci{\'{o}}n}},
url = {http://ir.obihiro.ac.jp/dspace/handle/10322/3933},
year = {2000}
}

@article{Codd,
author = {Codd, Edgar Frank},
title = {{Normalized Database Structure: A Brief Tutorial}}
}

@book{Codd1990,
abstract = {From the Preface (See Front Matter for full Preface) An important adjunct to precision is a sound theoretical foundation. The relational model is solidly based on two parts of mathematics: firstorder predicate logic and the theory of relations. This book, however, does not dwell on the theoretical foundations, but rather on all the features of the relational model that I now perceive as important for database users, and therefore for DBMS vendors. My perceptions result from 20 years of practical experience in computing and data processing (chiefly, but not exclusively, with large-scale customers of IBM), followed by another 20 years of research. I believe that this is the first book to deal exclusively with the relational approach. It does, however, include design principles in Chapters 21 and 22. It is also the first book on the relational model by the originator of that model. All the ideas in the relational model described in this book are mine, except in cases where I explicitly credit someone else. In developing the relational model, I have tried to follow Einstein's advice, "Make it as simple as possible, but no simpler." I believe that in the last clause he was discouraging the pursuit of simplicity to the extent of distorting reality. So why does the book contain 30 chapters and two appendixes? To answer this question, it is necessary to look at the history of research and development of the relational model.},
author = {Codd, Edgar Frank},
booktitle = {Database},
isbn = {0201141922},
pages = {538},
pmid = {1917145},
title = {{The Relational Model for Database Management : Version 2}},
url = {http://books.google.co.kr/books?q=9780201141924},
year = {1990}
}

@article{Codd1970,
abstract = {for Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation). A prompting service which supplies such information is not a satisfactory solution. Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed. Changes in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information. Existing noninferential, formatted data systems provide users with tree-structured files or slightly more general network models of the data. In Section 1, inadequacies of these models are discussed. A model based on n-ary relations, a normal form for data base relations, and the concept of a universal data sublanguage are introduced. In Section 2, certain opera-tions on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model. KEY WORDS AND PHRASES: data bank, data base, data structure, data organization, hierarchies of data, networks of data, relations, derivability, redundancy, consistency, composition, join, retrieval language, predicate calculus, security, data integrity CR CATEGORIES: 3.70, 3.73, 3.75, 4.20, 4.22, 4.29 1. R e l a t i o n a l Model and N o r m a l F o r m},
author = {Codd, E F},
journal = {Communications of the ACM},
keywords = {and phrases,calculus,composition,consistency,data,data bank,data base,data integrity,data structure,derivability,hierarchies of data,join,networks of data,organization,predicate,redundancy,relations,retrieval language,security},
number = {6},
pages = {377--387},
title = {{Information Retrieval A Relational Model of Data Large Shared Data Banks}},
url = {https://cs.uwaterloo.ca/{\%}7B{~}{\%}7Ddavid/cs848s14/codd-relational.pdf},
volume = {13},
year = {1970}
}

@incollection{Clarke1996a,
abstract = {Functions that map vectors with binary values into the inte- gers are important for the design and verification of arithmetic circuits. We demonstrate how multi-terminal binary decision diagrams (MTBDDs) can be used to represent such functions concisely. The Walsh transform and Reed-Muller transform have numerous applications in computer-aided design, but the usefulness of these techniques in practice has been limited by the size of the binary valued functions that can be transformed. We show how to compute the MTBDD representations of the Walsh transform and Reed-Muller transform for functions with several hundred variables. Bryant and Chen have proposed binary moment diagrams (BMDs) for rep- resenting the class of functions that we have considered. We discuss the relationship between these methods and describe a generalization called hybrid decision diagrams which is often much more concise.},
address = {Boston, MA},
author = {Clarke, Edmund M and Fujita, Masahiro and Zhao, Xudong},
booktitle = {Representations of Discrete Functions},
doi = {10.1007/978-1-4613-1385-4_4},
pages = {93--108},
publisher = {Springer US},
title = {{Multi-Terminal Binary Decision Diagrams and Hybrid Decision Diagrams}},
url = {http://link.springer.com/10.1007/978-1-4613-1385-4{\_}4},
year = {1996}
}

@article{Clarke1996,
author = {Clarke, E and Fujita, M and Zhao, X},
pages = {93--108},
title = {{Multi-Terminal Binary Decision Diagrams and Hybrid Decision Diagrams, Representations of Discrete Functions}},
year = {1996}
}

@article{Clark2004,
abstract = {Dihydrotestosterone (DHT) is the primary metabolite of testosterone in the prostate and skin. Testosterone is converted to DHT by 5{\$}\alpha{\$}-reductase, which exists in two isoenzyme forms (types 1 and 2). DHT is associated with development of benign prostatic hyperplasia (BPH), and reduction in its level with 5{\$}\alpha{\$}-reductase inhibitors improves the symptoms associated with BPH and reduces the risk of acute urinary retention and prostate surgery. A selective inhibitor of the type 2 isoenzyme (finasteride) has been shown to decrease serum DHT by about 70{\%}. We hypothesized that inhibition of both isoenzymes with the dual inhibitor dutasteride would more effectively suppress serum DHT levels than selective inhibition of only the type 2 isoenzyme. A total of 399 patients with BPH were randomized to receive once-daily dosing for 24 wk of dutasteride (0.01, 0.05, 0.5, 2.5, or 5.0 mg), 5 mg finasteride, or placebo. The mean percent decrease in DHT was 98.4 ± 1.2{\%} with 5.0 mg dutasteride and 94.7 ± 3.3{\%} with 0.5 mg dutasteride, significantly lower (P {\textless}0.001) and with less variability than the 70.8 ± 18.3{\%} suppression observed with 5 mg finasteride. Mean testosterone levels increased but remained in the normal range for all treatment groups. Dutasteride appeared to be well tolerated with an adverse event profile similar to placebo.},
author = {Clark, Richard V and Hermann, David J and Cunningham, Glenn R and Wilson, Timothy H and Morrill, Betsy B and Hobbs, Stuart},
doi = {10.1210/jc.2003-030330},
issn = {0021972X},
journal = {Journal of Clinical Endocrinology and Metabolism},
number = {5},
pages = {2179--2184},
title = {{Marked Suppression of Dihydrotestosterone in Men with Benign Prostatic Hyperplasia by Dutasteride, a Dual 5{\$}\alpha{\$}-Reductase Inhibitor}},
volume = {89},
year = {2004}
}

@book{Ciliendo2007,
abstract = {Over the past few years, Linux has made its way into the data centers of many corporations worldwide. The Linux operating system is accepted by both the scientific and enterprise user population. Today, Linux is by far the most versatile operating system. You can find Linux on embedded devices such as firewalls, cell phones, and mainframes. Naturally, performance of the Linux operating system has become a hot topic for scientific and enterprise users. However, calculating a global weather forecast and hosting a database impose different requirements on an operating system. Linux must accommodate all possible usage scenarios with optimal performance. Most Linux distributions contain general tuning parameters to accommodate all users.},
author = {Ciliendo, Eduardo and Kunimasa, Takechika},
keywords = {IBM Redbooks eServer xSeries z/OS AIX DB2 DS8000 P},
pages = {168},
publisher = {IBM Redbooks},
title = {{Linux Performance and Tuning Guidelines}},
year = {2007}
}

@article{Chuang1999,
abstract = {An algorithm is presented to generate a piecewise curve with G1 continuity using arcs, which approximates a B-spline curve without crossing the curve. Initially, the B-spline curve in database is decomposed into piecewise Bezier curves. Using convex hulls of the Bezier curves to protect the original curve from interference, the line segments are chosen all on the same side of the B-spline curve to give a one-sided approximating curve. Based on the obtained approximating line segments, biarc fitting and single arc fitting methods are applied to construct a smooth, G1 continuous curve which does not cross the original curve. If the resulting curve is offset to generate tool paths for pocketing boundaries with B-spline curves, the over-cutting problem can be eliminated completely, and abrupt direction changes on tool paths can be greatly reduced. This method can also be applied to generate collision free paths for robot arms.},
author = {Chuang, S H.Frank and Kao, C Z},
doi = {10.1016/S0010-4485(99)00019-6},
issn = {00104485},
journal = {CAD Computer Aided Design},
keywords = {b-spline curve,biarcs,cutter-path generation,fitting,interference,offsetting},
number = {2},
pages = {111--118},
title = {{One-sided arc approximation of B-spline curves for interference-free offsetting}},
volume = {31},
year = {1999}
}

@article{Chou,
abstract = {This paper presents a method of producing readable proofs for theorems in solid geometry. The method is for a class of constructive geometry statements about straight lines, planes, circles, and spheres. The key idea of the method is to eliminate points from the conclusion of a geometric statement using several (fixed) high level basic propositions about the signed volumes of tetrahedrons and Pythagoras differences of triangles. We have implemented the algorithm and more than 80 examples from solid geometry have been used to test the program. Our program is efficient and the proofs produced by it are generally short and readable. Keywords.},
author = {Chou, S.-C. and Gao, X.-S. and Zhang, J.-Z.},
title = {{Automated Production of Traditional Proofs in Solid Geometry}}
}

@article{Chlipala,
author = {Chlipala, A},
title = {{An Introduction to Programming and Proving with Dependent Types in Coq}}
}

@article{Chen2018,
abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
archivePrefix = {arXiv},
arxivId = {1806.07366},
author = {Chen, Ricky T Q and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
eprint = {1806.07366},
number = {Nips},
pages = {1--18},
title = {{Neural Ordinary Differential Equations}},
url = {http://arxiv.org/abs/1806.07366},
year = {2018}
}

@article{Chen2017,
abstract = {The automation of tasks in community question answering (cQA) is dominated by machine learning approaches, whose performance is often limited by the number of training examples. Starting from a neural sequence learning approach with attention, we explore the impact of two data augmentation techniques on question ranking performance: a method that swaps reference questions with their paraphrases, and training on examples automatically selected from external datasets. Both methods are shown to lead to substantial gains in accuracy over a strong baseline. Further improvements are obtained by changing the model architecture to mirror the structure seen in the data.},
author = {Chen, Charles and Bunescu, Razvan},
journal = {Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP)},
pages = {442--447},
title = {{An Exploration of Data Augmentation and RNN Architectures for Question Ranking in Community Question Answering}},
year = {2017}
}

@article{Chen2019,
abstract = {Deep Learning (DL) algorithms are the central focus of modern machine learning systems. As data volumes keep growing, it has become customary to train large neural networks with hundreds of millions of parameters to maintain enough capacity to memorize these volumes and obtain state-of-the-art accuracy. To get around the costly computations associated with large models and data, the community is increasingly investing in specialized hardware for model training. However, specialized hardware is expensive and hard to generalize to a multitude of tasks. The progress on the algorithmic front has failed to demonstrate a direct advantage over powerful hardware such as NVIDIA-V100 GPUs. This paper provides an exception. We propose SLIDE (Sub-LInear Deep learning Engine) that uniquely blends smart randomized algorithms, with multi-core parallelism and workload optimization. Using just a CPU, SLIDE drastically reduces the computations during both training and inference outperforming an optimized implementation of Tensorflow (TF) on the best available GPU. Our evaluations on industry-scale recommendation datasets, with large fully connected architectures, show that training with SLIDE on a 44 core CPU is more than 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained using TF on Tesla V100 at any given accuracy level. On the same CPU hardware, SLIDE is over 10x faster than TF. We provide codes and scripts for reproducibility.},
archivePrefix = {arXiv},
arxivId = {1903.03129},
author = {Chen, Beidi and Medini, Tharun and Farwell, James and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
eprint = {1903.03129},
month = {mar},
title = {{SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems}},
url = {http://arxiv.org/abs/1903.03129},
year = {2019}
}

@book{Chege,
author = {Chege, Paul},
pages = {124},
title = {{Probability and Statistics}}
}

@article{Chawla2011,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, N V and Bowyer, K W and Hall, L O and Kegelmeyer, W P},
doi = {10.1613/jair.953},
eprint = {1106.1813},
journal = {Journal Of Artificial Intelligence Research},
month = {jun},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
url = {http://arxiv.org/abs/1106.1813 http://dx.doi.org/10.1613/jair.953},
year = {2011}
}

@article{Chard2019,
abstract = {Growing data volumes and velocities are driving exciting new methods across the sciences in which data analytics and machine learning are increasingly intertwined with research. These new methods require new approaches for scientific computing in which computation is mobile, so that, for example, it can occur near data, be triggered by events (e.g., arrival of new data), or be offloaded to specialized accelerators. They also require new design approaches in which monolithic applications can be decomposed into smaller components, that may in turn be executed separately and on the most efficient resources. To address these needs we propose funcX---a high-performance function-as-a-service (FaaS) platform that enables intuitive, flexible, efficient, scalable, and performant remote function execution on existing infrastructure including clouds, clusters, and supercomputers. It allows users to register and then execute Python functions without regard for the physical resource location, scheduler architecture, or virtualization technology on which the function is executed---an approach we refer to as "serverless supercomputing." We motivate the need for funcX in science, describe our prototype implementation, and demonstrate, via experiments on two supercomputers, that funcX can process millions of functions across more than 65000 concurrent workers. We also outline five scientific scenarios in which funcX has been deployed and highlight the benefits of funcX in these scenarios.},
archivePrefix = {arXiv},
arxivId = {1908.04907},
author = {Chard, Ryan and Skluzacek, Tyler J and Li, Zhuozhao and Babuji, Yadu and Woodard, Anna and Blaiszik, Ben and Tuecke, Steven and Foster, Ian and Chard, Kyle},
eprint = {1908.04907},
title = {{Serverless Supercomputing: High Performance Function as a Service for Science}},
url = {http://arxiv.org/abs/1908.04907},
year = {2019}
}

@article{Carroll2001,
abstract = {General relativity (GR) is the most beautiful physical theory ever invented. Nevertheless, it has a reputation of being extremely difficult, primarily for two reasons: tensors are ev- erywhere, and spacetime is curved. These two facts force GR people to use a different language than everyone else, which makes the theory somewhat inaccessible. Nevertheless, it is possible to grasp the basics of the theory, even if you're not Einstein (and who is?).},
author = {Carroll, Sean M},
journal = {Online},
pages = {24},
title = {{A No-Nonsense Introduction to General Relativity}},
url = {http://fizisist.web.cern.ch/fizisist/academia/A{\_}No-Nonsens{\_} Introduction{\_}to General{\_}Relativity.pdf},
year = {2001}
}

@article{Carneiro2019,
abstract = {As the usage of theorem prover technology expands, so too does the reliance on correctness of the tools. Metamath Zero is a verification system that aims for simplicity of logic and implementation, without compromising on efficiency of verification. It is formally specified in its own language, and supports a number of translations to and from other proof languages. This paper describes the abstract logic of Metamath Zero, essentially a multi-sorted first order logic, as well as the binary proof format and the way in which it can ensure essentially linear time verification while still being concise and efficient at scale. Metamath Zero currently holds the record for fastest verification of the {\$}\backslashbackslashmathtt{\{}\backslash{\{}{\}}set.mm{\{}\backslash{\}}{\}}{\$} Metamath library of proofs in ZFC (including 71 of Wiedijk's 100 formalization targets), at less than 200 ms. Ultimately, we intend to use it to verify the correctness of the implementation of the verifier down to binary executable, so it can be used as a root of trust for more complex proof systems.},
archivePrefix = {arXiv},
arxivId = {1910.10703},
author = {Carneiro, Mario},
eprint = {1910.10703},
month = {oct},
title = {{Metamath Zero: The Cartesian Theorem Prover}},
url = {http://arxiv.org/abs/1910.10703},
year = {2019}
}

@article{Capel2000,
abstract = {The objective of this work is the super-resolution enhancement of image sequences. We consider in particular images of scenes for which the point-to-point image transformation is a plane projective transformation. We first describe the imaging model, and a maximum likelihood (ML) estimator of the super-resolution image. We demonstrate the extreme noise sensitivity of the unconstrained ML estimator. We show that the Irani and Peleg [9, 10] super-resolution algorithm does not suffer from this sensitivity, and explain that this stability is due to the error back-projection method which effectively constrains the solution. We then propose two estimators suitable for the enhancement of text images: a maximum a posterior (MAP) estimator based on a Huber prior, and an estimator regularized using the Total Variation norm. We demonstrate the improved noise robustness of these approaches over the Irani and Peleg estimator. We also show the effects of a poorly estimated point spread function (PSF) on the super-resolution result and explain conditions necessary for this parameter to be included in the optimization. Results are evaluated on both real and synthetic sequences of text images. In the case of the real images, the projective transformations relating the images are estimated automatically from the image data, so that the entire algorithm is automatic. {\textcopyright}2000 IEEE.},
author = {Capel, David and Zisserman, Andrew},
doi = {10.1109/icpr.2000.905409},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
number = {1},
pages = {600--605},
title = {{Super-resolution enhancement of text image sequences}},
volume = {15},
year = {2000}
}

@misc{Candes2010,
author = {Cand{\`{e}}s, Emmanuel},
pages = {49},
title = {{Compressive Sensing – A 25 Minute Tour}},
year = {2010}
}

@techreport{Calder,
abstract = {This paper reports on a research project that examined the beliefs and attitudes of reluctant 16 to 18-year-old learners when using apps in their numeracy and literacy programmes. In particular, it considers the students' change of attitude towards numeracy learning. The data were consistent that the use of apps in the numeracy programme was instrumental in transforming student attitudes towards numeracy.},
author = {Calder, Campbell and Calder, Nigel and Campbell, Anthony},
title = {{"You play on them. They're active." Enhancing the mathematics learning of reluctant teenage students}}
}

@article{Cagli2017a,
abstract = {{\textcopyright}International Association for Cryptologic Research 2017. In the context of the security evaluation of cryptographic implementations, profiling attacks (aka Template Attacks) play a fundamental role. Nowadays the most popular Template Attack strategy consists in approximating the information leakages by Gaussian distributions. Nevertheless this approach suffers from the difficulty to deal with both the traces misalignment and the high dimensionality of the data. This forces the attacker to perform critical preprocessing phases, such as the selection of the points of interest and the realignment of measurements. Some software and hardware countermeasures have been conceived exactly to create such a misalignment. In this paper we propose an end-to-end profiling attack strategy based on the Convolutional Neural Networks: this strategy greatly facilitates the attack roadmap, since it does not require a previous trace realignment nor a precise selection of points of interest. To significantly increase the performances of the CNN, we moreover propose to equip it with the data augmentation technique that is classical in other applications of Machine Learning. As a validation, we present several experiments against traces misaligned by different kinds of countermeasures, including the augmentation of the clock jitter effect in a secure hardware implementation over a modern chip. The excellent results achieved in these experiments prove that Convolutional Neural Networks approach combined with data augmentation gives a very efficient alternative to the state-of-the-art profiling attacks.},
author = {Cagli, Eleonora and Dumas, C{\'{e}}cile and Prouff, Emmanuel},
doi = {10.1007/978-3-319-66787-4_3},
isbn = {9783319667867},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural networks,Data augmentation,Jitter,Machine learning,Side-channel attacks,Trace misalignment,Unstable clock},
pages = {45--68},
title = {{Convolutional Neural Networks with Data Augmentation Against Jitter-Based Countermeasures}},
url = {http://link.springer.com/10.1007/978-3-319-66787-4{\_}3},
volume = {10529 LNCS},
year = {2017}
}

@article{Buot2005,
author = {Buot, Max and Richards, Donald},
number = {June},
title = {{Homotopy Continuation Computational Algorithms}},
year = {2005}
}

@article{Buisson2016,
abstract = {Software systems have to face evolutions of their running context and users. Therefore, the so-called dynamic reconfiguration has been commonly adopted for modifying some components and/or the architecture at runtime. Traditional approaches typically stop the needed components, apply the changes, and restart the components. However, this scheme is not suitable for critical systems and degrades user experience. This paper proposes to switch from the stop/restart scheme to dynamic software updating (DSU) techniques. Instead of stopping a component, its implementation is replaced by another one specifically built to apply the modifications while maintaining the best quality of service possible. The major contributions of this work are: (i) the integration of DSU techniques in a component model; (ii) a reconfiguration development process including specification, proof of correctness using Coq, and; (iii) a systematic method to produce the executable script. In this perspective, the use of DSU techniques brings higher quality of service when reconfiguring component-based software. Moreover, the formalization allows ensuring the safety and consistency of the reconfiguration process.},
author = {Buisson, J{\'{e}}r{\'{e}}my and Dagnat, Fabien and Leroux, Elena and Martinez, S{\'{e}}bastien},
doi = {10.1016/j.jss.2015.11.039},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Component model,Dynamic reconfiguration,Dynamic software updating,Runtime evolution},
pages = {430--444},
title = {{Safe reconfiguration of Coqcots and Pycots components}},
volume = {122},
year = {2016}
}

@book{Bueche1999,
author = {Bueche, Frederick and Hecht, Eugene},
isbn = {0071367500},
keywords = {acceleration angle angular applied atoms axis body},
mendeley-tags = {acceleration angle angular applied atoms axis body},
pages = {138},
publisher = {McGraw Hill Professional},
title = {{Schaum's Easy Outline of College Physics}},
year = {1999}
}

@book{Bueche1997,
author = {Bueche, F J and Hetch, Eugene},
isbn = {0071367497},
pages = {1--80},
publisher = {McGraw-Hill},
title = {{Schaum's Outline of Theory and Problems of College Physics}},
year = {1997}
}

@book{Brownlee2011,
abstract = {Welcome to Clever Algorithms! This is a handbook of recipes for com- putational problem solving techniques from the fields of Computational Intelligence, Biologically Inspired Computation, and Metaheuristics. Clever Algorithms are interesting, practical, and fun to learn about and implement. Research scientists may be interested in browsing algorithm inspirations in search of an interesting system or process analogs to investigate. Developers and software engineers may compare various problem solving algorithms and technique-specific guidelines. Practitioners, students, and interested amateurs may implement state-of-the-art algorithms to address business or scientific needs, or simply play with the fascinating systems they represent. This introductory chapter provides relevant background information on Artificial Intelligence and Algorithms. The core of the book provides a large corpus of algorithms presented in a complete and consistent manner. The final chapter covers some advanced topics to consider once a number of algorithms have been mastered. This book has been designed as a reference text, where specific techniques are looked up, or where the algorithms across whole fields of study can be browsed, rather than being read cover-to-cover. This book is an algorithm handbook and a technique guidebook, and I hope you find something useful.},
author = {Brownlee, Jason},
booktitle = {Search},
isbn = {9781446785065},
issn = {0269-2821},
pages = {436},
title = {{Clever Algorithms - Nature-Inspired Programming Recipes}},
url = {http://www.cleveralgorithms.com},
year = {2011}
}

@article{Brooks,
author = {Brooks, Alison Wood and Huang, Laura and Kearney, Sarah Wood and Murray, Fiona E},
doi = {10.1073/pnas.1321202111},
title = {{Investors prefer entrepreneurial ventures pitched by attractive men}},
url = {www.pnas.org/cgi/doi/10.1073/pnas.1321202111}
}

@article{Bronstein1998,
author = {Bronstein, Manuel},
doi = {http://doi.acm.org/10.1145/281508.281649},
isbn = {1-58113-002-3},
journal = {Issac'98},
number = {November},
title = {{Symbolic integration tutorial}},
year = {1998}
}

@book{Bronson2003a,
abstract = {What could be better than the bestselling Schaum's Outline series? For students looking for a quick nuts-and-bolts overview, it would have to be Schaum's Easy Outline series. Every book in this series is a pared-down, simplified, and tightly focused version of its bigger predecessor. With an emphasis on clarity and brevity, each new title features a streamlined and updated format and the absolute essence of the subject, presented in a concise and readily understandable form. Graphic elements such as sidebars, reader-alert icons, and boxed highlights feature selected points from the text, illuminate keys to learning, and give students quick pointers to the essentials.},
author = {Bronson, Richard},
isbn = {0071428461},
keywords = {algebraically arbitrary constants ax ax ax boundar},
mendeley-tags = {algebraically arbitrary constants ax ax ax boundar},
pages = {144},
publisher = {McGraw Hill Professional},
title = {{Schaum's Easy Outline of Differential Equations}},
year = {2003}
}

@book{Bronson2003,
author = {Bronson, R},
editor = {Schaum},
isbn = {0071428461},
title = {{Differential Equations Crash Course}},
year = {2003}
}

@article{Braithwaite2011,
abstract = {This book is made out of nearly 100{\%} recycled blog posts. It collects Reg "Raganwald" Braithwaite's celebrated series of essays about Combinatory Logic, Method Combinators, and Ruby Meta-Programing into a convenient and inexpensive e-book.},
author = {Braithwaite, Reginald},
journal = {Methods},
title = {{Kestrels , Quirky Birds , and Hopeless Egocentricity: Raganwald's collected adventures in Combinatory Logic}},
year = {2011}
}

@article{Bradley2018,
abstract = {This is a collection of introductory, expository notes on applied category theory, inspired by the 2018 Applied Category Theory Workshop, and in these notes we take a leisurely stroll through two themes (functorial semantics and compositionality), two constructions (monoidal categories and decorated cospans) and two examples (chemical reaction networks and natural language processing) within the field.},
archivePrefix = {arXiv},
arxivId = {1809.05923},
author = {Bradley, Tai-Danae},
eprint = {1809.05923},
title = {{What is Applied Category Theory?}},
url = {http://arxiv.org/abs/1809.05923},
year = {2018}
}

@misc{Brachthauger,
author = {Brachthauger, J},
title = {{The Hitchhiker's Guide to Morphisms}}
}

@article{Boyer2015,
author = {Boyer, Brice and Faug, Jean-charles and Boyer, Brice and Faug, Jean-charles},
doi = {10.1145/2755996.2756673},
title = {{Linear Algebra for Computing Grobner Bases of Linear Recursive Multidimensional Sequences}},
year = {2015}
}

@book{Boyd2017,
abstract = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra - vectors, matrices, and least squares - that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB, and data sets accompanying the book online. It is suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
author = {Boyd, S and Vandenberghe, L},
doi = {10.1017/9781108583664},
isbn = {978-1-316-51896-0},
pages = {476},
title = {{Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares}},
url = {https://web.stanford.edu/{\%}7B{~}{\%}7Dboyd/vmls/},
year = {2017}
}

@article{Boyapati2013,
abstract = {We report on a 47-year-old man who was initially treated with finasteride for androgenetic alopecia. Despite continuous treatment, after year 4 his hair density was not as good as at year 2, and low-dose dutasteride at 0.5 mg/week was added to the finasteride therapy. This resulted in a dramatic increase in his hair density, demonstrating that combined therapy with finasteride and dutasteride can improve hair density in patients already taking finasteride. {\textcopyright}2012 The Authors. Australasian Journal of Dermatology {\textcopyright}2012 The Australasian College of Dermatologists.},
author = {Boyapati, Ann and Sinclair, Rodney},
doi = {10.1111/j.1440-0960.2012.00909.x},
issn = {00048380},
journal = {Australasian Journal of Dermatology},
keywords = {androgenetic alopecia,dutasteride,finasteride,male pattern hair loss},
number = {1},
pages = {49--51},
title = {{Combination therapy with finasteride and low-dose dutasteride in the treatment of androgenetic alopecia}},
volume = {54},
year = {2013}
}

@article{Bouma1993,
author = {Bouma, William and Hoffmann, Christoph M and Cai, Jiazhen and Paige, Robert and Bouma, William ; and Fudos, Ioannis ; and Hoffmann, Christoph M ; and Cai, Jiazhen ; and Robert, Paige},
number = {January 1994},
title = {{A Geometric Constraint Solver}},
url = {https://docs.lib.purdue.edu/cstech/1068},
year = {1993}
}

@techreport{Bottino1994,
abstract = {The interest in the use of Logic Programming in education stems from the great enthusiasm for Prolog that arose in the late seventies. However, Prolog is neither the purest Logic Programming language nor the easiest language for novices to come to terms with-but the opportunity to introduce very powerful ideas by teaching students to program in Prolog has been seized by many teachers at all levels of education. The continued success of Logic Programming depends in part on improved support for learning Prolog. Therefore, we concentrate here on issues connected with the development of a more integrated and complete view of learning and teaching Prolog.},
author = {Bottino, In and Forcheri, R M and Molfino, P},
title = {{Logic Programming in Education: a Perspective on the State of the Art}},
year = {1994}
}

@article{Bosnjak2016,
abstract = {Given that in practice training data is scarce for all but a small set of problems, a core question is how to incorporate prior knowledge into a model. In this paper, we consider the case of prior procedural knowledge for neural networks, such as knowing how a program should traverse a sequence, but not what local actions should be performed at each step. To this end, we present an end-to-end differentiable interpreter for the programming language Forth which enables programmers to write program sketches with slots that can be filled with behaviour trained from program input-output data. We can optimise this behaviour directly through gradient descent techniques on user-specified objectives, and also integrate the program into any larger neural computation graph. We show empirically that our interpreter is able to effectively leverage different levels of prior program structure and learn complex behaviours such as sequence sorting and addition. When connected to outputs of an LSTM and trained jointly, our interpreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities expressed in natural language stories.},
archivePrefix = {arXiv},
arxivId = {1605.06640},
author = {Bo{\v{s}}njak, Matko and Rockt{\"{a}}schel, Tim and Naradowsky, Jason and Riedel, Sebastian},
eprint = {1605.06640},
title = {{Programming with a Differentiable Forth Interpreter}},
url = {http://arxiv.org/abs/1605.06640},
year = {2016}
}

@article{Bose2006,
abstract = {We study the problem of reconstruction of a high-resolution image fromseveral blurred low-resolution image frames. The image frames consist of blurred, decimated, and noisy versions of a high-resolution image. The high-resolution image is modeled as a Markov random field (MRF), and a maximuma posteriori (MAP) estimation technique is used for the restoration.We show that with the periodic boundary condition, a high-resolution image can be restored efficiently by using fast Fourier transforms. We also apply the preconditioned conjugate gradient method to restore high-resolution images in the aperiodic boundary condition. Computer simulations are given to illustrate the effectiveness of the proposed approach.},
author = {Bose, Nirmal K and Ng, Michael K and Yau, Andy C},
doi = {10.1155/ASP/2006/35726},
issn = {11108657},
journal = {Eurasip Journal on Applied Signal Processing},
pages = {1--14},
title = {{A fast algorithm for image super-resolution from blurred observations}},
volume = {2006},
year = {2006}
}

@article{Borders2019,
author = {Borders, William A and Pervaiz, Ahmed Z and Fukami, Shunsuke and Camsari, Kerem Y and Ohno, Hideo and Datta, Supriyo},
doi = {10.1038/s41586-019-1557-9},
issn = {0028-0836},
journal = {Nature},
number = {7774},
pages = {390--393},
title = {{Integer factorization using stochastic magnetic tunnel junctions}},
url = {http://www.nature.com/articles/s41586-019-1557-9},
volume = {573},
year = {2019}
}

@book{Bonanno2015,
abstract = {This is an Open Access textbook on non-cooperative Game Theory with 165 solved exercises.},
author = {Bonanno, Giacomo},
pages = {585},
title = {{Game Theory (Open Access textbook with 165 solved exercises)}},
url = {http://arxiv.org/abs/1512.06808},
year = {2015}
}

@book{Bona2017,
author = {B{\'{o}}na, Mikl{\'{o}}s},
isbn = {9789813148840},
title = {{A Walk Through Combinatorics}},
year = {2017}
}

@inproceedings{Bohne2018,
abstract = {We have developed an alternative approach to teaching computer science students how to prove. First, students are taught how to prove theorems with the Coq proof assistant. In a second, more difficult, step students will transfer their acquired skills to the area of textbook proofs. In this article we present a realisation of the second step. Proofs in Coq have a high degree of formality while textbook proofs have only a medium one. Therefore our key idea is to reduce the degree of formality from the level of Coq to textbook proofs in several small steps. For that purpose we introduce three proof styles between Coq and textbook proofs, called line by line comments, weakened line by line comments, and structure faithful proofs. While this article is mostly conceptional we also report on experiences with putting our approach into practise.},
author = {B{\"{o}}hne, Sebastian and Kreitz, Christoph},
booktitle = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
doi = {10.4204/EPTCS.267.1},
issn = {20752180},
month = {mar},
pages = {1--18},
publisher = {Open Publishing Association},
title = {{Learning how to prove: From the Coq proof assistant to textbook style}},
volume = {267},
year = {2018}
}

@article{Blinn2002,
author = {Blinn, J},
journal = {Siggraph 2002, Course},
title = {{Using tensor diagrams to represent and solve geometric problems}},
url = {http://www.msr-waypoint.net/pubs/79791/UsingTensorDiagrams.pdf},
year = {2002}
}

@article{Blanchette2016,
abstract = {This paper surveys the emerging methods to automate reasoning over large libraries developed with formal proof assistants. We call these methods hammers. They give the authors of formal proofs a strong "one-stroke" tool for discharging difficult lemmas without the need for careful and detailed manual programming of proof search. The main ingredients underlying this approach are efficient automatic theorem provers that can cope with hundreds of axioms, suitable translations of the proof assistant's logic to the logic of the automatic provers, heuristic and learning methods that select relevant facts from large libraries, and methods that reconstruct the automatically found proofs inside the proof assistants. We outline the history of these methods, explain the main issues and techniques, and show their strength on several large benchmarks. We also discuss the relation of this technology to the QED Manifesto and consider its implications for QED-like efforts.},
author = {Blanchette, Jasmin C and Kaliszyk, Cezary and Paulson, Lawrence C and Urban, Josef},
doi = {10.6092/issn.1972-5787/4593},
issn = {19725787},
journal = {Journal of Formalized Reasoning},
title = {{Hammering towards QED}},
year = {2016}
}

@article{Biamonte2017,
abstract = {Tensor network methods are taking a central role in modern quantum physics and beyond. They can provide an efficient approximation to certain classes of quantum states, and the associated graphical language makes it easy to describe and pictorially reason about quantum circuits, channels, protocols, open systems and more. Our goal is to explain tensor networks and some associated methods as quickly and as painlessly as possible. Beginning with the key definitions, the graphical tensor network language is presented through examples. We then provide an introduction to matrix product states. We conclude the tutorial with tensor contractions evaluating combinatorial counting problems. The first one counts the number of solutions for Boolean formulae, whereas the second is Penrose's tensor contraction algorithm, returning the number of {\$}3{\$}-edge-colorings of {\$}3{\$}-regular planar graphs.},
archivePrefix = {arXiv},
arxivId = {1708.00006},
author = {Biamonte, Jacob and Bergholm, Ville},
eprint = {1708.00006},
month = {jul},
title = {{Tensor Networks in a Nutshell}},
url = {http://arxiv.org/abs/1708.00006},
year = {2017}
}

@article{Bhoopchand2016,
abstract = {To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very long-range dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.},
archivePrefix = {arXiv},
arxivId = {1611.08307},
author = {Bhoopchand, Avishkar and Rockt{\"{a}}schel, Tim and Barr, Earl and Riedel, Sebastian},
eprint = {1611.08307},
pages = {1--11},
title = {{Learning Python Code Suggestion with a Sparse Pointer Network}},
url = {http://arxiv.org/abs/1611.08307},
year = {2016}
}

@article{Bhasin2009,
abstract = {Background Athletes often take androgenic steroids in an attempt to increase their strength. The efficacy of these substances for this purpose is unsubstantiated, however. Methods We randomly assig...},
author = {Bhasin, Shalender and Storer, Thomas W and Berman, Nancy and Callegari, Carlos and Clevenger, Brenda and Phillips, Jeffrey and Bunnell, Thomas J and Tricker, Ray and Shirazi, Aida and Casaburi, Richard},
doi = {10.1056/NEJM199607043350101},
issn = {0028-4793},
journal = {http://dx.doi.org/10.1056/NEJM199607043350101},
number = {1},
pages = {1--7},
title = {{The Effects of Supraphysiologic Doses of Testosterone on Muscle Size and Strength in Normal Men}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJM199607043350101 https://www.nejm.org/doi/full/10.1056/NEJM199607043350101},
volume = {335},
year = {2009}
}

@book{Bhargava2016a,
abstract = {Grokking Algorithms is a fully illustrated, friendly guide that teaches you how to apply common algorithms to the practical problems you face every day as a programmer. You'll start with sorting and searching and, as you build up your skills in thinking algorithmically, you'll tackle more complex concerns such as data compression and artificial intelligence. Each carefully presented example includes helpful diagrams and fully annotated code samples in Python.},
author = {Bhargava, Aditya},
isbn = {9781617292231},
keywords = {Breadth-first search,Dijkstra's algorithm,Dynamic programming,Greedy algorithms,Hash tables,Introduction to algorithms,K-nearest neighbors,Quicksort,Recursion,Selection sort},
mendeley-tags = {Breadth-first search,Dijkstra's algorithm,Dynamic programming,Greedy algorithms,Hash tables,Introduction to algorithms,K-nearest neighbors,Quicksort,Recursion,Selection sort},
pages = {256},
publisher = {Manning Publications},
title = {{Grokking Algorithms: An illustrated guide for programmers and other curious people}},
year = {2016}
}

@article{Bertot2005,
author = {Bertot, Yves},
title = {{Coq in a Hurry}},
year = {2005}
}

@article{Berthomieu2015,
abstract = {Let f=(f1,...,fm) and g=(g1,...,gm) be two sets of m≥1 nonlinear polynomials in K[x1,...,xn] (K being a field). We consider the computational problem of finding-if any-an invertible transformation on the variables mapping f to g. The corresponding equivalence problem is known as Isomorphism of Polynomials with one Secret (IP1S) and is a fundamental problem in multivariate cryptography. Amongst its applications, we can cite Graph Isomorphism (GI) which reduces to equivalence of cubic polynomials with respect to an invertible linear change of variables, according to Agrawal and Saxena. The main result is a randomized polynomial-time algorithm for solving IP1S for quadratic instances-a particular case of importance in cryptography. To this end, we show that IP1S for quadratic polynomials can be reduced to a variant of the classical module isomorphism problem in representation theory. We show that we can essentially linearize the problem by reducing quadratic-IP1S to test the orthogonal simultaneous similarity of symmetric matrices; this latter problem was shown by Chistov, Ivanyos and Karpinski (ISSAC 1997) to be equivalent to finding an invertible matrix in the linear space Kn×nof n×n matrices over K and to compute the square root in a certain representation in a matrix algebra. While computing square roots of matrices can be done efficiently using numerical methods, it seems difficult to control the bit complexity of such methods. However, we present exact and polynomial-time algorithms for computing a representation of the square root of a matrix in Kn×n, for various fields (including finite fields), as a product of two matrices. Each coefficient of these matrices lies in an extension field of K of polynomial degree. We then consider {\#}IP1S, the counting version of IP1S for quadratic instances. In particular, we provide a (complete) characterization of the automorphism group of homogeneous quadratic polynomials. Finally, we also consider the more general Isomorphism of Polynomials (IP) problem where we allow an invertible linear transformation on the variables and on the set of polynomials. A randomized polynomial-time algorithm for solving IP when f=(x1d,...,xnd) is presented. From an algorithmic point of view, the problem boils down to factoring the determinant of a linear matrix (i.e. a matrix whose components are linear polynomials). This extends to IP a result of Kayal obtained for PolyProj.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.4974v5},
author = {Berthomieu, J{\'{e}}r{\'{e}}my and Faug{\`{e}}re, Jean Charles and Perret, Ludovic},
doi = {10.1016/j.jco.2015.04.001},
eprint = {arXiv:1307.4974v5},
issn = {10902708},
journal = {Journal of Complexity},
keywords = {Computer algebra,Module isomorphism,Multivariate cryptography,Polynomial isomorphism,Quadratic forms},
number = {4},
pages = {590--616},
title = {{Polynomial-time algorithms for quadratic isomorphism of polynomials: The regular case}},
volume = {31},
year = {2015}
}

@article{Bernstein2017,
abstract = {Cryptography is essential for the security of online communication, cars and implanted medical devices. However, many commonly used cryptosystems will be completely broken once large quantum computers exist. Post-quantum cryptography is cryptography under the assumption that the attacker has a large quantum computer; post-quantum cryptosystems strive to remain secure even in this scenario. This relatively young research area has seen some successes in identifying mathematical operations for which quantum algorithms offer little advantage in speed, and then building cryptographic systems around those. The central challenge in post-quantum cryptography is to meet demands for cryptographic usability and flexibility without sacrificing confidence.},
author = {Bernstein, Daniel J and Lange, Tanja},
doi = {10.1038/nature23461},
issn = {14764687},
journal = {Nature},
number = {7671},
pages = {188--194},
publisher = {Nature Publishing Group},
title = {{Post-quantum cryptography}},
url = {http://dx.doi.org/10.1038/nature23461},
volume = {549},
year = {2017}
}

@article{Berkhahn2019,
abstract = {We present a new flavor of Variational Autoencoder (VAE) that interpolates seamlessly between unsupervised, semi-supervised and fully supervised learning domains. We show that unlabeled datapoints not only boost unsupervised tasks, but also the classification performance. Vice versa, every label not only improves classification, but also unsupervised tasks. The proposed architecture is simple: A classification layer is connected to the topmost encoder layer, and then combined with the resampled latent layer for the decoder. The usual evidence lower bound (ELBO) loss is supplemented with a supervised loss target on this classification layer that is only applied for labeled datapoints. This simplicity allows for extending any existing VAE model to our proposed semi-supervised framework with minimal effort. In the context of classification, we found that this approach even outperforms a direct supervised setup.},
archivePrefix = {arXiv},
arxivId = {1908.03015},
author = {Berkhahn, Felix and Keys, Richard and Ouertani, Wajih and Shetty, Nikhil and Gei{\ss}ler, Dominik},
eprint = {1908.03015},
title = {{One Model To Rule Them All}},
url = {http://arxiv.org/abs/1908.03015},
year = {2019}
}

@book{Ben-yaacov2006,
author = {Ben-yaacov, Ilan and Roig, Francesc},
pages = {10},
title = {{Index Notation for Vector Calculus}},
year = {2006}
}

@article{Belter,
author = {Belter, Chris and Federer, Lisa},
title = {{Creating Infographics with Inkscape}}
}

@article{Behle2007,
author = {Behle, Markus},
journal = {Solutions},
title = {{Binary Decision Diagrams and Integer Programming}},
year = {2007}
}

@book{Beekmans2018,
author = {Beekmans, Gerard},
edition = {Version 8.},
editor = {Dubbs, Bruce},
title = {{Linux From Scratch}},
year = {2018}
}

@book{Beazley2013,
author = {Beazley, David and Jone, Brian K},
isbn = {9781449340377},
title = {{Python Cookbook}},
year = {2013}
}

@book{Beaver2013a,
abstract = {The best way to stay safe online is to stop hackers before they attack - first, by understanding their thinking and second, by ethically hacking your own site to measure the effectiveness of your security. This practical, top-selling guide will help you do both. Fully updated for Windows 8 and the latest version of Linux, Hacking For Dummies, 4th Edition explores the malicious hacker's mindset and helps you develop an ethical hacking plan (also known as penetration testing) using the newest tools and techniques. More timely than ever, this must-have book covers the very latest threats, including web app hacks, database hacks, VoIP hacks, and hacking of mobile devices. Guides you through the techniques and tools you need to stop hackers before they hack you Completely updated to examine the latest hacks to Windows 8 and the newest version of Linux Explores the malicious hackers's mindset so that you can counteract or avoid attacks completely Suggests ways to report vulnerabilities to upper management, manage security changes, and put anti-hacking policies and procedures in place If you're responsible for security or penetration testing in your organization, or want to beef up your current system through ethical hacking, make sure you get Hacking For Dummies, 4th Edition.},
author = {Beaver, Kevin},
isbn = {978-1-118-38093-2},
keywords = {1118380932,Computer Books: General,Computer Data Security,Computer fraud {\&} hacking,Computer networks,Computer security,Computers,Computers - Computer Security,Computers / Security / General,Computing: Professional {\&} Programming,Hackers,Hacking For Dummies,Kevin Beaver,Security - General,Security measures,Wiley John + Sons},
pages = {411},
publisher = {John Wiley {\&} Sons},
title = {{Hacking For Dummies}},
url = {https://www.wiley.com/en-us/Hacking+For+Dummies{\%}2C+4th+Edition-p-9781118380932},
year = {2013}
}

@article{Bates2006,
author = {Bates, Dan and Wampler, Charles},
title = {{Bertini: A new software package for computations in numerical algebraic geometry}},
year = {2006}
}

@article{Basten2015,
author = {Basten, Ulrike and Hilger, Kirsten and Fiebach, Christian J},
doi = {10.1016/j.intell.2015.04.009},
issn = {01602896},
journal = {Intelligence},
month = {jul},
pages = {10--27},
title = {{Where smart brains are different: A quantitative meta-analysis of functional and structural brain imaging studies on intelligence}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0160289615000562},
volume = {51},
year = {2015}
}

@article{Bartholomew1991,
author = {Bartholomew, Kim and Horowitz, Leonard M},
doi = {10.1037/0022-3514.61.2.226},
issn = {1939-1315},
journal = {Journal of Personality and Social Psychology},
number = {2},
pages = {226--244},
title = {{Attachment styles among young adults: A test of a four-category model.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0022-3514.61.2.226},
volume = {61},
year = {1991}
}

@article{Barnard1986,
author = {Barnard, H Jack and Metz, Robert F and Price, Arthur L},
doi = {10.1109/TSE.1986.6312941},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {feb},
number = {2},
pages = {258--263},
title = {{A recommended practice for describing software designs: IEEE standards project 1016}},
url = {http://ieeexplore.ieee.org/document/6312941/},
volume = {SE-12},
year = {1986}
}

@book{Barendregt1994,
address = {Berlin, Heidelberg},
doi = {10.1007/3-540-58085-9},
editor = {Barendregt, Henk and Nipkow, Tobias},
isbn = {978-3-540-58085-0},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Types for Proofs and Programs}},
url = {http://link.springer.com/10.1007/3-540-58085-9},
volume = {806},
year = {1994}
}

@book{BarbaraHoffmanJohnSchorgeJosephSchafferLisaHalvorsonKarenBradshaw2012,
abstract = {The only gynecology resource that combines a full-color text and a procedural atlas—revised and updated Part medical text, part surgical atlas, Williams Gynecology is written by the renowned team of ob-gyn clinicians at Dallas' Parkland Hospital who are responsible for the landmark Williams Obstetrics. The new edition of Williams Gynecology maintains the consistent tone, leading-edge clinical insights, and quality illustrations of the successful first edition, while expanding and refreshing its content to keep pace with the most recent developments in this dynamic field. The many important topics covered in Williams Gynecology are evidence-based, yet the book is specifically designed as a practical quick-reference guide, aided throughout by helpful teaching points. Reflecting the latest clinical perspectives and research, the second edition features outstanding new coverage of minimally invasive procedures, robotics, and gynecologic anatomy. Features Two resources in one—full-color medical text and surgical atlas—conveniently surveys the entire spectrum of gynecologic disease, including general gynecology, reproductive endocrinology and infertility, urogynecology, and gynecologic oncology Atlas of gynecologic surgery contains 450 figures that illustrate operative techniques Unique consistent text design for an efficient approach to diagnosis and treatment Strong procedure orientation covers a vast array of surgical operations, which are illustrated in detail Evidence-based discussion of disease evaluation reinforces and supports the clinical relevance of the book's diagnostic and treatment methods Distinguished authorship from the same Parkland Hospital-based team which edited Williams Obstetrics—the leading reference in obstetrics for more than a century Newly illustrated gynecologic anatomy chapter created with the surgeon in mind to emphasize critical anatomy for successful surgery New coverage of minimally invasive procedures and robotics, the latest procedures in gynecologic oncology, and in-vitro fertilization Numerous illustrations, photographs, tables, and treatment algorithms},
author = {{Barbara Hoffman, John Schorge, Joseph Schaffer, Lisa Halvorson, Karen Bradshaw}, F Cunningham},
edition = {2},
isbn = {0071716726},
keywords = {abdominal abnormal abortion adenomyosis adnexal ad},
mendeley-tags = {abdominal abnormal abortion adenomyosis adnexal ad},
pages = {1401},
publisher = {McGraw Hill Professional},
title = {{Williams Gynecology}},
year = {2012}
}

@article{Balog2019,
abstract = {We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.},
archivePrefix = {arXiv},
arxivId = {1611.01989},
author = {Balog, Matej and Gaunt, Alexander L. and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
eprint = {1611.01989},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {nov},
title = {{DeepCoder: Learning to Write Programs}},
url = {http://arxiv.org/abs/1611.01989},
year = {2016}
}

@article{Balci2006,
abstract = {In this paper, we establish the exact relationship between the continuous and the discrete phase-difference of two shifted images, and show that their discrete phase difference is a 2-dimensional sawtooth signal. As a result, the exact shifts between two images can be determined to sub-pixel accuracy by counting the number of cycles of the phase difference matrix along each frequency axis. The sub-pixel portion is represented by a fraction of a cycle corresponding to the non-integer part of the shift. The problem is formulated as an over-determined system of equations, and is solved by imposing a regularity constraint, using the method of Generalized Cross Validation (GCV).},
author = {Balci, Murat and Foroosh, Hassan},
doi = {10.1155/ASP/2006/60796},
issn = {11108657},
journal = {Eurasip Journal on Applied Signal Processing},
pages = {1--11},
title = {{Subpixel registration directly from the phase difference}},
volume = {2006},
year = {2006}
}

@techreport{Baker,
author = {Baker, Andrew and Baker, A J},
title = {{An Introduction to Galois Theory}}
}

@book{Bah2011,
abstract = {Inkscape: Guide to a Vector Drawing Program, Fourth Edition, is the guide to the Inkscape program. With coverage of versions 0.47 and 0.48, this authoritative introduction and reference features hundreds of useful illustrations.Using Inkscape, you can produce a wide variety of art, from photorealistic drawings to organizational charts. Inkscape uses SVG, a powerful vector-based drawing language and W3C web standard, as its native format. SVG drawings can be viewed directly in browsers such as Firefox, Opera, Chrome, Safari, and Internet Explorer 9. A subset of SVG has been adopted by the mobile phone market. Inkscape is available free for Windows,Macintosh, and Linux operating systems.},
author = {Bah, Tavmjong},
edition = {4},
isbn = {9780132764148},
keywords = {data visualization,infographics},
pages = {473},
publisher = {Prentice Hall},
title = {{Inkscape: Guide to a Vector Drawing Program}},
year = {2011}
}

@article{Baez2011,
abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology. Namely, a linear operator behaves very much like a "cobordism": a manifold representing spacetime, going between two manifolds representing space. This led to a burst of work on topological quantum field theory and "quantum topology". But this was just the beginning: similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of "closed symmetric monoidal category". We assume no prior knowledge of category theory, proof theory or computer science. {\textcopyright}2010 Springer-Verlag Berlin Heidelberg.},
archivePrefix = {arXiv},
arxivId = {0903.0340},
author = {Baez, J and Stay, M},
doi = {10.1007/978-3-642-12821-9_2},
eprint = {0903.0340},
isbn = {9783642128202},
issn = {00758450},
journal = {Lecture Notes in Physics},
pages = {95--172},
title = {{Physics, topology, logic and computation: A Rosetta Stone}},
volume = {813},
year = {2011}
}

@article{Backus1978,
author = {Backus, John},
title = {{Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs}},
year = {1978}
}

@book{Ayres1999,
abstract = {What could be better than the bestselling Schaum's Outline series? For students looking for a quick nuts-and-bolts overview, it would have to be Schaum's Easy Outline series. Every book in this series is a pared-down, simplified, and tightly focused version of its predecessor. With an emphasis on clarity and brevity, each new title features a streamlined and updated format and the absolute essence of the subject, presented in a concise and readily understandable form.},
author = {Ayres, Frank and Mendelson, Elliott},
isbn = {0071367292},
keywords = {2x dx angle of intersection antiderivative approxi},
mendeley-tags = {2x dx angle of intersection antiderivative approxi},
pages = {135},
publisher = {McGraw Hill Professional},
title = {{Schaum's Easy Outline of Calculus}},
year = {1999}
}

@book{Ayres2000,
author = {Ayres, Frank and Mendelson, Elliot},
isbn = {0070527105},
title = {{Calculus Crash Course}},
year = {2000}
}

@book{Ayres1952,
author = {Ayres, Frank},
pages = {296},
title = {{Schaum's outline of theory and problems of differential equations}},
url = {https://books.google.com.uy/books?id=8vs-AAAAIAAJ},
year = {1952}
}

@book{Ayres,
author = {Ayres, F and Jaisingh, L R},
edition = {2},
isbn = {0-07-140327-2},
title = {{Schaum's outline of theory and problems of abstract algebra}},
url = {https://books.google.com.uy/books?id=U0r4{\%}5C{\_}S7eeDYC}
}

@book{Ayres2004,
author = {Ayres, F and Jaisingh, L R},
isbn = {0071430989},
title = {{Theory and Problems of Abstract Algebra}},
year = {2004}
}

@book{Awodey2006,
abstract = {This book is a text and reference book on Category Theory, a branch of abstract algebra. The book contains clear definitions of the essential concepts, which are illuminated with numerous accessible examples. It provides full proofs of all the important propositions and theorems, and aims to make the basic ideas, theorems, and methods of Category Theory understandable. Although it assumes few mathematical pre-requisites, the standard of mathematical rigour is not compromised. The material covered includes the standard core of categories; functors; natural transformations; equivalence; limits and colimits; functor categories; representables; Yoneda's lemma; adjoints; and monads. An extra topic of cartesian closed categories and the lambda-calculus is also provided.},
author = {Awodey, Steve},
booktitle = {Oxford Logic Guides},
doi = {10.1093/acprof:oso/9780198568612.001.0001},
edition = {2},
isbn = {9780198568612},
keywords = {Adjoints,Cartesian closed categories,Equivalence,Functor categories,Functors,Limits and colimits,Monads,Natural transformations,Representables,Yoneda's lemma},
month = {may},
pages = {328},
publisher = {Oxford University Press},
title = {{Category Theory}},
url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780198568612.001.0001/acprof-9780198568612},
year = {2006}
}

@article{Avron2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1901.06688v2},
author = {Avron, J and Kenneth, O},
eprint = {arXiv:1901.06688v2},
title = {{An elementary introduction to the geometry of quantum states with a picture book}},
year = {2019}
}

@book{Ashmore,
author = {Ashmore, Derek C},
title = {{The JAVA EE Architect ' s Handbook}}
}

@article{Ashmore2016,
abstract = {Given published success stories from Netflix, Amazon, and many others; many companies are adopting microservices architecture. In fact, the published success at microservices for some major companies has started a fad. For organizations that are heavily invested in Java technologies, writing microservices using Java is a natural progression.},
author = {Ashmore, Derek C},
pages = {105},
publisher = {DVT Press},
title = {{Microservices for Java EE Architects: Addendum for The Java EE Architect's Handbook}},
year = {2016}
}

@book{Ashmore,
author = {Ashmore, Derek C},
title = {{The JAVA EE Architect ' s Handbook}}
}

@article{Aron1997,
abstract = {A practical methodology is presented for creating closeness in an experimental context. Whether or not an individual is in a relationship, particular pairings of individuals in the relationship, an...},
author = {Aron, Arthur and Melinat, Edward and Aron, Elaine N and Vallone, Robert Darrin and Bator, Renee J},
doi = {10.1177/0146167297234003},
issn = {0146-1672},
journal = {Personality and Social Psychology Bulletin},
number = {4},
pages = {363--377},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{The Experimental Generation of Interpersonal Closeness: A Procedure and Some Preliminary Findings}},
url = {http://journals.sagepub.com/doi/10.1177/0146167297234003},
volume = {23},
year = {1997}
}

@article{Ap-Apid,
abstract = {This paper presents an algorithm for detecting nudity in color images. A skin color distribution model based on the RGB, Normalized RGB, and HSV color spaces is constructed using correlation and linear regression. The skin color model is used to identify and locate skin regions in an image. These regions are analyzed for clues indicating nudity or non-nudity such as their sizes and relative distances from each other. Based on these clues and the percentage of skin in the image, an image is classified nude or non-nude. The skin color distribution model performs with 96.29{\%} recall and 6.76{\%} false positive rate on a test set consisting of 2,303,824 manually labeled skin pixels and 24,285,952 manually labeled non-skin pixels. The Nudity Detection Algorithm is able to detect nudity with a 94.77{\%} recall and a false positive rate of 5.04{\%} on a set of images consisting of 421 nude images and 635 non-nude images.},
author = {Ap-Apid, Rigan},
title = {{An Algorithm for Nudity Detection}}
}

@article{Antoy2017,
abstract = {This book is about programming in Curry, a general-purpose declarative programming lan- guage that integrates functional with logic programming. Curry seamlessly combines the key features of functional programming (nested expressions, lazy evaluation, higher-order functions), logic programming (logical variables, partial data structures, built-in search), and concurrent programming (concurrent evaluation of constraints with synchronization on logical variables).},
author = {Antoy, Sergio and Hanus, Michael},
title = {{Curry A Tutorial Introduction}},
url = {https://web.cecs.pdx.edu/{\%}7B{~}{\%}7Dantoy/Courses/TPFLP/tutorial.pdf},
year = {2017}
}

@article{Amorim,
author = {Amorim, Arthur Azevedo De},
title = {{Software Foundations Benjamin C . Pierce Chris Casinghino Marco Gaboardi Michael Greenberg Cătălin Hriţcu Vilhelm Sj{\"{o}}berg Brent Yorgey}}
}

@book{AmericanPsychiatricAssociation2013,
author = {{American Psychiatric Association}},
booktitle = {Diagnostic and Statistical Manual of Mental Disorders},
doi = {10.1176/appi.books.9780890425596},
isbn = {0890425574},
pages = {991},
publisher = {American Psychiatric Pub},
title = {{Diagnostic and Statistical Manual of Mental Disorders DSM-5}},
url = {https://psychiatryonline.org/doi/book/10.1176/appi.books.9780890425596},
year = {2013}
}

@article{AmericanForest&PaperAssociation2001,
author = {{American Forest {\&} Paper Association}},
title = {{Details for Conventional Wood Frame Construction}},
year = {2001}
}

@incollection{Alyafeaia,
author = {Alyafeai, Zaid},
title = {{Advanced Integration Techniques}}
}

@article{Alpaydin2010,
abstract = {The machine learning field, which can be briefly defined as enabling computers make successful predictions using past experiences, has exhibited an impressive development recently with the help of the rapid increase in the storage capacity and processing power of computers. Together with many other disciplines, machine learning methods have been widely employed in bioinformatics. The difficulties and cost of biological analyses have led to the development of sophisticated machine learning approaches for this application area. In this chapter, we first review the fundamental concepts of machine learning such as feature assessment, unsupervised versus supervised learning and types of classification. Then, we point out the main issues of designing machine learning experiments and their performance evaluation. Finally, we introduce some supervised learning methods.},
archivePrefix = {arXiv},
arxivId = {0904.3664},
author = {Alpaydin, Ethem},
doi = {10.1007/978-1-62703-748-8_7},
eprint = {0904.3664},
isbn = {9780262012430},
issn = {1940-6029},
journal = {Introduction to Machine Learning},
pages = {350--380},
pmid = {24272434},
title = {{Introduction to Machine Learning Third Edition}},
year = {2010}
}

@article{Allamanis2014,
abstract = {We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic role. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present HAGGIS, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply HAGGIS to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q{\&}A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management.},
archivePrefix = {arXiv},
arxivId = {1404.0417},
author = {Allamanis, Miltiadis and Sutton, Charles},
doi = {10.1145/2635868.2635901},
eprint = {1404.0417},
keywords = {code idioms,naturalness of,syntactic code patterns},
title = {{Mining Idioms from Source Code}},
url = {http://arxiv.org/abs/1404.0417{\%}0Ahttp://dx.doi.org/10.1145/2635868.2635901},
year = {2014}
}

@article{Allahyari2017,
abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
archivePrefix = {arXiv},
arxivId = {1707.02268},
author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and Trippe, Elizabeth D and Gutierrez, Juan B and Kochut, Krys},
eprint = {1707.02268},
keywords = {acm reference format,eliza-,knowledge bases,mehdi allahyari,mehdi assefi,saeid safaei,seyedamin pouriyeh,text summarization,topic models},
number = {1},
title = {{Text Summarization Techniques: A Brief Survey}},
url = {http://arxiv.org/abs/1707.02268},
year = {2017}
}

@book{Aldini2010,
abstract = {In the field of formalmethods in computer science, concurrency theory is receiving a constantly increasing interest. This is especially true for process algebra. Although it had been originally conceived as a means for reasoning about the semantics of con- current programs, process algebraic formalisms like CCS, CSP, ACP, {\$}\pi{\$}-calculus, and their extensions were soon used also for comprehending functional and nonfunctional aspects of the behavior of communicating concurrent systems.},
address = {London},
author = {Aldini, Alessandro and Bernardo, Marco and Corradini, Flavio},
doi = {10.1007/978-1-84800-223-4},
isbn = {978-1-84800-222-7},
pages = {304},
publisher = {Springer London},
title = {{A Process Algebraic Approach to Software Architecture Design}},
url = {http://link.springer.com/10.1007/978-1-84800-223-4},
year = {2010}
}

@book{Alcock2013,
author = {Alcock, Lara},
isbn = {0199661316},
publisher = {Oxford University Press},
title = {{How To Study As A Mathematics Major}},
year = {2013}
}

@book{Alchin2010,
abstract = {Advanced coding techniques and tools},
author = {Alchin, Marty},
isbn = {9781430227571},
keywords = {{\_}{\_}init{\_}{\_} accept actually addition allows annotatio},
mendeley-tags = {{\_}{\_}init{\_}{\_} accept actually addition allows annotatio},
pages = {368},
publisher = {Apress},
title = {{Pro Python}},
year = {2010}
}

@book{Aho1988,
abstract = {Originally developed by Alfred Aho, Brian Kernighan, and Peter Weinberger in 1977, AWK is a pattern-matching language for writing short programs to perform common data-manipulation tasks. In 1985, a new version of the language was developed, incorporating additional features such as multiple input files, dynamic regular expressions, and user-defined functions. This new version is available for both Unix and MS-DOS. This is the first book on AWK. It begins with a tutorial that shows how easy AWK is to use. The tutorial is followed by a comprehensive manual for the new version of AWK. Subsequent chapters illustrate the language by a range of useful applications, such as: *Retrieving, transforming, reducing, and validating data *Managing small, personal databases *Text processing *Little languages *Experimenting with algorithms The examples illustrates the book's three themes: showing how to use AWK well, demonstrating AWK's versatility, and explaining how common computing operations are done. In addition, the book contains two appendixes: summary of the language, and answers to selected exercises. 020107981XB04062001},
author = {Aho, Alfred V and Kernighan, Brian W and Weinberger, Peter J},
booktitle = {Addison-Wesley series in computer science and information processing},
isbn = {020107981X},
pages = {225},
publisher = {Addison-Wesley Publishing Company},
title = {{The AWK Programming Language}},
url = {https://books.google.com.uy/books?id=53ueQgAACAAJ},
year = {1988}
}

@book{Aho,
author = {Aho, A and Kernighan, Brian W and Weinberger, P},
title = {{The AWK Programming Language}}
}

@article{Ahmed,
author = {Ahmed, S},
title = {{Online Algorithms in Computational Geometry}}
}

@article{Ahlgren2013,
abstract = {We present NrSample, a framework for program synthesis in inductive logic programming. NrSam-ple uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph's default) and Progol's A * search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A * , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A * substantially sacrificed accuracy to induce faster, and one in which Progol A * was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for.},
author = {Ahlgren, John and Yuen, Shiu Yin},
journal = {Journal of Machine Learning Research},
keywords = {Boolean satisfiability problem,constraint satis-faction,inductive logic programming,program synthesis,theory induction},
pages = {3649--3681},
title = {{Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming}},
volume = {14},
year = {2013}
}

@techreport{Adiwardana,
abstract = {We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is trained to minimize perplexity, an automatic metric that we compare against human judgement of multi-turn conversation quality. To capture this judgement, we propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of good conversation. Interestingly , our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72{\%} on multi-turn evaluation) suggests that a human-level SSA of 86{\%} is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79{\%} SSA, 23{\%} higher in absolute SSA than existing chatbots that we evaluated.},
archivePrefix = {arXiv},
arxivId = {2001.09977v1},
author = {Adiwardana, Daniel and Luong, Minh-Thang and So, David R and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and {Lu Quoc}, Yifeng and Le, V},
eprint = {2001.09977v1},
title = {{Towards a Human-like Open-Domain Chatbot}}
}

@article{Abdesselam2002,
abstract = {We show, in great detail, how the perturbative tools of quantum field theory allow one to rigorously obtain: a ``categorified'' Faa di Bruno type formula for multiple composition, an explicit formula for reversion and a proof of Lagrange-Good inversion, all in the setting of multivariable power series. We took great pains to offer a self-contained presentation that, we hope, will provide any mathematician who wishes, an easy access to the wonderland of quantum field theory.},
archivePrefix = {arXiv},
arxivId = {math/0212121},
author = {Abdesselam, Abdelmalek},
eprint = {0212121},
primaryClass = {math},
title = {{Feynman Diagrams in Algebraic Combinatorics}},
url = {http://arxiv.org/abs/math/0212121},
year = {2002}
}

@misc{,
title = {{Machine Learning Cheat-sheet}}
}

@article{,
abstract = {This cheat sheet is a condensed version of machine learning manual, which contains many classical equations and diagrams on machine learning, and aims to help you quickly recall knowledge and ideas in machine learning. This cheat sheet has two significant advantages: 1. Clearer symbols. Mathematical formulas use quite a lot of confusing symbols. For example, X can be a set, a random variable, or a matrix. This is very confusing and makes it very difficult for readers to understand the meaning of math formulas. This cheat sheet tries to standardize the usage of symbols, and all symbols are clearly pre-defined, see section {\S}. 2. Less thinking jumps. In many machine learning books, authors omit some intermediary steps of a mathematical proof process, which may save some space but causes difficulty for readers to understand this formula and readers get lost in the middle way of the derivation process. This cheat sheet tries to keep important intermediary steps as where as possible},
pages = {135},
title = {{Machine Learning Cheat Sheet: classical equations, diagrams and tricks in machine learning}},
url = {https://github.com/soulmachine/machine-learning-cheat-sheet},
year = {2017}
}

@book{,
title = {scikit-learn user guide},
year = {2017}
}

@misc{,
title = {{Real Life Cheat Sheets Cookbook for Common Linux Tasks}}
}

@incollection{,
abstract = {Coq is a tool to help you write formal proofs, that are mechanically veri?able. This means that once you have proved something in Coq, you have very high assurance that it is true ? more than what you usually have when doing a pen-and-paper proof. It can be used in an interactive style, thus we call it an interactive proof assistant. It is based on a very expressive logic, the Calculus of Inductive Constructions.},
chapter = {1},
title = {{Theorem proving with Coq}}
}

@article{,
title = {{Categories of algorithms}}
}

@misc{,
booktitle = {Cheat Sheets},
number = {100},
pages = {100},
title = {{Python for Data Science}}
}

@book{,
edition = {Wikibooks},
title = {en.wikibooks.org},
year = {2018}
}

@book{,
title = {{Haskell Programming from first principles}}
}

@techreport{,
title = {{Quick-'n'-Dirty Prolog Tutorial}},
year = {2004}
}

@article{,
title = {{Professor Frisby's Mostly Adequate Guide to Functional Programming}},
url = {https://www.gitbook.com/download/pdf/book/mostly-adequate/mostly-adequate-guide}
}

@article{,
title = {{Probability Cheatsheet}}
}

@misc{,
issn = {0194312437},
title = {{Oxford Collocations Dictionary for Students of English}}
}

@article{,
title = {{Sparse Linear Systems}}
}

@article{,
title = {{Semantic Backpropagation in Genetic Programming}}
}

@article{,
title = {{Longman Essay Activator: The Key to Writing Success}}
}

@misc{,
title = {{Knuth403-422.Pdf}}
}

@incollection{,
abstract = {Before introducing the more advanced techniques, we will look at a shortcut for the easier of the substitution-type integrals. Advanced integration techniques then follow: integration by parts, trigonometric integrals, trigonometric substitution, and partial fraction decompositions. 7.1 Substitution-Type Integration by Inspection In this section we will consider integrals which we would have done earlier by substitution, but which are simple enough that we can guess the approximate form of the antiderivatives, and then insert any factors needed to correct for discrepancies detected by (mentally) computing the derivative of the approximate form and comparing it to the original integrand. Some general forms will be mentioned as formulas, but the idea is to be able to compute many such integrals without resorting to writing the usual u-substitution steps. Example 7.1.1 Compute cos 5x dx. Solution: We can anticipate that the approximate form 1 of the answer is sin 5x, but then d dx sin 5x = cos 5x {\textperiodcentered}d dx (5x) = cos 5x {\textperiodcentered}5 = 5 cos 5x. Since we are looking for a function whose derivative is cos 5x, and we found one whose derivative is 5 cos 5x, we see that our candidate antiderivative sin 5x gives a derivative with an extra factor of 5, compared with the desired outcome. Our candidate antiderivative's derivative is 5 times too large, so this candidate antiderivative, sin 5x must be 5 times too large. To compensate and arrive at a function with the proper derivative, we multiply our candidate sin 5x by 1},
chapter = {7},
isbn = {9789401037396},
title = {{Advanced Integration Techniques}}
}

@misc{,
pages = {1--12},
title = {{Inkscape Interface Tutorial}},
year = {2019}
}
