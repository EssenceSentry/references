Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{VanDyk2001a,
abstract = {The term data augmentation refers to methods for constructing iterative optimization or sampling algorithms via the introductionof unobserved data or latent variables. For de- terministic algorithms, the method was popularizedin the general statistical community by the seminal article by Dempster, Laird, and Rubin on the EM algorithm for maximizing a likelihood function or, more generally, a posterior density. For stochastic algorithms, the method was popularized in the statistical literature by Tanner and Wong's Data Augmenta- tion algorithmfor posterior sampling and in the physics literatureby Swendsen andWang's algorithm for sampling from the Ising and Potts models and their generalizations; in the physics literature, the method of data augmentationis referred to as the method of auxiliary variables. Data augmentationschemes were used by Tanner and Wong to make simulation feasible and simple, while auxiliary variables were adopted by Swendsen and Wang to im- prove the speedof iterativesimulation.In general,however, constructingdata augmentation schemes that result in both simple and fast algorithms is a matter of art in that successful strategiesvary greatlywith the (observed-data)models being considered.After an overview ofdata augmentation/auxiliaryvariablesand some recent developmentsin methods for con- structingsuchef?cientdataaugmentationschemes,we introduceaneffectivesearch strategy that combines the ideas of marginal augmentationand conditionalaugmentation, together with a deterministic approximationmethod for selecting good augmentation schemes. We then apply this strategy to three common classes of models (speci?cally, multivariate t, probit regression,and mixed-effectsmodels) to obtain ef?cient Markov chainMonte Carlo algorithms for posterior sampling. We provide theoretical and empirical evidence that the resulting algorithms, while requiring similar programming effort, can show dramatic im- provement over the Gibbs samplers commonly used for these models in practice. A key featureofall thesenewalgorithmsis that they arepositiverecurrentsubchainsofnonpositive recurrent Markov chains constructed in larger spaces.},
author = {van Dyk, David A and Meng, Xiao-Li},
doi = {10.1198/10618600152418584},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Auxiliary variables,Conditional augmentation,EM algorithm,Gibbs sampler,Haar measure,Hierarchical models,Marginal augmentation,Markov chain Monte Carlo,Mixed-effects models,Nonpositive recurrent Markov chain,Posterior distributions,Probit regression,Rate of convergence.},
month = {mar},
number = {1},
pages = {1--50},
title = {{The Art of Data Augmentation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/10618600152418584},
volume = {10},
year = {2001}
}
